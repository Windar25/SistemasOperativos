## 2.4 SCHEDULING

Cuando un ordenador está multiprogramado, suele tener varios procesos o hilos compitiendo por la CPU al mismo tiempo. Esta situación ocurre cuando por lo menos 2 de ellos se encuentran en estado ready. Si solo hay una CPU disponible, se debe realizar una decisión para escoger el siguiente proceso a ejecutar. La parte del sistema operativo que realiza está decisión llamada el **scheduler** y el algoritmo utilizado para es llamado **scheduling algorithm**. Estos temas constituyen el objeto de las siguientes secciones.

Muchas de las mismas cuestiones que se aplican a la programación de procesos también se aplican a la programación de hilos, aunque algunas son diferentes. Cuando el kernel maneja hilos, la planificación se realiza por un hilo, sin tener en cuenta a qué proceso pertenece el hilo. Inicialmente vamos a concentrarnos en los errores de planificación aplicados a procesos  y a hilos. Más adelante veremos explícitamente la planificación de hilos y algunos de los problemas específicos que plantea. Nos ocuparemos de los chips multinúcleo en el cap 8.

### 2.4.1 Introduction to Scheduling

En los tiempos de sistemas por lotes con entradas en forma de imágenes de tarjetas en una cinta magnética, el algoritmo de programación era sencillo: solo había que ejecutar el siguiente trabajo en la cinta. Con los sistemas de multiprogramación, el algoritmo de planificación se volvió más complejo dado que habían multiples usuarios esperando por el mismo servicio. Algunos mainframes se mantuvieron combinando el batch y el timesharing service, lo que requiere que el planificador decida si el siguiente paso debe ser un trabajo por lotes o un usuario interactivo en un terminal(Como inciso, un trabajo por lotes puede ser una solicitud para ejecutar varios programas en sucesión, pero para esta sección, sólo asumiremos que es una solicitud para ejecutar un solo programa). Dado que el tiempo de CPU es un recurso escaso en estas máquinas, un buen planificador puede marcar una gran diferencia en el rendimiento percibido y la satisfacción del usuario. Por ello se ha dedicado mucho trabajo a idear algoritos de planificación inteligentes y eficientes.

Con la llegada de las computadoras personales, la situación cambió en 2 sentidos. 

Primero, la mayor parte del tiempo solo hay un proceso activo. Es poco probable que un usuario que introduce un documento en un procesador de textos, esté compilando simúltaneamente en segundo plano. Cuando el usuario escribe una orden en el procesador de textos, el planificador no tiene porque hacer mucho trabajo par averiguar qué proceso debe ejecutarse: el procesador de textos es el único candidato.

Segundo, las computadoras se han vuelto mucho más rápidas con el paso de los años, por lo que la CPU ya casi nunca es un recurso escaso. La mayoría de los programas para ordenadores personales están limitados por la velocidad a la que el usuario puede introducir datos(tecleando o haciendo click), no por la velocidad a que la CPU puede procesarlos. Incluso las compilaciones que antes consumían muchos ciclos de CPU, hoy en día solo tardan segundos. Además, cuando se ejecutan 2 programas a la vez, como un procesador de textos y una hoja de cálculo, apenas importa cuál va primero, ya que el usuario probablemente está esperando que terminen los 2. En consecuencia, la planificación no tiene mucha importancia en las PCs sencillas. Por supuesto, hay aplicaciones que prácticamente se comen viva a la CPU. Por ejemplo, renderizar una hora de video en alta resolución mientras se ajustan los colores de cada uno de los 107892 frames en NTSC o 90 000 frames en PAL, esto requiere una potencia de cálculo industrial. Aplicaciones similares a esta son excepciones.

Cuando vamos a los servidores network, vemos que múltiples procesos compiten continuamente por el CPU, entonces la planificación es importante nuevamente. Por ejemplo, cuando la CPU escoge entre ejecutar un proceso que recoja las estadísticas diarias y otra que atienda las peticiones de los usuarios, éstos estarán mucho más contentos si esta última se queda con la CPU en primer lugar.

El argumento de "abundancia de recursos" no es igual de sustentable en CPUs como en los smartphones (excepto quizás los modelos más potentes) y nodos en redes de sensores. En los smartphones, la CPU puede ser más débil y la memoria más pequeña. Otro punto a tomar en cuenta es la batería, cuyo tiempo de vida es más importante en estos dispositivos por lo que los planificadores tratan de optimizar el consumo de esta.
Además de escoger el proceso correcto para ejecutar, el planificador debe preocuparse de realizar un uso eficiente de la CPU, dado que el cambio entre procesos es caro. Para comenzar, debe producirse un cambio del modo de usuario al modo kernel. A continuación, se debe guardar el estado del proceso actual, incluyendo el almacenamiento de sus registros en la tabla de proceso para que puedan ser recargados más tarde. En algunos sistemas, el memory map también debe guardarse. Luego, un nuevo proceso debe ser seleccionado por el algoritmo del planificador para que se ejecute. Además de todo esto, el cambio de proceso puede invalidar la memoria cache y las tablas relacionadas, obligando a recargarla dinámicamente desde la memoria principal 2 veces(al entrar en el kernel y al salir de él). En definitiva, realizar demasiados cambios de proceso por segundo puede consumir una cantidad sustancial de tiempo en CPU, por lo que se recomienda precaución.

#### Process Behavior

Casi todos los procesos alternan ráfagas de computación con peticiones de I/O (disco o red), como se muestra en la Fig. 2-39. A menudo, la CPU funciona durante un tiempo sin detenerse y a continuación realiza una llamada al sistema para leer de un archivo o escribir en un archivo. Cuando la llamada se completa, la CPU computa denuevo hasta que necesite más datos o tenga que escribir más datos y así sucesivamente. Nótese que algunas actividades de I/O cuentan como computación. Por ejemplo, cuando la CPU copia bits a una RAM de video para actualizar la pantalla, está computando, no haciendo I/O, porque la CPU está en uso. Los I/O en este sentido es cuando un proceso entra al estado de bloqueo esperando a que un dispositivo externo complete su trabajo.

![imagen2.39](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-39.png?raw=true)

Es importante notar que en el proceso mostrado en la figura 2-39(a), este se encuentra gran parte del tiempo computando, mientras que en el ejemplo, este se encuentra esperando por un dispositivo I/O.

El primero es llamado **compute-bound** o **CPU-bound**, el segundo es llamado **I/O bound**. 

Los procesos ligados a la computación suelen tener ráfagas largas de CPU, y por tanto, la espera de I/O es poco frecuente. Por otro lado, los procesos ligados a los I/O tienen ráfagas cortas de CPU y por tanto, esperas de I/O frecuentes. Tenga en cuenta que el factor clave es la duración de la ráfaga de CPU, no la duración de la ráfaga de I/O. Los procesos I/O bound son I/O bound porque no computan mucho entre peticiones I/O, no porque tengan peticiones I/O especialmente largas. Se tarda el mismo tiempo en emitir la petición de hardware para leer un bloque de disco, independientemente de lo mucho o poco que se tarde en procesar los datos después de que lleguen. 

Cabe destacar que a medida que las CPUs se hacen más rápidas, los procesos tienden a estar más ligados a los I/O. Este efecto ocurre porque las CPUs mejoran mucho más rápido que los discos. Como consecuencia es probable que la planificación de procesos ligados a I/O se convierta en un tema más importante en el futuro. La idea básica aquí es que si un proceso ligado a I/O quiere ejecutarse, debe tener una oportunidad rápidamente para que pueda emitir su petición de disco y mantener al disco ocupado.

Como vimos en la Fig. 2-6, cuando los procesos están ligados a la E/S, se necesitan bastantes de ellos para mantener la CPU totalmente ocupada.

#### When to Schedule

Un problema clave relacionado a la planificación es cuando se deben tomar decisiones de planificación. 
Resulta que cuando hay una gran variedad de situaciones en las que es necesario planificar. Primero, cuando un proceso es creado, se necesita decidir si se va a ejecutar el proceso padre o el hijo. Desde que ambos procesos se encuentran en estado listo, es una decisión normal de planificación y puede ir en cualquier dirección, es decir, el planificador puede elegir ejecutar el programa padre o el hijo.

Segundo, una decisión de planificación debe darse cuando se termine un programa. Este proceso ya no podrá ejecutarse, entonces otro proceso que se encuentre listo debe escogerse. Si no se encuentra algún proceso, normalmente se ejecuta un proceso inactivo suministrado por el sistema.

Tercero, cuando un proceso se bloquea en I/O, en un semáforo, o por alguna otra razón, debe escogerse otro programa para ejecutarse. Aveces el motivo del bloqueo puede influir en la elección. Por ejemplo, si A es un proceso importante y está esperando a que B salga de la región crítica, dejar que B se ejecute a continuación le permitirá salir de su región crítica y así dejar que A continúe. El problema, es que el planificador generalmente no tiene la información necesaria para tener estas dependencias en cuenta.

Cuarto, cuando una interrupcion I/O ocurre, se debe tomar una decisión de planificación. Si una interrupción viene de un I/O que ahora ha completado su trabajo, algún proceso que estaba bloqueado esperando la I/O puede estar ahora listo para ejecutarse. Es decisión del planificador quien va a ser el nuevo proceso a ejecturase, el proceso que se estaba ejecutando antes de la interrupción u otro proceso.

Si un clock en el hardware provee interrupciones periódicas en los 50 o 60 Hz o en alguna otra frecuencia, una decisión de planificación puede realizarse en cada interrupción de clock o cada *k* interrupciones de clock. Los algoritmos de planificación pueden dividirse en 2 categorías respecto a como lidiar con las interrupciones de clock. Un **nonpreemptive** algoritmo de planificación toma un proceso para ejecutar y luego simplemente lo deja correr hasta que se bloquea(ya sea por I/O o esperando otro proceso) o libera voluntariamente la CPU. Incluso si este es ejecutado por muchas horas, este no puede ser suspendido forzosamente. En efecto, no se realizan decisiones de planificación durante las interrupciones de clock. Una vez finalizado el procesamiento de la interrupción de reloj, se reanuda el proceso que se estaba ejecutando antes de la interrupción, a menos que un proceso de mayor prioridad estuviera esperando un tiempo de espera ya cumplido.

Por otro lado, un **preemptive** algoritmo de planificación toma un proceso y lo deja ejecutando por un máximo de tiempo determinado. Si dicho proceso se mantiene ejecutando hasta el final de este tiempo, este es suspendido y el planificador escoge otro proceso para ejecutarse(si alguno está disponible). Realizar este tipo de planificación preferente requiere tener una interrupcion de reloj al final de cada intervalo de tiempo para devolverle el control de la CPU al planificador. Si no hay reloj disponible, la programación no preferente es la única opción.

#### Categorías de algoritmos de planificación

No es de sorprender el hecho de que distintos casos requieren distintos algoritmos de planificación. Esta situación surge debido a las distintas areas de aplicación(y diferentes tipos de sistemas operativos)
requieren distintos objetivos. En otras palabras, lo que el algoritmo de planificación debe optimizar no es siempre lo mismo para todos los sistemas. Se distinguen 3 entornos:

-   Lote
-   Interactivo
-   Tiempo real

Los sistemas por lotes se siguen utilizando mucho en el mundo empresarial para hacer payroll, inventarios, cuentas por cobrar, cuentas por pagar, cálculo de intereses(en los bancos), tramitación de siniestros(en las aseguradoras) y otras tareas periódicas. En los sistemas por lotes, no hay usuarios impacientes esperando las respuestas rápidas a solicitudes breves. En consecuencia, los algoritmos no preferentes, o preferentes con largos periodos de tiempo para cada proceso, suelen ser aceptables. Este enfoque reduce los cambios de proceso y por tanto mejorará el rendimiento. Los algoritmos por lotes son en realidad bastante generales y a menudo aplicables también a otras situaciónes, lo que hace que merezca la pena estudiarlos.

En un entorno con usuarios interactivos, la preferencia es importante para mantener un proceso  acaparando la CPU y negar el servicio a los demás. Incluso si no hay procesos ejecutandose para siempre, un proceso podría dejar afuera a todos los demás indefinidamente debido a un bug del programa. La preferencia es necesaria para evitar este comportamiento. Los servidores también entran en esta categoría, ya que normalmente sirven a múltiples usuarios(remotos), todos ellos con mucha prisa. Los usuarios de ordenadores siempre tienen mucha prisa.

En los sistema con restricciones de tiempo real, curiosamente aveces no es necesaria la preferencia dado que los procesos saben que no pueden ejecutarse durante largos periodos de tiempo y suelen hacer su trabajo y bloquearse rápidamente. La diferencia con los sistemas interactivos es que los sistemas en tiempo real sólo ejecutan programas destinados a hacer avanzar la aplicación en cuestión. Los sistemas interactivos son de propósito general y pueden ejecutar programas arbitrarios que no son cooperativos e incluso que pueden ser maliciosos.

#### Scheduling Algorithm Goals

Con el fin de diseñar un algoritmo de planificación, es necesario tener idea de que tan bueno debe ser un algoritmo. Algunos objetivos dependen del entorno(lote, interactivo o tiempo real), pero algunos son deseables en todos los casos. Algunos objetivos son listados en la figura 2-40.

![imagen2.40](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-40.png?raw=true)

En cualquier caso, la equidad es importante. Los procesos comparabalbes deben recibir un servicio comparable. Dar a un proceso mucho más tiempo que a otro equivalente no es justo. Por supuesto, distintas categorías de procesos pueden ser tratados de manera diferente. Piense en el control de seguridad y en hacer las nóminas(payroll) en el centro informático de un reactor nuclear. Algo relacionado con la equidad es la aplicación de las políticas del sistema. Si la política local es que los procesos de control de seguridad se ejecuten cuando quieran, incluso si esto significa que la nómina se retrasa 30 segundos, el programador tiene que asegurarse que esta política se cumpla.

Otro objetivo general es mantener todas las partes del sistema ocupadas siempre que sea posible. Si la CPU y todos los dispositivos I/O pueden mantenerse en funcionamiento todo el tiempo, se realiza más trabajo por segundo que si algunos de los componentes están inactivos. En un sistema por lotes, por ejemplo, el programador tiene el control de qué trabajos se traen a memoria para ejecutarse.

Tener algunos procesos ligados a la CPU y algunos procesos ligados a los I/O en la misma memoria es una mejor idea que primero cargar y ejecutar todos los trabajos ligados a la CPU y cuando terminan, cargar y ejecutar todos los trabajos ligados a los I/O. Si se utiliza esta última estrategia, cuando los proceso ligados a la CPU se estén ejecutando, lucharán por la CPU y el disco estará inactivo. Más tarde cuando lleguen los trabajos ligados al I/O, lucharán por el disco y la CPU estará ociosa. Es mejor mantener todo el sistema funcionando a la vez mediante una cuidadosa mezcla de procesos.

Los responsables de grandes centros informáticos que ejecutan muchos trabajos por lotes suelen fijarse en 3 parámetros para comprobar el rendimiento de sus sistemas: rendimiento, tiempo de respuesta y utilización de CPU. **Throughput** es el número de trabajos por hora completados por el sistema. **Turnaround time** es el tiempo medio estadístico desde el momento en que se envía un trabajo por lotes hasta el momento en que se completa. Mide cuanto tiempo tiene que esperar un usuario promedio para obtener la salida.

Un algoritmo de planificación que trate de maximizar el rendimiento no necesariamente debe reducir el tiempo de respuesta. Por ejemplo, dado un conjunto de trabajos cortos y largos, un planificador que siempre ejecute trabajos cortos y nunca ejecute trabajos largos podría lograr un rendimiento excelente(muchos trabajos cortos por hora), pero a costa de un tiempo de respuesta terrible para los trabajos largos. Si los trabajos cortos siguieran llegando a un ritmo bastante constante, los trabajos largos podrían no ejecutarse nnunca, con lo que el tiempo de ejecución sería infinito y se lograría un alto throughput.

La utilización de la CPU se utiliza a menudo como métrica en los sistemas por lotes. Sin embargo, no es una buena métrica. Lo que realmente importa es cuántos trabajos por hora salen del sistema(rendimiento) y cuánto tiempo se tarde en recuperar un trabajo(tiempo de respuesta). Utilizar el uso de CPU como métrica es como clasificar los autos en función de cuántas veces por hora gira el motor. Sin embargo, saber cuándo la utilización de la CPU es casi del 100% es útil para saber cuándo es el momento de conseguir más potencia de cálculo.

En el caso de los sistemas interactivos , los objetivos son distintos. El más importante es minimizar el tiempo de respuesta, es decir, el tiempo que transcurre entre la emisión de un comando y la obtención del resultado. En un ordenador personal en el que se esté ejecutando un proceso en segundo plano(por ejemplo, leyendo y almacenando correo electrónico de la red) una petición del usuario para iniciar un programa o abrir un archivo debe tener prioridad sobre el trabajo en segundo plano. Hacer que todas las peticiones interactivas vayan primero se percibirá como un buen servicio.

Un problema relacionado es **proportionality**. Los usuarios tiene una idea a menudo incorrecta del tiempo que deben tardar las cosas. Cuando una solicitud que el usuario percibe como compleja tarda mucho, los usuarios lo aceptan, pero cuando una solicitud que se percibe como sencilla tarda mucho, los usuarios se irritan. Por ejemplo, si hacer click en un icono que inicia la carga de un video de 500 MB a un servidor en la nube tarda 60 segudos, el usuario probablemente lo aceptará como un hecho porque no espera la carga tarde 5 segundos. El sabe que toma tiempo.

Por otro lado, cuando un usuario hace click en el icono que interrumpe la conexión con el servidor en la nube después de cargar el video, sus expectativas son diferentes. Si no se ha completado después de 30 segundos, el usuario probablemente se enojará y después de 60 segundos será peor. Este comportamiento se debe a la percepción común del usuario de que se supone que enviar muchos datos lleva mucho más tiempo que interrumpir la conexión. En algunos casos(como éste), el planificador no puede hacer nada sobre el tiempo de respuesta, pero en otros sí, especialmente cuando el retraso se debe a una mala elección del orden de los procesos.

Los sistemas en tiempo real tienen propiedades distintas que los sistemas interactivos, y por ende, diferentes objetivos de planificación. Están caracterizados por tener deadlines que deben o almenos deberían cumplirse. Por ejemplo, si un ordenador controla un dispositivo que produce datos a un ritmo regular, si no se ejecuta el proceso de recogida de datos a tiempo pueden perderse datos. Por tanto, la principal necesidad de un sistema en tiempo real es cumplir todos o la mayoría de los deadlines.

En algunos sistemas de tiempo real, especificamente en aquellos relacionados al mutimedia, la predictabilidad es importante. Perder algún deadline no es fatal, pero si el proceso de audio se ejecuta inadecuadamente, la calidad del sonido puede deteriorarse rápidamente. El video también es un problema, pero el oido es mucho más sensible a las fluctuaciones que el ojo. Para evitar este problema, la planificación de los procesos debe ser muy previsible y regular. En este capítulo estudiaremos los algoritmos de planificacion por lotes e interactivos.

### 2.4.2 Scheduling in Batch Systems

Es hora de pasar de las cuestiones generales de planificación a los algoritmos de planificación específicos. En esta sección analizaremos los algoritmos utilizados en los sistemas por lotes. Luego examinaremos sistemas interactivos y en tiempo real. Cabe señalar que algunos algoritmos se utilizan tanto en sistemas por lotes como interactivos.

#### First-Come, First-Served

Probablemente, el más sencillo de todos los algoritmos de planificación es el de orden de llegada no preferente. Con este algoritmo, a los procesos se les asigna la CPU en el orden que la solicitan. Básicamente, hay una única cola de procesos listos.
Cuando el primer trabajo entra en el sistema desde el inicio de la mañana, se inicia inmediatamente y se le permite funcionar el tiempo que quiera. No se interrumpe porque haya durado demasiado. A medida que entran los otros trabajos, se ponen al final de la cola. Cuando el proceso en ejecución se bloquea, se ejecuta a continuación el primer proceso de la cola. Cuando un proceso bloqueado está listo, como un trabajo recién llegado, se coloca al final de la cola, detrás de todos los procesos en espera.

Lo bueno de este algoritmo es que es fácil de entender y fácil de programa. También es justo en el mismo sentido en el que es justo asignar las escasas entradas para un concierto o los Iphones a las personas que están dispuestas a hacer la cola desde temprano. Con este algoritmo, una única lista enlazada lleva la cuenta de todos los procesos listos. Elegir un proceso para ejecutarlo sólo requiere retirar uno de la parte delantera de la cola. Añadir un nuevo trabajo o proceso desbloqueado solo requiere agregarlo al final de la cola.

Sin embargo, el orden de llegada tambien tiene una gran desventaja. Supongamos que hay un proceso de cálculo que se ejecuta durante 1 segundo cada vez y muchos procesos de I/O que utilizan poco tiempo de CPU, pero cada uno tiene que realizar 1000 lecturas de disco para completar. El proceso de cálculo se ejecuta durante un segundo, luego lee un bloque del disco. Todos los procesos de I/O se ejecutan y comienzan a leer el disco.
Cuando el proceso de cálculo obtiene su bloque de disco, se ejecuta durante otro segundo, seguido por todos los procesos de I/O en rápida sucesión.

<<<<<<< HEAD
El resultado neto es que cada proceso de I/O puede leer un bloque por segundo y tardará 1000 segundos en terminar. Con un algoritmo de planificación que se adelantara al proceso de cálculo cada 10 mseg, los procesos de I/O terminarían en 10 seg en lugar de 1000 seg y sin ralentizar mucho el proceso de cálculo.
=======
El resultado neto es que cada proceso de I/O puede leer un bloque por segundo y tardará 1000 segundos en terminar. Con un algoritmo de planificación que se adelantara al proceso de cálculo cada 10 mseg, los procesos de I/O terminarían en 10 seg en lugar de 1000 seg y sin ralentizar mucho el proceso de cálculo. 
>>>>>>> 9bf386f (update L3)

#### Shortest Job First

Veamos ahora otro algoritmo por lotes no preventivo que asume que los tiempos de ejecución se conocen de antemano. En una compañía de seguros, por ejemplo, la gente puede predecir con bastante exactitud cuánto tardará en tramitarse un lote de 1000 reclamaciones, dado que se realiza un trabajo similar todos los días. Cuando hay varios trabajos igual de importantes en la cola de entrada esperando a ser iniciados, el planificador escoge primero el **shortest job first**. Veamos la figura 2-41, acá podemos encontrar 4 trabajos A, B , C y D con tiempos de ejecución de 8, 4, 4 y 4 minutos respectivamente. Si ejecutamos estos procesos en dicho orden, el tiempo de respuesta para A es de 8 minutos, para B es de 12 minutos, para C es de 16 minutos y para D es de 20 minutor con un promedio de 14 minutos.

![imagen2.41](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-41.png?raw=true)


Ahora consideremos ejecutar estos 4 procesos usando el shortest job first, como se ve en la figura 2-41(b). Los tiempos de respuesta ahora son de 4, 8, 12 y 20 minutos con un promedio de 11 minutos. Shortest job first es probablemente óptimo. Consideremos el caso de 4 trabajos coin tiempos de ejecución de a, b, c y d respectivamente. El primer trabajo termina con un tiempo a, el segundo con a +b y así sucesivamente. El promedio de tiempo de espera es de (4a + 3b + 2c +d)/4. Es claro que a contribuye más al promedio  que los otros tiempos, entonces tiene que ser el trabajo más corto, seguido de b, luego c y finalmente d como el más largo dado que este afectá más al tiempo promedio. Los mismos argumentos aplican para cualquier número de trabajos.

Cabe señalar que el trabajo más corto primero solo es óptimo cuando todos los trabajos están disponibles simultáneamente. Veamos un contraejemplo, consideremos 5 trabajos de A hasta E con tiempos de trabajo 2, 4, 1, 1 y 1 respectivamente. Sus tiempos de llegada con 0,0,3,3 y 3. Inicialmente solo A y B pueden ser reprogramados, dado que los otros trabajos todavía no están disponibles. Usando el trabajo más corto primero, vamos a ejecutar los trabajos en el orden A,B, C, D, E con un promedio de 4.6. De todos modos, ejecutar en el orden B, C, D, E, A tiene un promedio de 4.4.

#### Shortest Remaining Time Next

Una versión preferente del shortest time first es el **shortest remaining time next**. Con este algoritmo, el planificador siempre va a escoger al proceso cuyo tiempo de ejecución restante es el más corto. También en este caso, el tiempo de ejecución debe conocerse de antemano. Cuando llega un nuevo trabajo, su tiempo total se compara con el tiempo restante del proceso actual. Si el nuevo trabajo necesita menos tiempo para terminar que el proceso actual, se suspende el proceso actual y se inicia el nuevo trabajo. Este esquema permite que los nuevos trabajos cortos obtengan un buen servicio.

### 2.4.3 Scheduling in Interactive Systems

Vamos a ver algunos algoritmos que pueden ser usados en sistemas interactivos. Estos son comunes en computadoras personales, servidores y otros tipos de sistemas.

#### Round-Robin Scheduling

Uno de los algoritmos más antiguos, sencillos, justos y utilizados es el **round robin**. A cada proceso se le asigna un intervalo de tiempo, llamado **quantum**, durante el cual puede ejecutarse. Si el proceso sigue en marcha al final del quantum, la CPU se adelanta y se da a otro proceso. Si el proceso es bloqueado o es finalizado antes de que el quantum haya pasado, el cambio de CPU se da cuando el procesose haya bloqueado o finalizado. Round robin es fácil de implementar. Todo lo que el planificador necesita hacer es mantener una lista de procesos ejecutables, como se muestra en la Fig. 2-42(a). Cuando el proceso agota su quantum, se coloca al final de la lista, como se muestra en la Fig. 2-42(b). El único problema realmente interesante del round robin es la duración del quantum. Pasar de un proceso a otro requiere cierto tiempo para realizar toda la administración- guardar y cargar registros y mapas de memoria, actualizar varias tablas y listas, vaciar y recargar la memoria caché, etc.

![imagen2.42](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-42.png?raw=true)

Supongamos que este **process switch** o **context switch**, como es llamado aveces, toma 1 msec, incluyendo cambiar los mapas de memoria, vaciar y recargar la caché, etc. También supongamos que un quantum es de 4msec. Con estos parámetros, después de realizar 4msec de arduo trabajo, la CPU tendrá que gastar(es decir, desperdiciar) 1 mse en el cambio de proceso. Entonces 20% del tiempo en CPU se tomará como overhead administrativo. Esto es demasiado.

Para mejorar la eficiencia, cambiemos el quantum a 100 msec. Ahora el tiempo de desperdicio es del 1%. Pero piense en lo que ocurre en un sistema servidor si entran 50 peticiones en un intervalo de tiempo muy corto y con requisitos de CPU muy variables. Si la CPU está inactiva, la primera va a comenzar inmediatamente, la segunda no va a comenzar hasta 100 msec después. El desafortunado último puede tener que esperar 5 segundos antes de tener una oportunidad, suponiendo que todos los demás utilicen todos sus quantums. La mayoría de usuarios percibirán como lenta una solicitud de 5 segundos a una orden corta. Esta situación es especiamente mala si algunas de las peticions cercanas al final tienen solo unos milisegundos de tiempo de CPU. Con un quantum corto, habrían obtenido un mejor servicio.

Otro factor es que si el quantum se fija más largo que la ráfaga media de la CPU, el adelantamiento no ocurrirá muy a menudo. En su lugar, la mayoría de los procesos realizarán una operación de bloqueo antes de que se agote el quantum, provocando un cambio de proceso. La eliminación del adelantamiento mejora el rendimiento porque los cambios de proceso sólo se producen cuando son lógicamente necesarios, es decir, cuando un proceso se bloquea y no puede continuar.

La conclusión se puede formular como: tener un quantum muy corto puede causar muchos cambios de procesos, los cuales pueden disminuir la eficiencia de la CPU, pero tener quantums muy largos pueden causar una pobre respuesta a solicitudes de procesos interactivos cortos. Un quantum alrededor de 20-50 msec es una opción razonable.

#### Priority scheduling

Round-robin scheduling asume implícitamente que todos los procesos son igual de importantes. Con frecuencia, las personas que poseen y manejan ordenadores multiusuario tienen ideas muy distintas al respecto. En una universidad, por ejemplo, el orden jerárquico puede ser primero el presidente, luego los decanos de la facultad, después los profesores, las secretarias, los conserjes y por último, los estudiantes. La necesidad de tener en cuenta factores externos conduce a la programación prioritaria. La idea básica es sencilla: a cada proceso se le asigna una prioridad, y el proceso ejecutable con la prioridad más alta puede ejecutarse. 

Incluso en una PC con un único propietario, pueden haber múltiples procesos, algunos de ellos más importantes que otros. Por ejemplo, a un proceso daemon que envía correo electrónico en segundo plano se le debe asignar una prioridad menor que a un proceso que muestra una película de video en la pantalla en tiempo real.

Para prevenir que los procesos de alta prioridad se ejecuten indefinidamente, el planificador puede reducir la prioridad del proceso actual en cada tick de clock(por ejemplo clada clock interrupt). Alternativamente, a cada proceso se le puede asignar un quantum de tiempo máximo que se le permite ejecutar. Cuando este quantum se agota, el siguiente proceso de mayor prioridad tiene la oportunidad de ejecutarse.

Las prioridades pueden ser asignadas a los procesos de forma estática o dinámica. En una computadora militar, los procesos iniciados por los generales podrían comenzar en prioridad 100, los procesos iniciados por los coroneles podrían comenzar en prioridad 90, mayores en 80, capitanes con 70, tenientes con 60 y así sucesivamente. Alternativamente, un centro comercial computacional puede tener procesos de alta prioridad con un costo de 100$ la hora, prioridad media 75$ la hora, y la baja prioridad 50$ la hora.  

El sistema UNIX tiene un comando, nice, que permite a un usuario reducir voluntariamente la prioridad de su proceso, para ser amable con los demás usuarios. Nadie
lo utiliza nunca.

Las prioridades pueden ser asignadas dinámicamente por el sistema para lograr ciertos objetivos del sistema. Por ejemplo, algunos procesos están muy ligados al I/O y pasan la mayor parte de su tiempo esperando a que se complete el I/O. Siempre que un proceso de este tipo quiera la CPU, se le debe dar la CPU inmediatamente, para permitirle iniciar su siguiente petición de I/O, que puede entonces proceder en paralelo con otro proceso que esté realmente computando. Hacer que el proceso de I/O espere mucho tiempo por la CPU, solo significa tenerlo ocupando memoria durante un tiempo innecesariamente largo. Un algoritmo simple para dar un buen servicio a los procesos vinculados a los I/O es setear la prioridad en 1/f donde f es la fracción del último quantum que utilizó un proceso. Un proceso que solo usó 1 msec de los 50 msec de quantum, tendrá una prioridad de 50, mientras que un proceso que usó 25 msec antes de bloquearse obtendrá una prioridad 2 y el proceso que usó todo el quantum tendrá una prioridad 1.

A menudo es conveniente agrupar los procesos en clases prioritarias y utilizar la programación prioritaria entre las clases, pero la programación round-robin dentro de cada clase. La figura 2-43 muestra un sistema con 4 clases de prioridad. 

![imagen2.43](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-43.png?raw=true)

El algoritmo de programación es el siguiente: mientras haya procesos ejecutables en la clase de prioridad 4, ejecute cada uno de ellos durante un quantum, de forma round-robin y no se moleste nunca con las clases de prioridad inferior. Si la clase de prioridad 4 está vacía, ejecute los procesos de la clase 3 con round robin. Si las clases 4 y 3 están vacías, entonces ejecute la clase 2 en round robin y así sucesivamente. Si las prioridades no se ajustan de vez en cuando, las clases de prioridad pueden morir de hambre.

#### Multiple Queues

Uno de los primeros planificadores de prioridades fue el CTSS, el M.I.T. Compatible TimeSharing System que funcionaba en el IBM 7094 (Corbato'et al., 1962). CTSS tenía el problema de que la conmutación entre procesos era lenta porque el 7094 sólo podía contener un proceso en memoria. Cada cambio significaba intercambiar el proceso actual al disco y leer uno nuevo del disco. Los diseñadores de CTSS se dieron cuenta rápidamente que era más eficiente dar a los procesos ligados a la CPU un quantum más grande de vez en cuando, en lugar de darles quantums pequeños frecuentemente(para reducir el intercambio). Por otro lado, dar a todos los procesos un quantum grande puede significar tener un tiempo de respuesta pobre, como acabamos de ver. Su solución fue implementar clases de prioridad. Procesos en la prioridad más alta tendrán todo un quantum para ejecutarse. Procesos en el siguiente nivel más alto tendrán un tiempo de ejecución de hasta 2 quantums, los siguiente procesos tendrán un tiempo máximo de 4 quantums y así sucesivamente. Cuando un proceso usa todo el quantum asignado, este desciende de clase. A modo de ejemplo, consideremos un proceso el cual necesita usar la CPU continuamente por 100 quanta. Inicialmente se le daría un quantum y luego cambiaría. La siguiente vez recibiría 2 quantums antes de ser intercambiado. En las siguientes obtendría 4, 8, 32 y 64 quantums, aunque solo hubiera utilizado 37 de los 64 quantums finales para completar su trabajo. Solo se necesitarían 7 cambios (incluyendo la carga inicial) en vez de 100 con un algoritmo puro de round-robin. Además, a medida que el proceso se hundiera más y más en las colas de prioridad, se ejecutaría cada vez con menos frecuencia, ahorrando a la CPU, procesos cortos e interactivos. 

Dicha política fue adoptada para evitar castigar a los procesos que tengan que ejecutarse por un tiempo muy largo cuando se iniciaron por primera vez, peor que se volvió interactivo más tarde. Siempre que se de un return(Enter) en la terminal, el proceso perteneciente a ese terminal fue movido a la clase de mayor prioridad, asumiendo que estaba a punto de convertirse en interactivo. Un día un usuario con un proceso que consumía mucha CPU descubrió que sentarse en la terminal y teclear carriage returns al azar cada pocos segundos, hacía maravillas con su tiempo de respuesta. Se lo contó a todos sus amigos. Ellos se lo contaron a todos sus amigos. Moraleja de la historia: hacerlo bien en la práctica es mucho más difícil que hacerlo al principio.

#### Shortest Process Next

Dado que los trabajos cortos siempre producen el menor tiempo de respuesta promedio para los sistemas de lotes, estaría poder usarlos en los procesos interactivos. Hasta cierto punto, es posible. Los procesos interactivos suelen seguir el patrón de esperar comando, ejecutar comandos, esperar comando, etc. Si consideramos la ejecución de cada comando como un "trabajo" independiente, podemos minimizar el tiempo de respuestas total ejecutando primero el más corto. El problema es averiguar cual de los procesos actualmente ejecutables es el más corto.

Un enfoque consiste en hacer estimaciones basadas en el comportamiento pasado y ejecutar el proceso con el tiempo de ejecución estimado más corto. Supongamos que el tiempo estimado por comando para un proceso es T0. Luego supongamos que su siguiente instrucción es T1. Podríamos actualizar nuestra estimación tomando una suma ponderada de estos 2 números, es decir a*T0 + (1 - a)T1. Mediante la elección de a podemos decidir que el proceso de estimación olvide rápidamente las ejecuciones antiguas o que las recuerde por mucho tiempo. Con a = 1/2 obtenemos la siguiente sucesión

    T0, T0/2 + T1/2, T0/4 + T1/4 + T2/2, T0/8 + T1/8 + T2/4 + T3/2

Después de 3 neuvos turnos, el peso de T0 ha disminuido a 1/8. La técnica de estimar el siguiente valor de una serie tomando la media ponderada del valor medido actual y la estimación anterior se denomina aveces envejecimiento. Es aplicable a muchas situaciones en las que debe hacerse una predicción basada en valores anteriores. El envejecimiento es especialmente fácil de aplicar cuando a= 1/2. Basta con sumar el nuevo valor a la estimación actual y dividir la suma por 2(desplazamiento de un bita la derecha).

#### Guaranteed Scheduling

Un enfoque completamente distinto a la planificación consiste en hacer promesas reales a los usuarios sobre rendimiento y luego cumplirlas. Una promesa realista y fácil de cumplir es la siguiente: Si hay n usuarios conectados mientras estás trabajando, recibirás aproximadamente 1/n de la potencia de la CPU. De forma similar, en un sistema de single-user con n procesos ejecutándose, debería ser igual, cada proceso obtendría 1/n ciclos de CPU. Parece justo.

Para ser buenos en esta promesa, el sistema debe mantener la cuenta de cuanto CPU debe tener cada proceso desde que se crea. A continuación, calcula la cantidad de CPU a la que cada uno tiene derecho, es decir, el tiempo transcurrido desde la creación divido entre n. Dado que también se conoce la cantidad de tiempo de CPU que cada proceso ha tenido realmente, es bastante sencillo calcular la relación entre el tiempo de CPU real consumido y el tiempo de CPU al que tiene derecho. Un ratio de 0,5 significa que un proceso solo ha tenido la mitad de lo que debería haber tenido, y un ratio de 2,0 significa que un proceso ha tenido el doble de lo que le correspondía. El algoritmo consiste en ejecutar el proceso con el ratio más bajo hasta que su ratio supere al de su competidor más cercano. A continuación este es escogido para ejecutarse.

#### Lottery scheduling 

Mientras que algunos hacen promesas a los usuarios y luego cumplirlas es buena idea, es difícil de implementar. Sin embargo, se puede usar otro algoritmo para dar de manera resultados predecibles similares con una implementación mucho más simple. Esta es llamada **lottery scheduling**.

La idea básica es dar a los procesos tickets de lotería para varios recursos del sistema como el tiempo del CPU. Cuando una decisión de planificación es tomada, se toma un ticket de lotería de forma aleatoria, y el proceso que tiene el ticket es el que accede al recurso. Luego, hablando del planificador del CPU, este puede tener 50 tickets de lotería por segundo, con cada ganador con 20msec de tiempo en el CPU como premio.

Parafraseando a George Orwell: "Todos los procesos son iguales, pero algunos procesos son más iguales". Los procesos más importantes tendrán tickets extra, para incrementar sus posibilidades de ganar. Si hay 100 tickets pendientes y un proceso tiene 20 de esos, este tiene 20% de probabilidades de anar cada lotería. A largo plazo, este tiene 20% del tiempo en CPU. En contraste con el priority scheduler, donde es bastante dificil tener una prioridad de 40, aquí la regla es clara: un proceso que contiene una fracción f de los tickets, obtendrá aproximadamente una fracción f del recurso en cuestión.

Lottery scheduling tiene varias propiedades interesantes. Por ejemplo, si aparece un nuevo proceso y se le otorgan nuevos boletos, en la próxima lotería tendrá una posibilidad de ganar en proporción al número de entradas que posee. En otras palabras, la programación de lotería es altamente receptiva.

Los proces que cooperan pueden intercambiar tickets si ellos desean. Por ejemplo, cuando un proceso de cliente envia un mensaje a un proceso de servidor y luego se bloquea, este probablemente le haya dado todos sus tickets a dicho proceso para aumentar las posibilidades de que el servidor se ejecute después. Cuando el servidor finalice, este retorna los tickets al cliente  para que esta pueda continuar ejecutándose. En ausencia de los clientes, los servidores no necesitan tickets en absolutos.

Lottery scheduling puede ser usado para resolver problemas que son difíciles de manejar con otros métodos. Un ejemplo es un servidor de videos en el cual varios procesos  están alimentando secuencias de video a sus clientes, pero a diferentes velocidades de fotogramas. Supongamos que los procesos necesitan fotogramas a 10, 20, 35 fotogramas/seg. Mediante la asignación a estos procesos con 10, 20, 25 tickets respectivamente, ellos automáticamente dividirán a la CPU en la proporción correcta, la cual es: 10, 20 y 25.

#### Fair-Share Scheduling

Hasta ahora hemos asumido está planificado por su cuenta, sin importar quien sea su propietario. Como resultado si un usuario 1 comienza 9 procesos y un usuario 2 comienza un proceso, con round-robin o prioridads iguales el usuario 1 tiene 90% de la CPU y el usuario 2 solo el 10%.

Para evitar esta situación, algunos sistemas, tienen en cuenta qué usuario posee un proceso antes de planificarlo. En este modelo, cada usuario esta asignado con una fracción de la CPU y el planificador toma procesos de tal manera que se cumplan. Entonces si a 2 usuarios se les ha prometido el 50% de la CPU, cada uno obtendrá eso, no importa cuantos procesos tengan en existencia.

A modo de ejemplo, consideremos un sistema con 2 usuarios, a cada uno se le ha prometido el 50% de la CPU. El usuario 1 tiene 4 procesos, A, B, C y D, el usuario 2 tiene solo un proceso E. Si se usa el round robin, una posible secuencia de planificacion que cumple con todas las restricciones esta:

A E B E C E D E A E B E C E D E...

Por otro lado, si el usuario 1 tiene derecho al doble de tiempo de CPU que el usuario 2:

A B E C D E A B E C D E ..

Existen numerosas posibilidades, y pueden ser exploradas según la noción de equidad que se tenga.

### 2.4.4 Scheduling in Real-Time Systems

Un sistema **real-time** es uno en el cual el tiempo juega un rol esencial. Normalmente, uno o varios dispositivos físicos externos al ordenador generán estímulos y el ordenador debe reaccionar adecuadamente a ellos en un tiempo determinado. Por ejemplo, la computadora de un reproductor de discos compactos recibe los bits a medida que salen de la unidad y debe convertirlos en música en un intervalo de tiempo muy cerrado. Si el cálculo toma mucho tiempo, la música puede sonar peculiar. Otros sistemas de tiempo real son los monitores de los pacientes en la unidad de cuidados intensivos de un hospital, el piloto automático de un avión y el control de robots en una fábrica automatizada. En todos estos casos, tener la respuesta correcta, pero tenerla en un tiempo muy prolongado puede ser tan malo como no tenerla en absoluto.

Los sistemas en tiempo real se suelen clasificar en **hard real time**, es decir, que hay plazos absolutos que deben cumplirse y **soft real time**, es decir, que el incumplimiento ocasional de un plazo no es deseable, pero sí tolerable. En ambos casos, el comportamiento en tiempo real se consigue dividiendo el programa en varios procesos, cada uno de los cuales tiene un comportamiento predecible y conocido de antemano. Estos procesos suelen ser de corta duración y pueden completarse en menos de un segundo. Cuando se detecta un evento externo, la tarea del planificador consiste en planificar los procesos de forma en que se cumplan todos los plazos.

Los eventos a los que debe responder un sistema en tiempo real pueden clasificarse en **periódicos** (es decir, que se producen a intervalos regulares) o **aperiódicos**(es decir, que se producen de forma impredecible). Un sistema puede tener que responder a múltiples flujos de eventos periódicos. Dependiendo del tiempo que requiera el procesamiento de cada evento, puede que ni siquiera sea posible gestionarlos todos.Por ejemplo, si hay m eventos periódicos y el evento i ocurre con un periodo Pi y requiere Ci segundos de CPU para manejar cada evento, entonces la carga sólo se puede manejar si.

 m
 Σ Ci/Pi ≤ 1
i=1

Un sistema en tiempo real que tenga este criterio es llamado **planificable**. Esto significa que puede ser implementado. Un proceso que falla ante este test no puede ser programable dado que el tiempo total de CPU que el proceso busca colectivamente es mayor que el que el CPU puede brindar.

Como ejemplo, consideremos un soft real-time system con 3 eventos periódicos, con periodos de 100, 200 y 500 msec, respectivamente. Si este sistema necesitara 50, 30 y 100 msec de tiempo en CPU por evento. El sistema es planificable dado que 0.5 + 0.15 + 0.2 < 1. Si un cuarto evento de un periodo de 1 sec es añadido, el sistema va a seguir siendo planificable hasta que este evento no necesite más de 150 msec de tiempo en CPU por evento. Se asume implícitamente que la sobrecarga de cambio de contexto es tan pequeña que se puede ignorar.

Los algoritmos de planificación real-time pueden ser estáticos o dinámicos. Estos últimos toman sus decisiones de planificación en tiempo de ejecución, una vez iniciada la ejecución. La planificación estática solo funciona cuando se dispone de información perfecta de antemano sobre el trabajo que hay que hacer y los plazos que hay que cumplir. Los algoritmos de programación dinámica no tienen estas restricciones.

Hasta ahora, hemos asumido tácticamente que todos los proceso del sistema pertenecen a distintos usuarios y, por tanto, compiten por la CPU. Aunque esto suele ser cierto, a veces ocurre que un proceso tiene muchos hijos ejecutándose bajo su control. Por ejemplo, un proceso de sistema de gestión de base de datos puede tener muchos hijos. Cada hijo puede estar trabajando en una solicitud diferente, o cada uno puede tener alguna función específica que realizar(análisis de consulta de datos, acceso a disco, etc.).
### 2.4.5 Policy Versus Mechanism

Hasta ahora, hemos asumido tácticamente que todos los procesos del sistema pertenecen a distintos usuarios y por tanto, compiten por la CPU. Aunque aveces esto es cierto, puede suceder que un proceso tiene muchos hijos ejecutándose bajo su control. Por ejemplo, una proceso de gestión de bases de datos puede tener muchos hijos. Cada uno puede estar tratando una solicitud diferente, o cada uno puede tener alguna función específica que realizar(análisis de consultas, acceso a disco, etc). Es perfectamente posible que el proceso principal tenga una idea excelente de cuales son sus hijos más importantes(o los que más tiempo consumen) y cuáles son los menos importante. Sin embargo, ninguno de los planificadores mencionados acepta información de los procesos de usuario sobre las decisiones de planificación. Como resultado, el planificador no siempre toma la mejor decisión.

La solución a este problema es separa el **scheduling mechanism** de la **scheduling policy**, un principio establecido desde hace tiempo. Esto significa que el algoritmo de planificación está parametrizado de alguna manera, pero los parámetros pueden ser completados por los procesos del usuario. Consideremos un nuevo ejemplo de la base de datos. Supongamos que el núcleo utiliza un algoritmo de planificación por prioridades, pero proporciona una llamada al sistema mediante la cual un proceso puede establecer y cambiar las prioridades de sus hijos. De este modo, el padre puede controlar como se planifican sus hijos, aunque el mismo no realice la programación. En este caso, el mecanismo está en el núcleo, pero la política la establece un proceso de usuario. Policy-mechanism es una gran idea.

### 2.4.6 Thread Scheduling

Cuando varios procesos tienen varios hilos cada uno, tenemos 2 niveles de paralelismo presentes: proceso e hilos. La planificación de estos sistemas difiere sustancialmente dependiendo de si se soportan hilos a nivel usuario o a nivel del kernel(o ambos).

Primero consideremos los hilos a nivel de usuario. Dado que el kernel no es consciente de la existencia de los hilos, opera como siempre lo hace, eligiendo un proceso, digamos, A y dándole a A el control de su quantum. El planificador de hilos en A decide que hilo va a ejecutar, digamos A1. Dado que no hay interupciones de reloj para los hilos multiprograma, este hilo continuará ejecuntandose todo el tiempo que quiera. Si utiliza todo el quantum del proceso, el núcleo seleccionará otro para ejecutarlo.

Cuando el proceso A se vuelva a ejecutar, el hilo A1 va a continuar ejecutándose. Este continuará hasta que el tiempo de A se termine. Sin embargo, su comportamiento antisocial no afectará a otros procesos. Ellos obtendrán lo que el planificador considere que le corresponde, sin importar lo que ocurra en el proceso A. 

Ahora consideremos el caso de que los hilos del proceso A tienen relativamente poco trabajo que hacer por ráfaga de CPU, por ejemplo 5 msec de trabajo dentro de un quantum de 50msec. En consecuencia, cada uno se ejecuta durante un rato, y luego cede a la CPU de nuevo al programador de hilos.

Esto puede llevar a la secuencia A1, A2, A3, A1, A2, A3, A1, A2, A3, A1 antes de que el kernel cambie al proceso B. Esto se evidencia en la figura 2-44(a)

![imagen2.44](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-44.png?raw=true)

El algoritmo de planificación usado por el sistema en tiempo de ejecución puede ser cualquiera de los escritos anteriormente. En la práctica, los algoritmos más comunes de planificación son la planificación round-robin y la planificación con prioridades. La única restricción es la ausencia de un reloj para interrumpir a un hilo que se ha ejecutado demasiado tiempo. Dado que los hilos cooperan, esto no suele ser un problema. 

Consideremos ahora la situación con threads a nivel del kernel. En este caso, el núcleo elige un hilo concreto para ejecutarlo. No tiene que tener en cuenta a qué proceso le pertenece el hilo, pero puede hacerlo si quiere. Al hilo se le da un quantum y se suspende forzosamente si excede el quantum. Con un quantum de 50 mseg pero con hilos que se bloquean cada 5msec, el orden de los hilos durante algún periodo de 30 mseg podría ser A1, B1, A2, B2, A3, B3, algo imposible con estos parámetros y los hilos a nivel de usuario. Esta situación se presenta parcialmente en la fig 2-44(b)

Una diferencia importane entre los hilos a nivel de usuario y los hilos a nivel de kernel es el rendimiento. Hacer un cambio de hilos con hilos a nivel de usuario, requiere un puñado de instrucciones de máquina. Con los hilos a nivel de kernel se requiere un cambio de contexto completo, cambiando el mapa de memoria e invalidando la caché, lo que son varias ordenes de magnitud más lenta. Por otro lado, con los hilos a nivel del kernel, tener un hilo bloqueado en I/O no suspende todo el proceso como ocurre con los hilos a nivel de usuario.

Como el núclo sabe que pasar un hilo en el proceso A a un hilo en el proceso B es más caro que ejecutar un segundo hilo en el proceso A(debido a que tener que cambiar el mapa de memoria y se estropea la memoria cache), puede tener en cuenta esta información a la hora de tomar una decisión. Por ejemplo, dado 2 hilos de igual importancia, uno de ellos pertenece al mismo proceso que un hilo que acaba de bloquearse y otro perteneciente a un hilo diferente, se podría dar preferencia al primero.

Otro factor importante es que los hilos a nivel de usuario pueden enplear un planificador de hilos especifícos de la aplicación. Consideremos, el servidor de la figura 2-8. Supongamos que un hilo trabajadora caba de bloquearse y que el hilo despachador y 2 hilos trabajadores están listos. ¿Quién debe ser el siguiente en ejcutarse? El sistema en tiempo de ejecución, sabiendo lo que hacen todos los hilos, puede elegir fácilmente el despachador para que se ejecute a continuación, de modo que pueda iniciar otro trabajador en ejecución. Esta estrategia maximiza la cantidad de paralelismo en un entorno en el que los trabajadores se bloquean frecuentemente en I/O de disco. Con hilos a nivel de núcleo, el núcleo nunca sabría lo que hace cada hilo(aunque se les podría asignar distintas prioridades). En general, los planificadores de hilos específicos de la aplicación pueden ajustar una aplicación mejor que el núclo.