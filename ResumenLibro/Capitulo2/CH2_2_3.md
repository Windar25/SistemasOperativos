# 2.3 INTERPROCESS COMUNNICATION

Los procesos se comunican constantemente con otros procesos. Por ejemplo, en el shell pipeline, el output
del proceso 1 debe pasarse al proceso 2 y así sucesivamente. Por ende la comunicación de procesos es 
necesaria, de preferencia en una forma bien estructurada que no utilice interrupciones.

En la seccion siguiente vamos a ver algunos problemas relacionados a la comunicación entre procesos
o IPC.

### Problemas

El primer problema es ¿cómo un proceso puede enviar información a otro?.  El segundo está relacionado a 
asegurarse que 2 procesos no se estorben, por ejemplo, 2 procesos de un sistema de aerolínea intentando
conseguir el último asiento de un avión para un cliente diferente. La tercera se refiere a la 
secuencia adecuada cuando existen dependencias. Si el proceso A produce datos y el proceso B los imprime
B debe esperar que A termine de producir los datos antes de empezar a imprimir. 2 de estos procesos
se aplican igualmente a los hilos o threads. 

El primero es fácil para los hilos que comparten espacio de memoria. Los otros 2 pueden ser resueltos con 
threads. El mismo problema existe y se aplicará la misma solución, esto relacionado con hilos.

## 2.3.1 Race Conditions

En algunos sistemas operativos, los procesos que trabajan juntos comparten un mismo almacenamiento del cual
pueden leer y escribir. El almacenamiento compartido puede estar en la memoria principal(posiblemente en
una estructura de datos del kernel) o puede estar en un archivo compartido. La ubicación de la memoria compartida
no cambia la naturaleza del comunicación el problema que genera. Veamos un ejemplo: una cola de impresión.
Cuando un proceso a va realizar una impresión ingresa el nombre del file en un spooler directory. 
Otro proceso el printer daemon, revisa periodicamente si  hay algún file en ese lugar para ver si se necesita
alguna impresión, si existe, lo imprime y luego elimina su nombre del directorio.

Imaginemos que nuestro spooler directory tiene un numero muy largo de slots, cada uno capaz de tener un nombre
de archivo. Ahora, imaginemos que tenemos 2 variables compartidas, out, la cual apunta al siguiente archivo
a imprimir, e in, el cual apunta a un lugar libre del directorio. Estas 2 variables podrían guardarse
en un archivo de 2 palabras disponible para todos los procesos. En un momento dado, las ranuras 0 a 3 están vacías (los ficheros 
ya se han impreso) y las ranuras 4 a 6 están llenas (con los nombres de los ficheros en cola de impresión). Más o menos 
simultáneamente, los procesos A y B deciden poner en cola un fichero para imprimir. Esto se ve en la figura 2-21.

![imagen2.21](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-21.png?raw=true)
En jurisdicciones donde la ley de Murphy es aplicable, podría ocurrir lo siguiente. El proceso A lee y almacena el valor, 7, 
en una variable local llamada next_free_slot. Justo en ese momento se produce una interrupción del reloj y la CPU decide que el 
proceso A ya ha funcionado lo suficiente, por lo que pasa al proceso B. El proceso B también lee y también obtiene un 7. 
También lo almacena en su variable local next_free_slot. En este instante ambos procesos piensan que la siguiente ranura 
disponible es el 7. 

El proceso B continúa ejecutándose. Almacena el nombre de su archivo en la ranura 7 y lo actualiza para que 
sea un 8. Luego se va y hace otras cosas.

Finalmente, el proceso A se ejecuta de nuevo, comenzando desde el lugar donde lo dejó. Mira la siguiente ranura libre, encuentra 
un 7 allí, y escribe su nombre de archivo en la ranura 7, borrando el nombre que el proceso B acaba de poner allí. A continuación, 
calcula la siguiente ranura libre + 1, que es 8, y la coloca en 8. El directorio del spooler es ahora internamente consistente, 
por lo que el daemon de la impresora no notará ningún error, pero el proceso B nunca recibirá ninguna salida.

Situaciones como ésta, en la que dos o más procesos están leyendo o escribiendo algunos datos compartidos y el resultado final 
depende de quién los ejecuta precisamente cuándo, se denominan race conditions. Por desgracia, con el aumento del paralelismo 
debido al incremento del número de núcleos, los race condition son cada vez más comunes. Además los resultados de las 
depuraciones en programas con race condition pueden ser correctos en su mayoría, pero de vez en cuando suceden cosas raras,
difíciles de explicar.


## 2.3.2 Critical Regions

¿Cómo evitamos las race condition? La regla para prevenir problemas acá y en otras situaciones que tengan que ver con memoria, archivos y todo loo que sea compartido es prohibir a uno o mas procesos la lectura y escritura de datos simultanea. En otras palabras, necesitamos **exclusión mutua**, esto es una forma de asegurar de que mientras un proceso está usando una variable o archivo compartido, los demás procesos van a ser excluidos de usar el mismo file o variable. El problema anterior surgió dado que el proceso B comenzó a usar una variable compartida antes de que el proceso A termine de usarla. La elección de las operaciones primitivas apropiadas para lograr la exclusión mutua es una cuestión de diseño importante en cualquier sistema operativo, y un tema que examinaremos con gran detalle en las siguientes secciones.

El problema de evitar las race condition puede ser formulada de forma abstracta. Parte del tiempo, un proceso está ocupado realizando cálculos  u operaciones internas que no nos llevan a race condition. De todos modos, el proceso pued acceder a memoria o archivos compartidos, o algún lugar crítico que nos lleve a la condición de carrera. Esa parte del programa donde la memoria compartida es accedida es llamada **critical region** o **critical section**. Si pudiéramos organizar las cosas de forma que nunca hubiera dos procesos en sus regiones críticas al mismo tiempo, podríamos evitar las carreras.

Aunque este requisito evita las condiciones de carrera, no es suficiente para que los procesos paralelos cooperen correcta y eficientemente utilizando datos compartidos. Necesitamos que se cumplan cuatro condiciones para tener una buena solución: 
1.  No deben haber 2 procesos simultáneamente en sus regiones críticas.
2.  No se pueden hacer suposiciones sobre las velocidades o el número de CPU.
3.  Ningún proceso que esté afuera de su sección crítica debe poder bloquear algún proceso.
4.  Ningún proceso debería tener que esperar eternamente para entrar en su región crítica.

Este comportamiento es el que queremos mostrar en la figura 2-22. Acá el proceso A entra a la región crítica en el momento T1. Tiempo después en un tiempo B, el proceso B trata de entrar a la región crítica, pero falla porque otro proceso ya está en la región crítica y solo vamos a permitir que solo uno acceda a la vez. Consecuentemente, B es suspendido temporalmente, hasta un tiempo T3, donde A deje su región crítica, permitiendo a B entrar inmediatamente. Eventualmente, B deja la sección crítica en un tiempo T4 y nos encontramos en la situación inicial de no tener proceso en la región crítica.

![figura2.22](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-22.png?raw=true)

## 2.3.3 Mutual Exclusion with Busy Waiting

En esta sección vamos a examinar varias propuestas para lograr la exclusión mutua de tal modo que 
cuando un proceso se encuente en su región crítica, otro proceso entre y cause problemas.

### Deshabilitar interruptores
En un sistema de un solo procesador, la solución más simple es tener deshabilitado los interruptores de los
demás procesos, mientras nuestro proceso se encuentre en su zona crítica, al finalizar nuestro proceso, se volverán
a habilitar los interruptores. Con las interrupciones deshabilitadas no existirán interrupciones de clock.
La CPU solo cambia de proceso en proceso como resultado del clock o de interrupciones, con las interrupciones 
desactivadas la CPU no se cambiará a otro proceso. Entonces, un proceso que deshabilita las interrupciones puede
puede examinar y actualizar la memoria compartida sin miedo a que otro proceso intervenga.
Este enfoque es generalmente poco atractivo porque no es prudente dar a los procesos de usuario el poder de desactivar 
las interrupciones. ¿Qué pasaría si alguno de ellos nunca las reactiva? Esto sería el fin del sistema.

Además, si el sistema es un multiprocesador (con dos o más CPUs) desactivar las interrupciones afecta sólo a 
la CPU que ejecutó la instrucción de desactivación. Las demás seguirán funcionando y podrán acceder a la 
memoria compartida.

Por otro lado, con frecuencia es conveniente que el propio núcleo desactive las interrupciones durante unas 
pocas instrucciones mientras actualiza variables o especialmente listas. Si se produce una interrupción 
mientras la lista de procesos listos, por ejemplo, está en un estado inconsistente, podrían producirse 
*race condition*.

En conclusión, deshabilitar interrupciones puede ser útil pero no es apropiado como una exclusión general mutua
de mecanismos para los procesos de usuario.

Además, la posibilidad de deshabilitar interrupciones para responder a la race condition es cada vez menos eficiente, pues
las computadoras cada vez cuentan con más núcleos. Esto significa que si un núcleo desactiva sus interrupciones, esto no impide
que los demás núcleos puedan acceder a su espacio de memoria compartido, es decir, no evitan la race condition.


### Lock Variables

Una solución de software sería tener una única variable compartida (de bloqueo) iniciada en 0.
Si un proceso desea entrar a su critical region primero revisa el lock. Si el lock es 0, el proceso asigna 1
al lock y entra al critical region. Si el lock es 1, el proceso espera hasta que sea 0. Es decir, el 0 significa
que ningun proceso está en el critical region y 1 significa que algún proceso está en el critical region.

Desafortunadamente, esta idea contiene el mismo problema que en el spooler directory. Supongamos que un proceso
lee el lock y ve que es 0, antes de que setee el lock como 1, otro proceso comienza a ejecutarse y lee el lock en 0
y lo setea en 1. Cuando el primer proceso se reanude y setee el lock en 1, tendremos 2 procesos en la misma critical
region.

Ahora podrías pensar que podríamos evitar este problema leyendo primero el valor del bloqueo, y comprobándolo 
de nuevo justo antes de almacenarlo, pero eso realmente no ayuda. La carrera ocurre ahora si el segundo 
proceso modifica el bloqueo justo después de que el primer proceso haya terminado su segunda comprobación.

### Strict Alternation

Veamos un ejemplo en lenguaje C. En la figura 2-23 la variable tipo int turn, inicialmente puesta en 0, lleva 
la cuenta de a quién le toca entrar en la critical region y examina o actualiza la memoria compartida.
Inicialmente, el proceso 0 lee turn, ve que es 0 y entra a la región crítica. El proceso 1 ve que turn es 0 y
entra cíclicamente en un loop testeando turn para cuando este sea 1. Probar continuamente una variable hasta 
que aparezca algún valor se denomina *busy waiting*. Esto debe tratar de evitarse, dado que desperdicia tiempo
en la CPU. Solo es bueno usarlo cuando es necesario. El lock que produce el busy waiting es llamado spin lock.
Cuando el proceso 0 deja la critical region, este cambia el turn a 1, lo que permite al proceso 1 entrar al critical
region. Supongamos que el proceso 1 entra sale del critical region rápidamente, entonces ambos procesos
estarían en su zona no crítica, y se cambiaría el turn 0, evitando nuevamente que ambos procesos estén en la critical
region simultáneamente.

De repente, el proceso 0 termina su región no crítica y vuelve a la parte superior de su bucle. 
Desafortunadamente, no se le permite entrar en su región crítica ahora, porque el turno es 1 y el proceso 1 
está ocupado con su región no crítica. Se cuelga en su bucle while hasta que el proceso 1 pone turn a 0. 
Dicho de otra manera, tomar turnos no es una buena idea cuando uno de los procesos es mucho más lento que el 
otro.

![figura2.23](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-23.png?raw=true)

Esta situación viola la condición 3 expuesta anteriormente: el proceso 0 está siendo bloqueado por un proceso que no se encuentra 
en su región crítica.De hecho, esta solución requiere que los dos procesos se alternen estrictamente a la hora de entrar en sus 
regiones críticas, por ejemplo, en la cola de impresión de archivos. A ninguno de los dos se le permitiría 
poner en cola dos archivos seguidos. Aunque este algoritmo evita todas las carreras, no es realmente un 
candidato serio como solución porque viola la condición 3.

### Peterson Solution

Peterson descubrió una forma mucho más sencilla de lograr la exclusión mutua, con lo que la solución de 
Dekker quedó obsoleta. El algoritmon de Peterson es mostrado en la figura 2-24.

![imagen2.24](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-24.png?raw=true)

Antes de usar las variables compartidas, cada proceso llama a *enter_region* con su propio numero de proceso
0 o 1 como parámetro. Esta llamada puede causar una espera hasta que sea seguro entrar. Después de finalizar
con la variable compartida, el proceso llama al *leave_region* para indicar que ya terminó y así permitir
al otro proceso entrar si es que este lo desea.

Veamos un ejemplo, al inicio ningún proceso está en la critical region. Ahora el proceso 0 llama a 
*enter_region*. Este indica que está interesado y setea el array element a TRUE y coloca turno en 0.
Si el proceso 1 no está interesado *enter_region* retorna inmediatamente, en caso contrario, este proceso
también llama al *enter_region* y se quedará ahí hasta que interested[0] se torne a FALSE.
Y el evento solo puede suceder cuando el proceso 0 llame a *leave_region* para salir
de la region crítica.

Ahora veamos que pasa si ambos procesos llaman a *enter_region* simultáneamente.
Ambos van a setear su número de proceso en turn. El último que guarde su turno es el
que se va a contar, el primero se va a sobreescribir y se va a perder. Supongamos
que el proceso 1 guardó al último, entonces su turno es 1. Luego ambos procesos entrarán al while
statement, process 0 ejecuta el while 0 veces y entra a la región crítica.
El proceso 1 se queda en el loop y no entra a la región crítica hasta que
el proceso 0 salga de la región crítica.

### The TSL Intruction

Ahora veamos una propuesta que necesita una pequeña ayuda del hardware. Algunas
computadoras, especialmente las diseñadas con multiples procesadores en mente
tienen una instrucción como: 

        TSL RX,LOCK

(Test and Set Lock) que funciona de la siguiente manera:
Se lee el contenido en el memory word *lock* en el registro RX y se guarda un valor distinto
a 0 en la dirección de memoria *lock*. Se garantiza que las 
operaciones de lectura de la palabra y de almacenamiento en 
ella son indivisibles(ningun otro procesador puede acceder a la memory word hasta
que la instrucción finalice). La CPU ejecuta la instrucción TSL y bloquea el memory bus
para evitar que otras CPUs accedan a la direción de memoria hasta que se haya realizado la instrucción.

Cabe destacar que bloquear la memoria del bus es distinto a deshabilitar las interupciones.
Deshabilitar interrupciones para realizar un read en la memoria de una palabra seguido de un
write no previene que otro procesador en el bus puede acceder a la dirección de la palabra
entre la lectura y escritura. Es decir, deshabilitar interrupciones en un procesador
no las deshabilita en los demás procesadores. La única forma de evitar que los demás
procesadores accedan a la memoria de la palabra es cerrando el bus, el cual requiere
una facilidad de hardware(básicamente, una línea de bus que 
afirma que el bus está bloqueado y no está disponible para 
procesadores distintos del que lo bloqueó).

Para usar el TSL vamos a usar una variable compartida llamada *lock* para coordinar el acceso 
a la memoria compartida. Cuando el lock es 0, cualquier 
proceso puede ponerlo a 1 utilizando la instrucción TSL y 
luego leer o escribir en la memoria compartida. Cuando 
termina, el proceso vuelve a poner el bloqueo a 0 
utilizando una instrucción move normal.

Ahora, ¿cómo esta solución previene el race condition evitando el ingreso de 2
procesos a la misma región crítica?. La solución está en la figura 2-25.

![imagen2.25](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-25.png?raw=true)

Acá hay 4 instrucciones de subrutina en un lenguaje ensamblador ficticio.
La primera instrucción copia el antiguo valor de lock al registro y setea 1 al lock.
Luego, el antiguo valor es comparado con 0. Si no es 0, el lock se acaba de setear entonces
el programa vuelve al inicio y vuelve a testearlo. Tarde o 
temprano este será 0(cuando el proceso actualmente en su 
región crítica ha terminado con su región crítica), y la 
subrutina regresa, con el bloqueo establecido.

Limpiar el lock es simple, el programa solo guarda 0 en lock. No se necesita ninguna 
instrucción especial de sincronización. 

La solución al problema de la región crítica ahora es más fácil. Antes de entrar a la región crítica el proceso
llama a enter_region, el cual realiza busy waiting hasta que el lock se libere, luego adquiere el lock y retorna.
Después de dejar la región crítica el proceso llama a *leaves_region* y guarda el lock en 0.
Como todas las soluciones basadas en regiones críticas, los procesos deben llamar
*enter_region* y *leave_region* en los momentos correctos para que el método funcione.
Si un proceso hace trampa, la exclusión mutua va a fallar. En otras palabras las regiones
críticas funcionan solo si los procesos cooperan.

Una solución alternativa al STL es el XCHG el cual intercambia 
atómicamente el contenido de dos posiciones, por ejemplo, 
un registro y una palabra de memoria. El código se muestra 
en la Fig. 2-26, y, como puede verse, es esencialmente el 
mismo que la solución con TSL.

![imagen2.26](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-26.png?raw=true)

## 2.3.4 Sleep and Wakeup

Las soluciones mostradas anteriormente son correctas, pero tienen el problema del busy waiting.
Es decir, las soluciones proponen la siguiente estructura: Si un proceso desea entrar
a su critical region debe revisar si está permitido. En caso no esté el proceso se mantiene en un loop
hasta que se le dé el permiso.

Esto no solo desperdicia el tiempo del CPU, sino que puede tener efectos inesperados.
Consideremos una CPU con 2 procesos, H, con prioridad alta y L con menor prioridad.
Las reglas de programación son tales que H se ejecuta siempre que esté en estado listo.
En un cierto momento, cuando L está en la región crítica, H se encuentra listo para
ejecutarse(se completó una operacion I/O). H ahora se encuentra en busy waiting, pero 
como L nunca se programa mientras H se está ejecutando, L nunca tiene la oportunidad 
de salir de su región crítica, por lo que H hace un bucle eterno. Esta situación se 
denomina a veces problema de inversión de prioridad.

Veamos ahora algunos procesos de intercomunicación primitivos que bloquean en lugar de malgastar
tiempo en la CPU cuando no se les permite entrar a las regiones críticas.

Una de las más sencillas es el par de sleep y wakeup. Sleep es una llamada al sistema que 
hace que la llamada se bloquee, es decir, se suspenda hasta que otro proceso la despierte.
La llamada wakeup tiene un parámetro, proceso a ser despertado.
Alternativamente, tanto sleep como wakeup tiene cada uno un parámetro, una dirección de memoria
usada para emparejar sleeps con wakeups.

### The Producer-Consumer problem
----
Para ver como usar estos interprocesos veamos el ejemplo del productor y consumidor.
2 procesos comparten un buffer común. Uno de ellos, el productor, pone información
en el buffer y el otro, el consumidor, la toma(también se puede tomar el ejemplo con m productores
y n consumidores pero para facilitar la solución, veamos el caso de 1 a 1) .

El problema comienza cuando el productor quiere poner un nuevo item en el buffer, pero
este ya está lleno. La solución para el productor es ir a dormir, para luego despertarse
cuando el consumidor haya tomado uno o más items. De manera similar, si el consumidor quiere
remover un item del buffer y ve que este está lleno, este debe ir a dormir hasta que el productor
ponga algo en el buffer y así luego él se despierte.
Este enfoque suena simple, pero lleva al mismo caso de race conditions que vimos con el
spooler directory. Para mantener la cuenta de número de items en el buffer, necesitamos la variable
count. Si el número máximo de items en el buffer es N, el codigo del productor debe verificar
si el contador está en N. Si lo está, el productor se va a dormir, sino, el productor ingresa
otro item y actualiza el contador.

El código del consumidor es similar: primero revisa si el contador es 0, si lo es este se va a dormir, sino
remueve el item y disminuye el contador. Cada proceso revisa si el otro debe despertarse y si debe, lo despierta.
EL código para el productor y consumidor se encuentra en la figura 2-27.

Para expresar system calls como *sleep* o *wakeup* en C, las mostraremos como llamadas a rutinas de biblioteca.
No existen en la librería standar de C, pero se pueden usar en cualquier sistema que tenga estas systems calls.
Las funciones *insert_item* y *remove_item*, las cuales no se muestran 
, se encargan de la contabilidad de poner y sacar elementos de la memoria intermedia.



![imagen2.27](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-27.png?raw=true)

Ahora, respecto a la race condition, este puede ocurrir si el acceso a la variable count es sin restricciones.
En consecuencia podría pasar lo siguiente: El buffer está vacío y el consumidor acaba de leer
que el contador está en 0. En ese momento, el planificador de trabajos decide parar la ejecución del consumidor 
y comenzar a ejecutar al productor. El productor inserta un item en el buffer, incrementa el contador y se da cuenta que 
ahora es 1. Si el contador fue 0 hace unos momentos, entonces el consumidor va a hacer *sleep*, el productor llama
*wakeup* y "despierta" al consumidor.

Sin embargo, el consumidor no estaba durmiendo lógicamente aún, entonces
la señal de *wakeup* se pierde. Cuando sea el turno del consumidor, él va a leer valor del contador anteriormente
leído, va a encontrar que es 0 y se jecuta *sleep*. Ambos se irán a dormir para siempre.

La esencia de este problema es que se envía la señal de *wakeup* a una función que aún no duerme y por ende
esta señal se va a perder. Si esta no se perdiera, todo andaría bien en el programa. Una forma de arreglar esto
es modificar las reglas para añadir un *wakeup waiting bit*. Cuando se envía una señal
de wakeup a un proceso que sigue despierto, se setea este bit. Después cuando el proceso
intente ir a dormir, si el wakeup bit está en activo, este se apagará, pero el proceso
seguirá despierto. El wakeup waiting bit es una alcancía para guardar wakeup signals.
El consumidor limpia el waiting bit en cada iteración del loop.

Si bien el wakeup waiting bit funciona en este caso, esta solución no es útil para
3 o más procesos. Podríamos hacer otro parche y añadir un segundo bit de espera de 
despertar, o tal vez 8 o 32 de ellos, pero en principio el problema sigue ahí.

## 2.3.5 Semaphores

Esta era la situación en 1965, cuando E. W. Dijkstra (1965) sugirió utilizar una variable entera para contar el número de 
wakeups guardados para uso futuro. En su propuesta, se introdujo un nuevo tipo de variable, que denominó semáforo. Un 
semáforo podía tener el valor 0, indicando que no se había guardado ningún wakeup, o algún valor positivo si había uno 
o más wakeups pendientes.

Dijsktra propuso tener 2 operaciones para el semáforo: *down* and *up*(generalizacions de sleep y wakeup respectivamente).
La operación *down* en el semáforo, revisa si el valor es mayor a 0. Si lo es disminuye el valor(utiliza una activación almacenada)
y luego continua. Si el valor es 0, el proceso es puesto a dormir sin completar el down por el momento. Revisar el valor, cambiarlo
y posiblemente mandándolo a dormir, es hecho como una simple y única **atomic action**. Se garantiza que una vez que la operación
semaforo haya empezado, ningun otro proceso puede acceder al semáforo hasta que la operación se haya completado
o bloqueado. Esta atomicidad es muy esencial para solucionar problemas de sincronización y evitar race conditions.
Acciones atómicas, en las que un grupo de operaciones relacionadas se realizan todas sin interrupción o no se realizan en absoluto, 
son extremadamente importantes también en muchas otras áreas de la informática.

La operación *up* incrementa el valor del semáforo. Si uno más procesos estaban durmiendo en el semáforo, inhabilidatos para completar
su operación *down* iniciada anteriormente, uno de ellos, escogido por el sistema (aleatoriamente) es permitida para completar su función
down. Por lo tanto, después de usar un up en un semáforo con procesos durmiendo en él, el semáforo seguirá estando en 0, pero
habrá un proceso menos durmiendo en él. La operación de incrementar el semáforo y despertar algún proceso es indivisible.
Ningún proceso se bloquea al hacer un up, al igual que ningún proceso se bloquea al hacer un wakeup en el modelo anterior.

Estos términos se introdujeron por primera vez en el lenguaje de programación Algol 68.

### **Solving the Producer-Consumer Problem Using Semaphores**

Los semáforos solucionan la pérdida de wakeup mostrada en la figura 2-28. Para que esto funcione correctamente es necesario que
el semáforo sea implementado de una forma indivisible. La forma normal es implementar *up* y *down* como system calls, donde el sistema
operativo desactiva brevemente todas las interrupciones mientras comprueba el semáforo, lo actualiza y pone el proceso en reposo, 
si es necesario. Todas estas acciones toman solo pocas instrucciones, no se produce ningún daño al desactivar las interrupciones.
Si se están usando múltiples CPUs, cada semáforo debe ser protegido con una variable *lock*, con las
instrucciones TSL o XCHG usadas para asegurarse de que solo una CPU esté examinando el semáforo en un mismo momento.

Debemos entender que usar TSL o XCHG para evitar que varias CPUs accedan a un mismo semáforo es muy diferente de que el productor o el 
estén ocupados(busy waiting) esperando a que el otro vacíe o llene el búfer. Las operaciones de semáforo tomo solo pocos microsegundos, mientras que
el productor o consumidor pueden tomar mucho más.

![imagen2.28](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-28.png?raw=true)

Esta solución emplea 3 semáforos: uno llamado *full* para contar el número de slots que están llenos, otro llamado *empty* para contar el número
de slots que están vacíos y otro llamado *mutex* para asegurarse de que el productor y el consumidor no acceden al
mismo tiempo. *Full* es inicialmente 0, *empty* es inicialmente el número de slots del buffer y *mutex* es inicialmente 1. Los semáforos que se 
inicializan a 1 y son utilizados por dos o más procesos para garantizar que sólo uno de ellos puede entrar en su región crítica al mismo tiempo se denominan **binary semaphores**.

Si cada procesos realiza un down justo antes de entrar a su región crítica y realiza un up justo después de dejarla,
la exclusión mutua es garantizada.

Ahora que disponemos de una buena comunicación entre procesos, veamos la secuencia de interrupcion de la figura 2-5.
En un sistema con semáforos, el camino natural para ocultar interrupciones es tener un semáforo inicializado en 0 y asociado con
cada dispositivo I/O. Justo después de comenzar con el dispositivo I/O, el manejador de procesos realiza un down en el semáforo asociado
bloqueandolo inmediatamente. Cuando la interrupción retorne, el manejador de interrupciones realizará un up en el semáforo asociado, el cual permitirá
que el proceso relevante esté listo para ejecutarse nuevamente. En este modelo, el paso 5 en la figura 2-5 consiste en hacer up
en el semáforo correspondiente, para que en el paso 6 el planificador pueda ejecutar el administrador de dispositivos.
Por supuesto, si hay varios procesos listos, el planificador probablemente vaya a ejecutar el más importante. Vamos a ver algunos de estos
algoritmos más tarde en este capítulo.

En el ejemplo de la figura 2-28, estamos usando semáforos en 2 distintas formas. Es importante hacer explícita esta diferencia. El semaforo *mutex* es usado para exclusión
mutua. Está diseñado para garantizar que solo un proceso a la vez va a estar leyendo o escribiendo en el buffer y las variables asociadas.
Esta exclusión mutua es necesaria para evitar problemas. Vamos a estudiar exclusión mutua y como lograrla en la siguiente sección.

El otro uso de semáforos es para la **sincronización**. Los semáforos *full* y *empty* son necesarios para garantizar que ciertos eventos secuenciales
no ocurran. En este caso, ellos garantizan que el productor tendrá su ejecución cuando el buffer esté lleno y que el consumidor detendrá su ejecución cuando el buffer esté
vacío. Su uso es distinto para exclusión mutua.

## 2.3.6 Mutexes

Cuando la capacidad del semáforo para contar no es necesaria, a veces se utiliza una versión simplificada del semáforo, llamada mutex.
Los mutex solo son útiles para manejar la exclusión mutua para algunos recursos compartidos o partes de código.
Son fáciles y eficientes de implementar, lo que los hace especialmente útiles en paquetes de hilos que se implementan completamente en el espacio de usuario.

Un **mutex** es una variable compartida que puede encontrarse en 2 estados: bloqueado o desbloqueado.
En consecuencia, solo necesitamos un bit para representarlo, pero en la práctica un entero es más usado,
0 significa desbloqueado y todos los demás valores significan bloqueado.

Se usan 2 procesos con los mutex. Cuando un hilo o proceso necesita acceder a un región crítica, este llama al 
*mutex_lock*. Si el mutex esta desbloqueado(osea que la región crítica está disponible), la llamada tiene éxito y 
el hio llamante está libre para entrar a la región crítica. 

De otra forma, si el mutex se encuentra bloqueado, el hilo llamante is bloqueado hasta que el hilo en la región crítica termine y
ejecute la función *mutex_unlock*. Si múltiples hilos están bloqueados en el mútex, uno de ellos es escogido aleatoriamente y 
son permitidos para adquirir el lock.

Como los mutex son simples, pueden implementarse fácilmente en el espacio de usuario siempre que se disponga de una instrucción TSL o XCHG.
El código para *mutex_lock* y *mutex_unlock* para su uso con un paquete de hilos a nivel de usuario se muestran en la Fig. 2-29.
La solución con XCHG es esencialmente la misma.

![imagen2.29](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-29.png?raw=true)

El código para *mutex_lock* es muy similar al código de *enter_region* en la figura 2-25, pero con una diferencia
crucial. Cuando *enter_region* falla a la hora de entrar a la región crítica, este se mantiene consultando el lock repetidas veces
(busy waiting). Con el tiempo, el reloj se agota y algún otro proceso está programado para ejecutarse. Tarde o temprano, el proceso 
que tiene el bloqueo consigue ejecutarse y lo libera.

Con user threads la situación es distinta pues no existe un clock que detenga el hilo que ya no deba ejecutarse más. En consecuencia, 
un hilo que intenta adquirir un bloqueo mediante la espera ocupada hará un bucle eterno y nunca adquirirá el bloqueo porque nunca permite 
que ningún otro hilo se ejecute y libere el bloqueo.

Acá llega la diferencia entre *enter_region* y *mutex_lock*. Cuando se falle al adquirir el lock, este debe llamar
a la llamada thread_yield para dejar el CPU a otro hilo. De esta forma no hay busy waiting. Cuando el hilo se ejecute 
nuevamente esté revisará nuevamente el lock.

Comenzando que *thread_yield* es una llamada para el planificador de hilos en espacio de usuario, este es 
bastante rápido. En consecuencia ni *mutex_lock* ni *mutex_unlock* requieren alguna
llamada al kernel. Con ellas, los subprocesos de nivel de usuario pueden sincronizarse por completo en el espacio de usuario utilizando 
procedimientos que sólo requieren un puñado de instrucciones.

El sistema de mutex mostrado anteriormente es un conjunto básico de llamadas. Con todo software, siempre hay demanda de más funciones, y las 
primitivas de sincronización no son una excepción. Por ejemplo, aveces un packete de hilos ofrece la llamada *mutex_trylock* adquiere el 
bloqueo o devuelve un código en caso de fallo, pero no bloquea. Esta llamada da al hilo la flexibilidad de decidir qué hacer a continuación si 
hay alternativas para simplemente esperar.

Hay una cuestión sutil que hasta ahora hemos pasado por alto, pero vale la pena explicar.  Con un paquete de hilos en espacio de usuario 
no hay problema con que varios hilos tengan acceso al mismo mutex, ya que todos los hilos operan en un espacio de direcciones común.
Sin embargo, con la mayoría de las soluciones anteriores, como el algoritmo de Peterson y los semáforos, hay una suposición tácita de que múltiples 
procesos tienen acceso al menos a algo de memoria compartida, quizás sólo una palabra, pero algo. Si los procesos tienen espacios de direcciones disjuntos, 
como hemos dicho sistemáticamente, ¿cómo pueden compartir la variable de turno en el algoritmo de Peterson, o los semáforos o un buffer común? 

Hay 2 respuesta. Primero, alguna de las estructuras de datos comunes como los semáforos, pueden estar guardados en el kernel y solo son accedidas mediante
system calls. Este enfoque elimina el problema. Segundo, la mayoria de sistemas operativos modernos, ofrecen formas de compartir algunas porciones de espacio
de memoria con otros procesos. De esta forma, los buffers y otras estructuras de datos pueden ser compartidas. En otro caso, tomando el peor de todos, 
un archivo compartido puede ser usado. Si 2 o más procesos comparten la mayoría o todo su espacio de memoria, no habría mucha diferencia entre los hilos y 
los procesos. 2 procesos que compartan el mismo espacio de memoria siguen teniendo diferentes archivos abiertos, timers de alarma y otras propiedades, mientras 
que los hilos en un mismo proceso las comparten. Y siempre es cierto que los procesos múltiples que comparten un espacio de direcciones común nunca tienen la 
eficiencia de los hilos a nivel de usuario, ya que el núcleo está profundamente implicado en su gestión.

### Futexes

Con el crecimiento del paralelismo, la eficiencia de sincronización y el locking es bastante importante para el rendimiento. Los spin locks son rápidos si 
la espera es corta, pero desperdician ciclos de CPU si no lo son. Si hay mucha espera, es más eficiente bloquear el proceso y permitir al kernel desbloquearlo
solo cuando el lock esté libre.  Desafortunadamente esto tiene el problema inverso: funciona bien bajo una fuerte espera, pero cambiar continuamente al 
kernel es costoso si hay muy poca espera para empezar. Para hacer esto peor, puede no ser fácil predecir la cantidad de espera de bloqueos.

Una solución interesante a este problema es el **futex** o el "fast user space mutex". Un futex es una caracteristica de Linux que implementa el bloqueo 
básico(mutex) pero evita caer en el kernel a menos que este realmente lo necesite. Dado que cambiar entre el kernel es bastante caro, usar el futex es 
considerablemente rentable. Un futex consiste en 2 partes: en un servicio de kernel y una librería de usuario. El servicio de kernel provee una "wait queue"
que permite a múltiples procesos esperar en un lock. Ellos no se van a ejecutar, hasta que el kernel los desbloquee. Para poner un proceso en la cola de espera 
se requiere una llamada al sistema (costosa) y debe evitarse. En la ausencia de espera, los futex trabajan completamente en los user space. Los procesos comparten
una misma variable lock(32-bit integer).Supongamos que el bloqueo es inicialmente 1, lo que suponemos que significa que el bloqueo está libre. Un hilo toma 
el lock realizando un atomic "decremente and test" (Las funciones atómicas en Linux consisten en ensamblaje en línea envuelto en funciones C y se definen en 
archivos de cabecera). Luego, el hilo inspecciona el resultado para ver si el lock está libre o no. Si está libre, todo está bien y nuestro hilo ha tomado satisfactoriamente
el lock. Si el lock le pertenece a otro hilo, nuestro hilo se queda esperando. En este caso, la librería de futex no entra en spin, pero usa una llamada al sistema 
para poner al hilo en la cola de espera del kernel. Esperemos que el costo de cambiar al kernel, esté justificado, dado que el hilo fue bloqueado de todos modos.
Cuando un hilo termina con el lock, libera el lock con un atomic "increment and test" y revisa el resultado para ver si algún proceso continua bloqueado en la
cola de espera del kernel. Si es así, hará saber al kernel que puede desbloquear uno o más de estos procesos. Si no hay contención, el kernel no interviene en absoluto.

### Mutexes in Pthreads
Pthreads proporciona una serie de funciones que se pueden utilizar para sincronizar hilos.
Los mecanismos básicos usados en una variable mutex son para bloquear o desbloquear, es decir, cuidar la región crítica.
Un thread que quiere entrar a la zona crítica primero debe lockear el mutex asociado. Si el mutex no está bloqueado el hilo puede
entrar y el lock es atomicamente seteado, evitando que otros hilos entren. Si el mutex ya está bloqueado, el hilo llamado es bloqueado
hasta que este sea desbloqueado. Si varios hilos esperan en un mismo mutex, cuando este se desbloquee
uno de ellos es permitido para tomarlo y volver a bloquearlo. Estos locks no son mandatorios, está en manos del programador
asegurarse de usar los hilos correctamente.

La mayor llamada relacionada a los mutexes se encuentra en la figura 2-30. Como vemos, los mutex pueden ser creados y destruidos.
Las llamadas para realizar estas operaciones son:
- phtread_mutex_init
- pthread_mutex_destroy

Los hilos tambien pueden ser bloqueados con *pthread_mutex_lock* el cual trata de adquirir el lock y bloquea el hilo si ya estaba
lockeado. También existe la opción de tratar de lockear un mutex y fallar con un codigo de error, en vez de bloquear el hilo si es que 
el lock ya estaba seteado. Esta llamada es *pthread_mutex_trylock*. Esta llamada permite a los hilos realizar efectivamente el busy waiting
si es necesario. Finalmente, *pthread_mutex_unlock* desbloquea un mutex y libera exactamente un hilo si uno o más están en espera de él. Los mutex
también pueden tener atributos, pero estos son usados solo para propósitos especiales.

![imagen2.30](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-30.png?raw=true)

En adición a los mutex, Pthreads ofrece un segudo mecanismo de sincronización: **condition variables**. Los mutex son buenos para permitir o restringir accesos 
a las regiones críticas. Las variables de condición permiten que los hilos se bloqueen si no se cumple alguna condición. Casi siempre los dos métodos se utilizan 
juntos. Veamos ahora la interacción de hilos, mutexes y variables de condición con un poco más de detalle.

Como un ejemplo simple, consideremos el problema del productor-consumidor nuevamente: un hilo comienza a poner cosas en el buffer y otro comienza a tomar cosas del 
buffer. Si el productor descubre que no hay más slots disponibles en el buffer, este debe bloquearse hasta que un slot esté disponible. Los mutex permiten hacer la 
comprobación atómicamente sin interferencia de otros hilos, pero habiendo descubierto que el buffer está lleno, el productor necesita una forma de bloquearse y ser 
despertado más tarde. Esto es lo que permiten las variables de condición. 

Las llamadas más importantes relacionadas a las variables de condición se encuentran en la figura 2-31.
Como probablemente esperabas, existen llamadas para crear y destruir variables de condition. Pueden tener atributos y existen varias llamadas para gestionarlos (no se muestran).
Las operaciones primarias de las variables de condition son *pthread_con_wait* y *pthread_cond_signal*. La primera bloquea el hilo que realiza la llamada hasta que otro hilo le 
envía una señal (utilizando la segunda llamada).Los motivos de bloqueo y espera no forman parte del protocolo de espera y señalización, por supuesto. El hilo bloqueado suele estar esperando
la señal de un hilo para poder realizar algún trabajo, liberar algún recurso o realizar alguna otra actividad. Solo así podrá continuar el hilo bloqueado. Las variables de condición permiten que 
este waiting y blocking se realicen atómicamente. La llamada *pthread_cond_broadcast* se utiliza cuando hay varios hilos potencialmente todos bloqueados y esperando la misma señal.

Las variables de condición y los mutex siempre son usados juntos. El patrón es para un hilo que lockea un mutex, luego espera en una variable condicional si es que no obtiene lo que quiere.
Eventualmente otro hilo le envía una señal y él puede continuar. La llamada *pthread_cond_wait* desbloquea atómicamente el mutex que está reteniendo. 
Por esta razón, el mutex es uno de los parámetros.

También vale la pena señalar que las variables de condición (a diferencia de los semáforos) no tienen memoria. Si una señal es enviada a una variable de condición en el cual no hay un hilo esperando,
se pierde la señal. Los programadores deben ser cuidadosos para no perder señales. 

![imagen2.31](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-31.png?raw=true)

Como ejemplo de como los mutexes y variables de condición usadas, la figura 2-32 muestra un ejemplo simple del problema del productor-consumidor con un solo buffer.
Cuando el productor ha llenado el buffer, este debe esperar a que el consumidor la vacíe antes de producir el siguiente artículo. Similarmente, cuando el consumidor ha removido 
un ítem, debe esperar a que el consumidor produzca otro . Esto es bastante simple, este ejemplo muestra los mecanismos básicos. 
La sentencia que pone un hilo a dormir siempre debe comprobar la condición para asegurarse de que se cumple antes de continuar, ya que el hilo podría haber sido despertado debido a una señal UNIX o alguna otra razón.

![imagen2.32](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-32.png?raw=true)

## 2.3.7 Monitors

Con semaforos y mutex los interprocesos de comunicación se pueden ver fáciles, verdad?
Miremos cuidadosamente el orden de los downs antes de insertar o remover items del buffer en la figura 2-28. Supongamos que 2 downs en el código del productor
fueran cambiados en orden, entonces el mutex fue decrementado antes de que esté vacío en vez de realizarlo después. Entonces, la siguiente vez que el consumidor
quiera acceder al buffer, realizará un down en el mutex, ahora 0 y se bloqueará. Ambos proceso estarán bloqueados por siempre y no podrán terminar. Esto se llama deadlock
y lo veremos en el capítulo 6. 

Este problema apunta a mostrar la importancia de ser cuidadoso al usar semáforos. Un error sútil y todo se paraliza. Es como programar en lenguaje ensamblador, solo que peor, 
porque los errores son race condition, deadlock y otras formas de comportamiento impredecible e irreproducible.

Para facilitar la escritura de programas correctos, Brinch Hansen y Hoare propusieron una sincronización primitiva de alto nivel llamada **monitor**. Sus propósitos son se diferencian sutilmente,
como veremos. Un monitor es una colección de funciones, variables, y estructuras de datos que son agrupadas juntas en una especie de módulo o package. Los procesos pueden llamar a las funciones de un monitor
cuando ellas lo requieran, pero no pueden acceder a la estructura interna del monitor desde funciones declaradas afuera del monitor. La figura 2-33 muestra un monitor escrito
en un lenguaje imaginario, Pidgin Pascal. No se puede usar C dado que los monitores son un *language concept*.

![imagen2.33](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-33.png?raw=true)

Los monitores tienen una propiedad importante que la hace útil para lograr la exclusión mutua: solo un proceso puede estar activo en un monitor en cualquier instante. 
Los monitores son una construcción del lenguaje de programación, por lo que el compilador sabe que son especiales y puede manejar las llamadas a procedimientos de monitorización de forma diferente a otras llamadas a procedimientos.
Tipicamente, cuando un proceso llama a una función del monitor, las primeras intrucciones de la función revisa si otro proceso se encuentra activo con el monitor.
Si lo está, el proceso que realizó la llamada será suspendido hasya que el otro deje el monitor, en caso no haya otro proceso en el monitor, el proceso llamante podrá entrar.
Depende del compilador implementar la exclusión mutua en las entradas del monitor, pero una forma común es utilizar un mutex o un semáforo binario. Dado que el compilador, y no el programador, se encarga de la exclusión mutua, 
es mucho menos probable que algo salga mal. En cualquier caso, la persona que escribe el monitor no tiene por qué ser consciente de cómo el compilador organiza la exclusión mutua. Es suficiente saber que al convertir todas las 
regiones críticas en procedimientos de monitorización, nunca dos procesos ejecutarán sus regiones críticas al mismo tiempo. 

Aunque los monitores provean una manera fácil de lograr exclusion mutua, como hemos visto anteriomente, esto no es suficiente. También necesitamos una forma de que los procesos se bloqueen cuando no puedan continuar.
En el problema productor-consumidor, es bastante fácil poner todas las pruebas para buffer-full y buffer-empty en procedimientos de monitorización, pero ¿cómo debería bloquear el productor cuando encuentra el buffer lleno?
La solución reside en la introducción de variables de condición, junto con dos operaciones sobre ellas, wait y signal. Cuando una función del monitor descubra que ya no puede continuar(buffer lleno para el productor), este realiza
un wait en alguna variable de condición, dice, full. Esta acción causa que el proceso llamante se bloquee. Esto permite a otros procesos que anteriormente estaban prohibidos de entrar al monitor que entren. 
Hemos visto variables de condición y estas operaciones en el contexto del Pthread.

Para el otro proceso, por ejemplo, el consumidor, puede despertar a la parte durmiente realizando una señal en la variable de condición en la que espera su compañero.
Para evitar tener 2 procesos activos en el monitor al mismo tiempo, necesitamos una regla que diga qué sucede después de una señal. Hoare propuso dejar funcionar el proceso recién despertado, suspendiendo el otro.
Brinch Hansen propuso solucionar el problema exigiendo que un proceso que emita una señal salga inmediatamente del monitor. En otras palabras, una sentencia de señal sólo puede aparecer como sentencia final en un procedimiento de monitorización. 
Utilizaremos la propuesta de Brinch Hansen porque es conceptualmente más sencilla y también más fácil de implementar. Si se realiza una señal sobre una variable de condición en la que hay varios procesos en espera, sólo se reactivará uno de ellos, 
determinado por el planificador del sistema.

Existe una tercera solución que no fue propuesta ni por Hoare o Brinch Hansen. Esto es para permitir que el señalador continúe ejecutándose y que el proceso en espera comience a ejecutarse sólo después de que el señalador haya salido del monitor.
Las variables de condición no son contadores. Ellas no acumulan señales para su uso posterior como lo hacen los semáforos. Entonces, si una variable de condición es señalada sin nadie que lo espere, la señal se pierde para siempre.
En otras palabras, el wait debe estar antes de la señal. Esta regla hace la implementación mucho más simple. En la práctica esto no es un problema dado que es fácil llevar un registro del estado de cada proceso con variables si es que 
lo necesitamos. Un proceso que de otro modo podría hacer una señal puede ver que esta operación no es necesaria mirando las variables.

Un esqueleto del problema del productor-consumidor con monitores es mostrado en la figura 2-34 con el lenguaje imaginario, Pidgin Pascal. 

Puedes estar pensando que las operaciones *wait* y *signal* pueden parecer similares a sleep y wakeup, las cuales dijimos anteriormente que tenía un pésimo problema con los race condition. Bueno, estas instrucciones son similares, pero no son iguales.
Wait y sleep fallan, porque cuando un proceso intentaba dormirse, el otro intentaba despertarlo. Con los monitores, esto no puede suceder. La exclusión mutua está garantizada, es decir, si el productor en el monitor descubre que el buffer está lleno, podrá completar la operación wait sin tener que preocuparse por la posibilidad de que el planificador cambie al consumidor justo antes de que se complete la espera. El consumidor ni siquiera entrará en el monitor hasta que el wait haya terminado y el productor haya sido marcado como no ejecutable.

Como Pidgin Pascal es un lenguaje imaginario, algunos lenguajes de programación reales soportan el uso de monitores. Uno de ellos es Java. Si agregamos la keyword synchronized a un método de declaración, Java garantiza que una vez se ejecute ese hilo, ningún otro hilo va a poder ejecutar otro método synchronized de ese objeto. Sin synchronized, no existe garantía de esa inclusión.

![imagen2.34](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-34.png?raw=true)

Una solución al problema del productor-consumidor es mostrada en la figura 2-35. La solución tiene 4 clases. La clase externa ProducerConsumer crea e inicia 2 hilos, p y c. La segunda y tercera clase, productor y consumidor, respectivamente tienen el codigo para el productor y el consumidor. Finalmente la clase, *our_monitor* es el monitor. Esta clase contiene 2  hilos synchronized que son usados para insertar valores en el buffer y sacarlos. A diferencia de los ejemplos anteriores, aquí tenemos el código completo de inserción y eliminación.

![imagen2.35](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-35.png?raw=true)


Los hilos del productor y del consumidor son funcionalente idénticos a los mostrados en los ejemplos pasados. El consumidor tiene un loop infinito creando items y poniéndolos en el buffer, mientras que el consumidor tiene otro loop infinito consumiendo items y sacandolos del buffer.

La parte interesante del código es la de la clase *our_monitor*, donde se maneja el buffer, las variables de admistración y los 2 métodos sincronizados. Cuando el productor está activo en el insert, este sabe que el consumidor no puede estar activo en el remove, dando la seguridad de actualizar las variables en el buffer sin miedo a las race condition. La variable count lleva la cuenta de cuántos items hay en el buffer. Este puede tener cualquier valor desde 0 hasta N-1. La variable lo es el indice del buffer donde se va a recoger el siguiente item, mientras que la variable hi es el indice del buffer donde se va a poner el siguiente item. Está permitido el lo=hi, el cual significa que hay 0 elementos o N elementos en el búfer. El valor de
count indica qué caso se cumple. 

Los métodos sincronizados en Java se diferencian de los monitores clásicos ya que Java no incorpora variables de condición. En vez de eso, este tiene 2 procesos, wait y notify, los cuales son equivalentes a sleep y wakeup excepto por el hecho de que se usan en métodos sincronizados, no pueden tener race condition. En teoría el método wait no puede ser interrumpido,  de eso trata el código que lo rodea.Java requiere que el manejo de excepciones se haga explícito. Para nuestros propósitos, imagina que go to sleep es la forma de ir a dormir.

Realizando la exclusión mutua de las regiones críticas, los monitores permiten programar paralelamente con muchos menos errores que con los semáforos. De todos modos, igual presenta inconvenientes. Los monitores son un concepto de lenguaje de programación.El compilador debe reconocerlos y realizar la exclusión mutua de una manera u otra. Lenguajes como C, Pascal y otros no tiene monitores, entonces no tiene sentido esperar que sus compiladores puedan realizar la exclusión mutua. Entonces, ¿cómo el compilador puede saber qué funciones están en un monitor y cuales no?

Estos 2 lenguajes de programación tampoco tiene incluidos semáforos, pero añadirlos es fácil: Solo debes agregar 2 pequeñas instrucciones en lenguaje ensamblador a la librería para poder realizar las llamadas a up y down. Los compiladores no tiene porque saber que existen. Por supuesto, los sistemas operativos deben saber de los semáforos pero a menos que tengas un sistema operativo basado en semáforos, tu puedes escribir programas de usuario para ellos en C o C++. Para los monitores tú necesitas un lenguaje de programación escrito para ellos.

Otro problema con los monitores, al igual que con los semáforos es que están diseñados para resolver la exclusión mutua para una o más CPUs que tiene acceso a una memoria común. Colocando los semáforos en la memoria compartida y protegiéndolos con instrucciones TSL o XCHG, podemos evitar las race condition. Cuando pasamos a un sistema distribuido que consta de diferentes CPUs, cada una de ellas tiene su memoria privada conectada a una local network, estas primitivas se vuelven inaplicables.

La conclusión es que los semáforos son de muy bajo nivel y los monitores no pueden ser usados excepto en pocos lenguajes de programación. De todas formas, ninguna de las primitivas permite el intercambio de información entre máquinas. Se necesita algo más.

## 2.3.8 Message Passing

Este método de comunicación entre procesos utiliza dos primitivas, **send** y **recieve**, que, al igual que los semáforos y a diferencia de los monitores, son llamadas al sistema y no construcciones del lenguaje. Como tales, pueden incluirse fácilmente en procedimientos de biblioteca, tales como:

        send(destination,&message)
y

        receive(source,&message)

La primera llamada envía un mensaje a un destino determinado y la segunda recibe un mensaje de un origen determinado(o cualquiera si al receptor no le importa). Si no hay ningun mensaje disponible, el receptor puede bloquearse hasta que un mensaje llegue. De otra forma, este puede retornar inmediatamente con un código de error. 



### Design Issues for Message-Passing Systems

Los sistemas de mesagee-passing presentan varios problemas que los semáforos o monitores no tienen, especialmente si el proceso de comunicación se realiza entre diferentes máquinas conectadas a una network. Por ejemplo, los mensajes pueden perderse en la network. Para evitar la pérdida de mensajes, el emisor y el receptor pueden acordar que, en cuanto se reciba un mensaje, el receptor envíe de vuelta un mensaje especial de acuse de **mensaje de recibo**. Si el emisor no ha recibido el mensaje de recibo en un cierto intervalo de tiempo, este retransmite el mensaje.

Ahora consideremos que pasa si el mensaje ha sido recibido correctamente, pero el mensaje de confirmación enviado al emisor se pierde. El emisor retransmitirá el mensaje, entonces el receptor lo tendrá 2 veces. Es importante que el receptor sepa diferenciar un mensaje nuevo a uno retransmitido. Usualmente este problema es resuelto colocando números consecutivos en el mensaje original. Si el receptor recibe un mensaje con el mismo número de secuencia que el mensaje anterior, sabe que se trata de un duplicado que puede ignorar. 

Los sistemas de mensajes también tienen que abordar la cuestión de cómo se nombran los procesos, para que el proceso especificado en una llamada de envío o recepción no sea ambiguo. La **autenticación** es otro problema en el sistema de mensajes: ¿cómo puede el cliente decir que está comunicándose de un archivo real de servidor y no está siendo un intruso?

En el otro extremo del espectro, también hay cuestiones de diseño que son importantes cuando el emisor y el receptor están en la misma máquina. Uno de ellos es el rendimiento. Copiar mensajes de un proceso a otro es siempre más lento que hacer una operación semáforo o entrar en un monitor. Se ha trabajado mucho para que el paso de mensajes sea eficiente.

### The Producer-Consumer Problem with Message Passing

Ahora veamos como el problema del productor-consumidor puede ser resuelto con message passing y sin memoria compartida. La solución es mostrada en la figura 2-36. Nosotros asumimos que los mensajes tienen el mismo tamaño y que los mensajes enviados y no recibidos son guardados en el buffer automaticamente por el sistema operativo. En esta solución, se usa un total de N mensajes, análogo a los N slots en el buffer compartido. El consumidor comienza enviando N mensajes vacíos al productor. Cuando el productor tiene un item para enviarle al consumidor, toma un mensaje vacío y devuelve uno completo. De esta forma, el numero total de mensajes en el sistema se mantiene constante en el tiempo, para que puedan almacenarse en una determinada cantidad de memoria conocida de antemano.

Si el productor trabaja más rápido que el consumidor, todos los mensajes acabarán llenos, esperando al consumidor, el productor se encontrará bloqueado, esperando un mensaje vacío. Si el consumidor trabaja más rápido, ocurre lo contrario, todos los mensajes estarán vacíos a la espera de que el productor los llene; el consumidor estará bloqueado, a la espera de un mensaje lleno. 

Se pueden tener muchas variantes para los message passsing. Para empezar, veamos cómo se dirigen los mensajes. Una forma es asignar a cada proceso, una única dirección y tener mensajes que se envíen a los procesos. Otra forma es inventar una nueva estructura de datos, llamada **mailbox**. Un mailbox es un lugar donde se almanena cierto numero de mensajes, esto se especifica cuando se crea el mailbox. Cuando los mailbox son usados, los parámetros de dirección en las llamadas de send y receive son buzones, no procesos. Cuando un proceso trata de enviar algo a un mailbox que está lleno, este se suspende hasta que un mensaje sea removido del mailbox, para dejar espacio para uno nuevo.

Para el problema del productor-consumidor, ambos deben crear mailbox de un tamaño N para N mensajes. El productor debe enviar mensajes conteniendo el dato actual para el mailbox del consumidor y el consumidor debe enviar mensajes vacíos para el mailbox del productor. Cuando se usan mailbox, el mecanismo de almacenamiento en el buffer es claro: el mailbox de destino contiene mensajes que han sido enviados al proceso de destino pero que aún no han sido aceptados.

El otro extremo de tener mailbox es eliminar todo el almacenamiento del buffer. Cuando se adopta este enfoque, si el envío se realiza antes de la recepción, el proceso de envío se bloquea hasta que se produce la recepción, momento en el que el mensaje puede copiarse directamente del emisor al receptor, sin almacenamiento en el buffer. Del mismo modo, si la recepción se realiza en primer lugar, el receptor se bloquea hasta que se produce un envío. Esta estrategia se conoce a menudo como "rendezvous". Es más fácil de implementar que un esquema de mensajes en buffer pero es menos flexible, ya que el emisor y el receptor se ven obligados a funcionar al mismo tiempo.

![imagen2.36](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-36.png?raw=true)

Los message passing son usados sistemas de programación paralela. Un message passing system muy conocido es MPI(Message-Passing Interface)

## 2.3.9 Barriers

Nuestro último mecanismo de sincronización está orientado a un grupo de procesos, en vez del caso de 2 procesos como puede ser para el productor-consumidor. Algunas aplicaciones se dividen en fases y tienen la regla de que no deben tener procesos en la siguiente fase hasta que todos los procesos estén listo para avanzar de fase. Este comportamiento puede ser logrado poniendo una **barrier** al final de cada fase. Cuando un proceso llega a la barrera, este se bloquea hasta que todos los demás procesos hayan llegado a la barrera. Esto permite sincronizar grupos de procesos. La operación de barrera está ilustrada en la figura 2-37.

![imagen2.37](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-37.png?raw=true)

En la figura 2-37(a) vamos a ver 4 procesos que se acercan a una barrera. Esto significa que aún no alcanzan el final de su fase actual. Al cabo de un rato, el primer proceso termina toda la computación que se le había exigido durante la primera fase. Entonces ejecuta la **barrier primitive**, generalmente llamando a un procedimiento de biblioteca. A continuación el proceso se suspende. Poco tiempo después, el segundo y tercer proceso finalizan(figure 2-37b). Finalmente el último proceso C llega a la barrera y todos los procesos son liberos, como en la figura 2-37(b).

Como un ejemplo de un problema que requiere barreras, consideremos un problema in física o ingeniería. Usualmente existe una matriz que contiene valores iniciales, el valor puede representar la temperatura en varios puntos de una lámina de metal. La idea es calcular qué tanto le tomaría a una llama propagarse de una esquina hacia toda la lámina.

Comenzando con los valores iniciales, se aplica una aplicación a la matriz para obtener una segunda versión de la matriz, por ejemplo, aplicando las leyes de la termodinámica para ver todas las temperaturas después. Luego el proceso se repite una y otra vez, dando la temperatura de distintos puntos de la hoja como una función de tiempo mientras la hoja se quema. El algoritmo produce una secuencia de matrices a lo largo del tiempo, cada una para un momento dado.

Ahora, imaginemos que la matriz es bastante larga(1 millon x 1 millon), entonces se necesitan procesos paralelos de cálculo para acelerar el cálculo. Diferentes procesos trabajan en distintas partes de la matriz, calculando los nuevos elementos de la matriz a partir de los antiguos según las leyes de la física.
De todos modos, ningún proceso debe comenzar con la iteración n+1 hasta que la iteración n se complete, es decir, hasta que todos los procesos hayan terminado su trabajo actual. La forma de lograr este objetivo es programar cada proceso para ejecutar una operación barrier después de terminar su parte en la iteración actual. Cuando todos hayan acabado, la matriz nueva(el input para la nueva iteración) estará lista y todos los procesos van a ser liberados simultáneamente para comenzar la nueva iteración.

## 2.3.10 Avoiding Locks: Read-Copy-Update

Los locks rápidos no son locks del todo. La cuestión es si podemos permitir accesos concurrentes de lectura
y escritura a estructuras de datos compartidas sin bloqueo. En el caso general la respuesta es claramente no. Imaginemos un proceso A ordenando un array de números, mientras que el proceso B calcula el promedio. Debido a que A mueve los valores entre el array, B puede encontrar un valor varias veces o incluso no encontrarlo. El resultado puede ser cualquier cosa, por lo que probablemente sea erróneo.

En algunos casos, podemos permitir a un writer actualizar una estructura de datos mientrras otros procesos la están usando. El truco es de asegurarse de que cada reader lea la versión anterior del dato, o la nueva, pero no una combinación rara de ambas.  A modo de ilustración consideremos el arbol mostrado en la figura 2-38.

![imagen2.38](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter2/figure2-38.png?raw=true)

Los lectores recorren el árbol desde la raíz hasta las hojas. En la media mitad del arbol se agrega un nodo X. Para ello hacemos que el nodo sea "perfecto" antes de hacerlo visible en el árbol: inicializamos todos los valores del nodo X, incluidos los punteros a sus hijos.Luego con una escritura atómica hacemos que X sea hijo de A. Ningún lector leerá nunca una versión inconsistente. En la mitad inferior del arbol, eliminamos B y D. Primero, hacemos que el puntero izquierdo de A apunte a C. Todos los lectores que estaban en A continuarán en el nodo C y nunca verán B o D. En otras palabras sólo verán la nueva versión.
Del mismo modo, los lectores que se encuentren en B o D continuarán siguiendo los punteros de la estructura de datos original y verán la versión antigua. Todo va bien y nunca necesitaremos bloquear nada. La razón principal por la que la eliminación de B y D funciona sin bloquear la estructura de datos, es que RCU (Read-Copy-Update), desacopla las fases de eliminación y reclamación de la actualización.

Sin embargo, hay un problema, mientras no estemos seguros que no hay más lectores para B o D, no podemos liberarlos realmente.¿Pero cuánto debemos esperar?¿Un minuto?¿Diez? Tenemos que esperar hasta que el último lector deje los nodos. RCU detemina el tiempo máximo que un lector puede mantener una referencia a la estructura de datos. Transcurrido ese tiempo, puede recuperar la memoria de forma segura. En concreto, los lectores accceden a la escritura de datos en lo que se conoce como **read-side critical section**, que puede contener cualquier código siempre y cuando no se bloquee o duerma. En ese caso, sabemos el tiempo máximo que debemos esperar. Específicamente, definimos un **grace period** como cualquier periodo de tiempo en el que sabemos que cada hilo estará fuera de la sección crítica del lado de lectura al menos una vez. Todo irá bien si esperamos una duración que sea al menos igual al periodo de gracia antes de reclamar. Como el código de una sección crítica de lectura no puede bloquearse ni dormir, un criterio sencillo es esperar a que todos los hilos hayan ejecutado un cambio de contexto.