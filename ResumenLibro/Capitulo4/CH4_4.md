# 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION

Hacer que el sistema de archivos funcione es una cosa; hacer que funcione de forma eficiente y robusta en la vida real es algo muy distinto.  En las secciones siguientes veremos algunos de los problemas que plantea la gestión de discos.

## 4.4.1 Disk-Space Management

Los archivos se almacenan normalmente en disco, por lo que la gestión del espacio en disco es una de las principales preocupaciones de los diseñadores de sistemas de archivos. Existen dos estrategias generales para almacenar un archivo de n bytes: se asignan n bytes consecutivos de espacio en disco, o el archivo se divide en un número de bloques (no necesariamente) contiguos. Como hemos visto, el almacenamiento de un archivo como una secuencia contigua de bytes tiene el problema obvio de que si un archivo crece, es posible que haya que moverlo en el disco. El mismo problema se aplica a los segmentos en memoria, excepto que mover un segmento en memoria es una operación relativamente rápida en comparación con mover un archivo de una posición de disco a otra. Por este motivo, casi todos los sistemas de archivos dividen los archivos en bloques de tamaño fijo que no tienen por qué ser adyacentes.

### Block Size

Una vez que se ha decidido almacenar los archivos en bloques de tamaño fijo, surge la pregunta de qué tamaño debe tener el bloque. Dada la forma en que están organizados los discos, el sector, la pista y el cilindro son candidatos obvios para la unidad de asignación (aunque todos ellos dependen del dispositivo, lo cual es un inconveniente). En un sistema de paginación, el tamaño de la página también es un contendiente importante.

Un tamaño de bloque grande significa que cada archivo, aunque sea de 1 byte, ocupa un cilindro entero. Por otro lado, un tamaño de bloque pequeño significa que la mayoría de los archivos abarcarán varios bloques y, por tanto, necesitarán varias búsquedas y retrasos de rotación para leerlos, lo que reduce el rendimiento. Por tanto, si la unidad de asignación es demasiado grande, desperdiciamos espacio; si es demasiado pequeña, desperdiciamos tiempo.

Un tamaño de bloque grande significa que cada archivo, aunque sea de 1 byte, ocupa un cilindro entero. Por otro lado, un tamaño de bloque pequeño significa que la mayoría de los archivos abarcarán varios bloques y, por tanto, necesitarán varias búsquedas y retrasos de rotación para leerlos, lo que reduce el rendimiento. Por tanto, si la unidad de asignación es demasiado grande, desperdiciamos espacio; si es demasiado pequeña, desperdiciamos tiempo.

Para hacer una buena elección es necesario disponer de información sobre la distribución del tamaño de los archivos.  Tanenbaum et al. (2006) estudiaron la distribución del tamaño de los archivos en el Departamento de Informática de una gran universidad de investigación (la VU) en 1984 y de nuevo en 2005, así como en un servidor web comercial que albergaba un sitio web político (www.electoral-vote.com).  Los resultados se muestran en la Fig. 4-20, donde para cada tamaño de archivo de potencia de dos, se enumera el porcentaje de todos los archivos menores o iguales para cada uno de los tres conjuntos de datos. Por ejemplo, en 2005, el 59.13% de todos los archivos de la VU tenían un tamaño igual o inferior a 4 KB y el 90,84% de todos los archivos tenían un tamaño igual o inferior a 64 KB. El tamaño medio de los archivos era de 2.475 bytes. Puede que a algunas personas les sorprenda este pequeño tamaño.

![figure4-20](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-20.png?raw=true)

¿Qué conclusiones podemos sacar de estos datos? Por un lado, con un tamaño de bloque de 1 KB, sólo entre el 30 y el 50% de todos los archivos caben en un solo bloque, mientras que con un bloque de 4 KB, el porcentaje de archivos que caben en un bloque sube hasta el 60-70%. Otros datos del artículo muestran que con un bloque de 4 KB, el 93% de los bloques de disco son utilizados por el 10% de los archivos más grandes. Esto significa que perder algo de espacio al final de cada archivo pequeño apenas importa porque el disco está lleno por un pequeño número de archivos grandes (vídeos) y la cantidad total de espacio ocupado por los archivos pequeños apenas importa. Incluso duplicando el espacio que ocupa el 90% de los archivos más pequeños apenas se notaría.

Por otro lado, utilizar un bloque pequeño significa que cada archivo estará formado por muchos bloques. Normalmente, la lectura de cada bloque requiere una búsqueda y un retardo de rotación (excepto en un disco de estado sólido), por lo que la lectura de un archivo formado por muchos bloques pequeños será lenta.

Como ejemplo, consideremos un disco con 1 MB por pista, un tiempo de rotación de 8,33 mseg y un tiempo medio de búsqueda de 5 mseg. El tiempo en milisegundos para leer un bloque de k bytes es la suma de los tiempos de búsqueda, rotación y transferencia: 

    5 + 4. 165 + (k/1000000) × 8. 33

La curva discontinua de la Fig. 4-21 muestra la tasa de datos de un disco de este tipo en función del tamaño del bloque. Para calcular la eficiencia de espacio, tenemos que hacer una suposición sobre el tamaño medio de los archivos. Para simplificar, supongamos que todos los archivos son de 4 KB. Aunque este número es ligeramente mayor que los datos medidos en la VU, los estudiantes probablemente tienen más archivos pequeños de los que estarían presentes en un centro de datos corporativo, por lo que podría ser una mejor suposición en general. La curva continua de la Fig. 4-21 muestra la eficiencia de espacio en función del tamaño de bloque

Las dos curvas pueden entenderse de la siguiente manera. El tiempo de acceso a un bloque está totalmente dominado por el tiempo de búsqueda y el retardo de rotación, por lo que, dado que va a costar 9 mseg acceder a un bloque, cuantos más datos se obtengan, mejor.

![figure4-21](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-21.png?raw=true)

De ahí que la velocidad de transmisión de datos aumente casi linealmente con el tamaño del bloque (hasta que las transferencias duran tanto que el tiempo de transferencia empieza a importar).

Consideremos ahora la eficiencia espacial. Con archivos de 4 KB y bloques de 1, 2 o 4 KB, los archivos utilizan 4, 2 y 1 bloque, respectivamente, sin desperdicio. Con un bloque de 8 KB y archivos de 4 KB, la eficiencia espacial desciende al 50%, y con un bloque de 16 KB baja al 25%. En realidad, pocos archivos son un múltiplo exacto del tamaño del bloque de disco, por lo que siempre se desperdicia algo de espacio en el último bloque de un archivo.

Lo que muestra el gráfico, sin embargo, es que el rendimiento y la utilización del espacio están intrínsecamente en conflicto. Los bloques pequeños son malos para el rendimiento, pero buenos para la utilización del espacio en disco. Para estos datos, no existe un compromiso razonable. El tamaño más cercano al cruce de las dos curvas es 64 KB, pero la velocidad de transmisión de datos es de sólo 6,6 MB/s y la eficiencia del espacio es de aproximadamente el 7%, ninguna de las cuales es muy buena. Históricamente, los sistemas de archivos han elegido tamaños comprendidos entre 1 y 4 KB, pero ahora que los discos superan 1 TB, quizá sea mejor aumentar el tamaño de bloque a 64 KB y aceptar el espacio de disco desperdiciado. El espacio en disco ya no escasea.

En un experimento para ver si el uso de archivos de Windows NT era sensiblemente diferente del uso de archivos de UNIX, Vogels realizó mediciones de archivos en la Universidad de Cornell (Vogels, 1999). Observó que el uso de archivos en NT es más complicado que en UNIX. Escribió:

*When we type a few characters in the Notepad text editor, saving this to afile will trigger 26 system calls, including 3 failed open attempts, 1 fileoverwrite and 4 additional open and close sequences.*

Sin embargo, Vogels observó un tamaño medio (ponderado por el uso) de los archivos sólo leídos de 1 KB, de los archivos sólo escritos de 2,3 KB y de los archivos leídos y escritos de 4,2 KB. Dadas las diferentes técnicas de medición de los conjuntos de datos y el año, estos resultados son ciertamente compatibles con los de la VU.

### Keeping Track of Free Blocks

Una vez elegido el tamaño de bloque, la siguiente cuestión es cómo llevar la cuenta de los bloques libres. Dos métodos son ampliamente usados, como se muestra en la Fig. 4-22. El primero consiste en utilizar una lista enlazada de bloques de disco, en la que cada bloque contiene tantos números de bloque de disco libres como quepan. Con un bloque de 1 KB y un número de bloque de disco de 32 bits, cada bloque de la lista libre contiene los números de 255 bloques libres. (Considere un disco de 1 TB, que tiene alrededor de 1.000 millones de bloques de disco. Para almacenar todas estas direcciones a 255 por bloque se necesitan unos 4 millones de bloques. Generalmente, los bloques libres se utilizan para mantener la lista libre, por lo que el almacenamiento es esencialmente gratuito.

![figure4-22](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-22.png?raw=true)

La otra técnica de gestión del espacio libre es el mapa de bits. Un disco con n bloques requiere un mapa de bits con n bits. Los bloques libres se representan con 1s en el mapa, los bloques asignados con 0s (o viceversa).  Para nuestro disco de ejemplo de 1 TB, necesitamos 1.000 millones de bits para el mapa, lo que requiere almacenar unos 130.000 bloques de 1 KB. No es sorprendente que el mapa de bits requiera menos espacio, ya que utiliza 1 bit por bloque, frente a los 32 bits del modelo de lista enlazada. Sólo si el disco está casi lleno (es decir, tiene pocos bloques libres), el esquema de listas enlazadas necesitará menos bloques que el mapa de bits.

Si los bloques libres tienden a venir en largas series de bloques consecutivos, el sistema de lista libre puede modificarse para llevar la cuenta de series de bloques en lugar de bloques individuales. Se puede asociar a cada bloque un contador de 8, 16 o 32 bits que indique el número de bloques libres consecutivos. En el mejor de los casos, un disco básicamente vacío podría representarse con dos números: la dirección del primer bloque libre seguida del contador de bloques libres. En cambio, si el disco está muy fragmentado, el seguimiento de las ejecuciones es menos eficaz que el de los bloques individuales, ya que no sólo hay que almacenar la dirección, sino también el contador.

Esta cuestión ilustra un problema que suelen tener los diseñadores de sistemas operativos. Hay múltiples estructuras de datos y algoritmos que pueden utilizarse para resolver un problema, pero elegir el mejor requiere datos que los diseñadores no tienen y no tendrán hasta que el sistema se despliegue y se utilice intensamente. E incluso entonces, los datos pueden no estar disponibles.  Por ejemplo, nuestras propias mediciones del tamaño de los archivos en la VU en 1984 y 1995, los datos del sitio web y los datos de Cornell son sólo cuatro muestras. Aunque es mucho mejor que nada, no tenemos ni idea de si también son representativos de los ordenadores domésticos, los ordenadores de empresa, los ordenadores gubernamentales y otros. Con un poco de esfuerzo podríamos haber conseguido un par de muestras de otros tipos de ordenadores, pero incluso así sería absurdo extrapolarlos a todos los ordenadores del tipo medido.

Volviendo por un momento al método de la lista libre, sólo es necesario mantener un bloque de punteros en la memoria principal. Cuando se crea un archivo, los bloques necesarios se toman del bloque de punteros. Cuando se agota, se lee un nuevo bloque de punteros del disco. Del mismo modo, cuando se elimina un archivo, se liberan sus bloques y se añaden al bloque de punteros de la memoria principal. Cuando este bloque se llena, se escribe en el disco.

Bajo ciertas circunstancias, este método conduce a E/S de disco innecesarias. Considere la situación de la Fig. 4-23(a), en la que el bloque de punteros en memoria sólo tiene espacio para dos entradas más. Si se libera un fichero de tres bloques, el bloque de punteros se desborda y tiene que escribirse en disco, lo que lleva a la situación de la Fig. 4-23(b).  Si ahora se escribe un fichero de tres bloques, hay que volver a leer el bloque completo de punteros, lo que nos lleva de nuevo a la Fig. 4-23(a).  Si el archivo de tres bloques que se acaba de escribir era un archivo temporal, cuando se libera, se necesita otra escritura en disco para escribir el bloque completo de punteros de nuevo en el disco. En resumen, cuando el bloque de punteros está casi vacío, una serie de archivos temporales de corta duración pueden causar muchos I/O de disco.

Un enfoque alternativo que evita la mayor parte de esta I/O de disco es dividir el bloque completo de punteros. Así, en lugar de pasar de la Fig. 4-23(a) a la Fig. 4-23(b), pasamos de la Fig. 4-23(a) a la Fig. 4-23(c) cuando se liberan tres bloques. Ahora el sistema puede manejar una serie de archivos temporales sin hacer ninguna I/O de disco. Si el bloque en memoria se llena, se escribe en el disco, y se lee el bloque medio lleno del disco. La idea aquí es mantener la mayoría de los bloques de punteros en el disco llenos (para minimizar el uso del disco), pero mantener el que está en memoria medio lleno, para que pueda manejar tanto la creación como la eliminación de archivos sin I/O de disco en la lista libre.


Con un mapa de bits, también es posible mantener sólo un bloque en memoria, yendo al disco a por otro sólo cuando esté completamente lleno o vacío. Una ventaja adicional de este enfoque es que, al realizar toda la asignación a partir de un único bloque del mapa de bits, los bloques de disco estarán muy juntos, minimizando así el movimiento del brazo del disco.

![figure4-23](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-23.png?raw=true)

Dado que el mapa de bits es una estructura de datos de tamaño fijo, si el kernel está (parcialmente) paginado, el mapa de bits puede colocarse en memoria virtual y paginarse según sea necesario.

### Disk Quotas

Para evitar que los usuarios acaparen demasiado espacio en disco, los sistemas operativos multiusuario suelen ofrecer un mecanismo para imponer cuotas de disco. La idea es que el administrador del sistema asigne a cada usuario una asignación máxima de archivos y bloques, y el sistema operativo se asegure de que los usuarios no excedan sus cuotas. A continuación se describe un mecanismo típico.

Cuando un usuario abre un archivo, se localizan los atributos y las direcciones de disco y se introducen en una tabla de archivos abiertos en la memoria principal. Entre los atributos hay una entrada que indica quién es el propietario. Cualquier aumento en el tamaño del archivo se cargará a la cuota del propietario.

Una segunda tabla contiene el registro de cuota de cada usuario con un archivo actualmente abierto, aunque el archivo haya sido abierto por otra persona. Esta tabla se muestra en la Fig. 4-24. Es un extracto de un archivo de cuotas en disco para los usuarios cuyos archivos están actualmente abiertos. Cuando se cierran todos los archivos, el registro se vuelve a escribir en el archivos de cuotas.

Cuando se crea una nueva entrada en la tabla de archivos abiertos, se introduce en ella un puntero al registro de cuotas del propietario, para facilitar la búsqueda de los distintos límites. Cada vez que se añade un bloque a un archivo, se incrementa el número total de bloques cargados al propietario y se comprueban los soft y hard limits. Es posible que se supere el soft limit, pero no el hard limit. Si se intenta añadir datos a un archivo cuando se ha alcanzado el hard limit de bloques, se producirá un error. También existen comprobaciones análogas para el número de archivos para evitar que un usuario acapare todos los i-nodos

Cuando un usuario intenta iniciar sesión, el sistema examina el archivo de cuotas para ver si el usuario ha superado el límite suave para el número de archivos o el número de bloques de disco.

![figure4-24](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-24.png?raw=true)

Si se ha violado alguno de los límites, se muestra una advertencia y el recuento de advertencias restantes se reduce en uno. Si la cuenta llega a cero, el usuario ha ignorado la advertencia demasiadas veces y no se le permite iniciar sesión. 

Para obtener permiso para iniciar sesión de nuevo será necesario hablar con el administrador del sistema. Este método tiene la propiedad de que los usuarios pueden superar sus soft limits durante una sesión de inicio de sesión, siempre que eliminen el exceso antes de cerrar la sesión. Los hard limits no pueden superarse nunca.

## 4.4.2 File-System Backups

La destrucción de un sistema de archivos suele ser un desastre mucho mayor que la destrucción de un ordenador. Si un ordenador queda destruido por un incendio, un rayo o una taza de café derramada sobre el teclado, es molesto y costará dinero, pero por lo general se puede comprar uno nuevo con un mínimo de complicaciones. Los ordenadores personales más baratos pueden sustituirse incluso en una hora con sólo ir a una tienda de informática (excepto en las universidades, donde la emisión de una orden de compra requiere tres comités, cinco firmas y 90 días).

Si el sistema de archivos de un ordenador se pierde irrevocablemente, ya sea por culpa del hardware o del software, restaurar toda la información será difícil, llevará mucho tiempo y, en muchos casos, será imposible. Para las personas cuyos programas, documentos, registros fiscales, archivos de clientes, bases de datos, planes de marketing u otros datos desaparecen para siempre, las consecuencias pueden ser catastróficas. Aunque el sistema de archivos no puede ofrecer ninguna protección contra la destrucción física de los equipos y soportes, sí puede ayudar a proteger la información.  Es bastante sencillo: hacer copias de seguridad.  Pero no es tan sencillo como parece. Echemos un vistazo.

La mayoría de la gente no cree que merezca la pena dedicar tiempo y esfuerzo a hacer copias de seguridad de sus archivos, hasta que un buen día su disco se estropea abruptamente, momento en el que la mayoría sufre por no poder recuperar su información. Sin embargo, las empresas (normalmente) son conscientes del valor de sus datos y suelen hacer copias de seguridad al menos una vez al día, a menudo en cinta.  Las cintas modernas tienen capacidad para cientos de gigabytes y cuestan céntimos por gigabyte. Sin embargo, hacer copias de seguridad no es tan trivial como parece, por lo que a continuación examinaremos algunas de las cuestiones relacionadas.

as copias de seguridad en cinta suelen realizarse para solucionar uno de estos dos posibles problemas:

1. Recuperarse de un desastre.
2. Recuperarse de errores simples.

La primera consiste en volver a poner en marcha el ordenador tras un fallo de disco, un incendio, una inundación u otra catástrofe natural. En la práctica, estas cosas no ocurren muy a menudo, por lo que mucha gente no se molesta en hacer copias de seguridad. Por la misma razón, estas personas tampoco suelen tener un seguro contra incendios en sus casas.

La segunda razón es que los usuarios suelen eliminar accidentalmente archivos que luego vuelven a necesitar. Este problema ocurre tan a menudo que, cuando un archivo se ''elimina'' en Windows, no se borra del todo, sino que se mueve a un directorio especial, la papelera de reciclaje, para que pueda ser pescado y restaurado fácilmente más tarde. Las copias de seguridad llevan este principio más allá y permiten recuperar archivos borrados hace días, o incluso semanas, a partir de viejas cintas de copia de seguridad.
   
Hacer una copia de seguridad lleva mucho tiempo y ocupa mucho espacio, por lo que hacerla de forma eficaz y cómoda es importante. Estas consideraciones plantean las siguientes cuestiones. En primer lugar, ¿se debe hacer una copia de seguridad de todo el sistema de archivos o sólo de una parte?  En muchas instalaciones, los programas ejecutables (binarios) se guardan en una parte limitada del árbol del sistema de archivos. No es necesario hacer una copia de seguridad de estos archivos si todos ellos se pueden reinstalar desde el sitio web del fabricante o desde el DVD de instalación.  Además, la mayoría de los sistemas tienen un directorio para archivos temporales. Tampoco suele ser necesario hacer una copia de seguridad. En UNIX, todos los archivos especiales (dispositivos de E/S) se guardan en un directorio/dev. No sólo no es necesario hacer una copia de seguridad de este directorio, sino que es francamente peligroso porque el programa de copia de seguridad se colgaría para siempre si intentara leer cada uno de ellos hasta el final.  En resumen, normalmente es deseable hacer copias de seguridad sólo de directorios específicos y de todo lo que contienen en lugar de todo el sistema de ficheros.

En segundo lugar, es un desperdicio hacer copias de seguridad de archivos que no han cambiado desde la copia anterior, lo que nos lleva a la idea de los **incremental dumps**. La forma más sencilla de hacer un dump incremental es hacer un dump completo (copia de seguridad) periódicamente, por ejemplo semanal o mensualmente, y hacer un dump diario sólo de los archivos que se han modificado desde el último dump completo. Aún mejor es dumpear sólo los archivos que han cambiado desde el último dump. Aunque este esquema minimiza el tiempo de dump, complica la recuperación, ya que primero hay que restaurar el dump completo más reciente, seguido de todos los dump incrementales en orden inverso. Para facilitar la recuperación, a menudo se utilizan esquemas de dump incremental más sofisticados.

En tercer lugar, como normalmente se vuelcan cantidades inmensas de datos, puede ser conveniente comprimirlos antes de escribirlos en la cinta. Sin embargo, con muchos algoritmos de compresión, un solo punto defectuoso en la cinta de copia de seguridad puede frustrar el algoritmo de descompresión y hacer ilegible un archivo entero o incluso una cinta entera. Por lo tanto, la decisión de comprimir el flujo de copia de seguridad debe considerarse cuidadosamente.

En cuarto lugar, es difícil realizar una copia de seguridad en un sistema de archivos activo. Si se añaden, eliminan y modifican archivos y directorios durante el proceso de dump, el dump resultante puede ser incoherente. Sin embargo, como hacer un dump puede llevar horas, puede ser necesario desconectar el sistema durante gran parte de la noche para hacer la copia de seguridad, algo que no siempre es aceptable. Por este motivo, se han ideado algoritmos para realizar fotografías del estado del sistema de archivos copiando las estructuras de datos críticas y, a continuación, solicitando a los futuros cambios en los archivos y directorios que copien los bloques en lugar de actualizarlos(Hutchinson et al., 1999). De este modo, el sistema de archivos queda congelado en el momento de la foto, por lo que se puede hacer una copia de seguridad a posteriori.

Quinto y último, hacer copias de seguridad introduce muchos problemas no técnicos en una organización. El mejor sistema de seguridad en línea del mundo puede ser inútil si el administrador del sistema guarda todos los discos o cintas de copia de seguridad en su despacho y lo deja abierto y sin vigilancia cada vez que sale al pasillo a por café. Todo lo que tiene que hacer un espía es asomarse un segundo, meterse un pequeño disco o cinta en el bolsillo y marcharse alegremente. Adiós a la seguridad. Además, hacer una copia de seguridad diaria sirve de poco si el incendio que quema los ordenadores también quema todos los discos de copia de seguridad. Por esta razón, los discos de copia de seguridad deberían guardarse fuera de las instalaciones, pero esto introduce más riesgos de seguridad (porque ahora hay que proteger dos instalaciones). Para un análisis exhaustivo de estas y otras cuestiones prácticas de administración, véase Nemeth et al. (2013). A continuación, sólo trataremos las cuestiones técnicas relacionadas con las copias de seguridad de los sistemas de archivos.

Se pueden utilizar dos estrategias para el dump de un disco a un disco de copia de seguridad: un physical dump o un logical dump. Un **physical dump** comienza en el bloque 0 del disco, escribe todos los bloques del disco en el disco de salida en orden, y se detiene cuando ha copiado el último. Un programa de este tipo es tan sencillo que probablemente se pueda hacer 100% libre de errores, algo que probablemente no se puede decir de ningún otro programa útil.Sin embargo, vale la pena hacer varios comentarios sobre el dump físico. En primer lugar, no tiene sentido hacer copias de seguridad de los bloques de disco no utilizados. Si el programa de volcado puede acceder a la estructura de datos de bloques libres, puede evitar volcar los bloques no utilizados. Sin embargo, saltarse bloques no utilizados requiere escribir el número de cada bloque delante del bloque (o el equivalente), puesto que ya no es cierto que el bloque k de la copia de seguridad fuera el bloque k del disco.

Otra preocupación es el dump de bloques defectuosos. Es casi imposible fabricar discos grandes sin defectos.  Siempre hay algunos bloques defectuosos. A veces, cuando se realiza un formateo de bajo nivel, los bloques defectuosos se detectan, se marcan como defectuosos y se sustituyen por bloques de repuesto reservados al final de cada pista para este tipo de emergencias.  En muchos casos, la controladora de disco gestiona la sustitución de los bloques defectuosos de forma transparente, sin que el sistema operativo se entere.

Sin embargo, a veces los bloques se estropean después de formatearlos, en cuyo caso el sistema operativo acabará detectándolos. Normalmente, resuelve el problema creando un "archivo" con todos los bloques defectuosos para asegurarse de que nunca aparezcan en la reserva de bloques libres ni se asignen. Ni que decir tiene que este archivo es completamente ilegible.

Si todos los bloques defectuosos son reasignados por la controladora de disco y se ocultan al sistema operativo como se acaba de describir, el dump físico funciona correctamente.  Por otra parte, si son visibles para el sistema operativo y se mantienen en uno o más archivos de bloques defectuosos o mapas de bits, es absolutamente esencial que el programa de dump físico tenga acceso a esta información y evite volcarlos para evitar interminables errores de lectura del disco al intentar realizar una copia de seguridad del archivo de bloques defectuosos.

Los sistemas Windows tienen archivos de paginación e hibernación que no se necesitan en caso de restauración y de los que no se debe hacer copia de seguridad. Los sistemas específicos también pueden tener otros archivos internos de los que no se debe hacer copia de seguridad, por lo que el programa de dumping debe tenerlos en cuenta.

Las principales ventajas del dump físico son su sencillez y su gran velocidad (básicamente, puede ejecutarse a la velocidad del disco). Las principales desventajas son la imposibilidad de omitir directorios seleccionados, realizar dump incrementales y restaurar archivos individuales a petición. Por estas razones, la mayoría de las instalaciones realizan volcados lógicos.

Un **logical dump** comienza en uno o más directorios especificados y vuelca recursivamente todos los archivos y directorios que se encuentran allí y que han cambiado desde una fecha base determinada (por ejemplo, la última copia de seguridad para un dump incremental o la instalación del sistema para un dump completo). Así, en un dump lógico, el disco de dumping recibe una serie de directorios y archivos cuidadosamente identificados, lo que facilita la restauración de un archivo o directorio específico a petición.

Dado que el dump lógico es la forma más común, examinemos un algoritmo común en detalle utilizando el ejemplo de la Fig. 4-25 para guiarnos. La mayoría de los sistemas UNIX utilizan este algoritmo. En la figura vemos un árbol de archivos con directorios (cuadrados) y archivos (círculos). Los elementos sombreados han sido modificados desde la fecha base y por tanto necesitan ser volcados. Los que no están sombreados no necesitan ser volcados.

Este algoritmo también vuelca todos los directorios (incluso los no modificados) que se encuentran en la ruta de un archivo o directorio modificado por dos razones. La primera razón es hacer posible restaurar los archivos y directorios volcados a un sistema de archivos nuevo en un ordenador diferente. De este modo, los programas de volcado y restauración pueden utilizarse para transportar sistemas de archivos enteros entre ordenadores.

La segunda razón para volcar directorios no modificados por encima de archivos modificados es hacer posible la restauración incremental de un solo archivo (posiblemente para manejar la recuperación de la estupidez). Supongamos que se realiza un volcado completo del sistema de archivos el domingo por la noche y un volcado incremental el lunes por la noche.  El martes se elimina el directorio/usr/jhs/proj/nr3, junto con todos los directorios y archivos que contiene. Supongamos que el miércoles por la mañana el usuario quiere restaurar el archivo/usr/jhs/proj/nr3/plans/summary. Sin embargo, no es posible restaurar simplemente el archivoummary porque no hay ningún lugar donde colocarlo. Primero hay que restaurar los directorios nr3 y plans.  Para obtener sus propietarios, modos, tiempos, y lo que sea, correcto, estos di-rectorios deben estar presentes en el disco de volcado a pesar de que ellos mismos no fueron modificados desde el volcado completo anterior.


![figure4-25](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-25.png?raw=true)

El algoritmo de dumping mantiene un mapa de bits indexado por número de nodo-i con sever-al bits por nodo-i. Los bits se activan y desactivan en este mapa a medida que avanza el algoritmo.  El algoritmo funciona en cuatro fases. La fase 1 comienza en la dirección de inicio (la raíz en este ejemplo) y examina todas las entradas que contiene. Para cada archivo modificado, se marca su nodo i en el mapa de bits. También se marca cada directorio (tanto si ha sido modificado como si no) y, a continuación, se inspecciona recursivamente.

Al final de la fase 1, todos los archivos modificados y todos los directorios han sido marcados en el mapa de bits, como se muestra (por sombreado) en la Fig. 4-26(a).  La fase 2 vuelve a recorrer conceptualmente el árbol de forma recursiva, desmarcando todos los directorios que no tengan archivos o directorios modificados en ellos o debajo de ellos. Esta fase deja el mapa de bits como se muestra en la Fig. 4-26(b). Observe que los directorios 10, 11, 14, 27, 29 y 30 están ahora sin marcar porque no contienen nada bajo ellos que haya sido modificado. No serán volcados.  Por el contrario, los directorios 5 y 6 serán volcados aunque no hayan sido modificados porque serán necesarios para restaurar los cambios de hoy en una máquina nueva. Por razones de eficacia, las fases 1 y 2 pueden combinarse en un recorrido por el árbol.

En este punto se sabe qué directorios y ficheros deben ser volcados. Estos son los que están marcados en la Fig. 4-26(b).  La fase 3 consiste en escanear los nodos-i en orden numérico y volcar todos los directorios marcados para ser volcados.

Se muestran en la Fig. 4-26(c).  A cada directorio se le anteponen sus atributos (propietario, hora, etc.) para que puedan ser restaurados.   Finalmente, en la fase 4, los archivos marcados en la Fig. 4-26(d) también son volcados, de nuevo precedidos por sus atributos.  Esto completa el volcado.

Restaurar un sistema de archivos desde el disco de dumping es sencillo.  Para empezar, se crea un sistema de archivos vacío en el disco. A continuación, se vuelve a almacenar el volcado completo más reciente. Dado que los directorios aparecen en primer lugar en el disco de volcado, se restauran todos primero, lo que proporciona un esqueleto del sistema de archivos. A continuación, se restauran los archivos. Este proceso se repite con el primer volcado incremental realizado después del volcado completo, luego con el siguiente, etc.

Aunque el volcado lógico es sencillo, hay algunas cuestiones delicadas. Por un lado, como la lista de bloques libres no es un archivo, no se vuelca y, por tanto, debe reconstruirse desde cero una vez restaurados todos los volcados. Esto siempre es posible, ya que el conjunto de bloques libres no es más que el complemento del conjunto de bloques contenidos en todos los ficheros combinados.

Otro problema son los enlaces. Si un archivo está vinculado a dos o más directorios, es importante que el archivo se restaure una sola vez y que todos los directorios que se supone que deben apuntar a él lo hagan.

Otro problema es que los archivos UNIX pueden contener agujeros. Es legal abrir un archivo, escribir unos pocos bytes, luego buscar un offset de archivo distante y escribir unos pocos bytes más. Los bloques intermedios no forman parte del archivo y no deben volcarse ni restaurarse. Los core files a menudo tienen un hueco de cientos de megabytes entre el segmento de datos y la pila. Si no se maneja correctamente, cada core file restaurado llenará esta área con ceros y, por lo tanto, tendrá el mismo tamaño que el espacio de direcciones virtual (por ejemplo, 2^32bytes, o peor aún, 2^64bytes).

Por último, los archivos especiales, las tuberías con nombre y similares (cualquier cosa que no sea un archivo real) nunca deben volcarse, independientemente del directorio en el que se encuentren (no es necesario que se limiten a /dev).  Para más información sobre copias de seguridad de sistemas de ficheros, véase Chervenak et al. (1998) y Zwicky (1991).

## 4.4.3 File-System Consistency

Otra área en la que la fiabilidad es un problema es la consistencia del sistema de archivos. Muchos sistemas de archivos leen bloques, los modifican y los escriben más tarde. Si el sistema falla antes de que se hayan escrito todos los bloques modificados, el sistema de archivos puede quedar en un estado inconsistente. Este problema es especialmente crítico si algunos de los bloques que no se han escrito son bloques de nodo-i, bloques de directorio o bloques que contienen la lista libre.

Para hacer frente a los sistemas de archivos inconsistentes, la mayoría de los ordenadores tienen un programa de utilidad que comprueba la coherencia del sistema de archivos. Por ejemplo, UNIX tiene fsck; Windows tiene sfc(y otros). Esta utilidad puede ejecutarse siempre que se arranque el sistema, especialmente después de un fallo. La siguiente descripción explica cómo funciona fsck. Sfc es algo diferente porque funciona en un sistema de archivos diferente, pero el principio general de utilizar la redundancia inherente del sistema de archivos para repararlo sigue siendo válido. Todos los comprobadores de sistemas de archivos verifican cada sistema de archivos (partición de disco) independientemente de los demás.

Se pueden realizar dos tipos de comprobaciones de consistencia: bloques y archivos.  Para comprobar la coherencia de los bloques, el programa crea dos tablas, cada una de las cuales contiene un contador para cada bloque, inicialmente fijado en 0. Los contadores de la primera tabla registran cuántas veces está presente cada bloque en un archivo; los contadores de la segunda tabla registran cuántas veces está presente cada bloque en la lista de bloques libres (o el mapa de bits de bloques libres).

A continuación, el programa lee todos los nodos-i utilizando un dispositivo en bruto, que ignora la estructura del archivo y sólo devuelve todos los bloques de disco empezando por 0. A partir de un nodo-i, es posible construir una lista de todos los números de bloque utilizados en el archivo correspondiente.  A medida que se lee cada número de bloque, se incrementa su contador en la primera tabla. A continuación, el programa examina la lista libre o mapa de bits para encontrar todos los bloques que no están en uso. Cada vez que aparece un bloque en la lista de bloques libres, se incrementa su contador en la segunda tabla.

Si el sistema de archivos es consistente, cada bloque tendrá un 1 en la primera tabla o en la segunda tabla, como se ilustra en la Fig. 4-27(a).  Sin embargo, como resultado de un crash, las tablas podrían tener el aspecto de la Fig. 4-27(b), en la que el bloque 2 no aparece en ninguna de las tablas. Se informará de que hay un **missing block**. Si bien los bloques faltantes no causan ningún daño real, desperdician espacio y, por lo tanto, reducen la capacidad del disco. La solución a los bloques perdidos es sencilla: el verificador del sistema de archivos simplemente los añade a la lista de libres.

Otra situación que puede darse es la de la Fig. 4-27(c).  Aquí vemos un bloque, el número 4, que aparece dos veces en la lista libre. (Los duplicados sólo pueden ocurrir si la lista libre es realmente una lista; con un mapa de bits es imposible). La solución aquí también es simple: reconstruir la lista libre.

Lo peor que puede ocurrir es que el mismo bloque de datos esté presente en dos o más archivos, como se muestra en la Fig. 4-27(d) con el bloque 5. Si alguno de estos archivos se mueve de nuevo, el bloque 5 pasará a la lista de libres. Si cualquiera de estos archivos es movido, el bloque 5 será puesto en la lista de libres, llevando a una situación en la que el mismo bloque está en uso y libre al mismo tiempo. Si se eliminan ambos archivos, el bloque se pondrá en la lista de libres dos veces.

![figure4-27](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-27.png?raw=true)

La acción apropiada para el verificador del sistema de archivos es asignar un bloque libre, copiar el contenido del bloque 5 en él, e insertar la copia en uno de los archivos. De esta manera, el contenido de la información de los archivos no cambia (aunque casi con seguridad uno está distorsionado), pero la estructura del sistema de archivos al menos se hace consistente. El error debe ser reportado, para permitir al usuario inspeccionar el daño.

Además de comprobar que cada bloque está debidamente contabilizado, el comprobador del sistema de archivos también comprueba el sistema de directorios. También utiliza una tabla de contadores, pero éstos son por archivo, en lugar de por bloque. Comienza en el directorio raíz y desciende recursivamente por el árbol, inspeccionando cada directorio del sistema de archivos. Para cada i-nodo en cada directorio, incrementa un contador para el recuento de uso de ese archivo. Recuerde que, debido a los hard links, un archivo puede aparecer en dos o más directorios. Los symbolic links no cuentan y no hacen que se incremente el contador del archivo de destino.

Cuando el verificador ha terminado, tiene una lista, indexada por número de nodo-i, que indica cuántos directorios contienen cada archivo.  A continuación, compara estos números con los recuentos de links almacenados en los propios nodos-i. Estos recuentos comienzan en 1 cuando se crea un archivo y se incrementan cada vez que se realiza un enlace (duro) al archivo.  En un sistema de archivos coherente, ambos recuentos coinciden. Sin embargo, pueden producirse dos tipos de errores: el recuento de enlaces en el nodo-i puede ser mayor o menor.

Si el recuento de enlaces es mayor que el número de entradas de directorio, aunque se eliminen todos los archivos de los directorios, el recuento seguirá siendo distinto de cero y el nodo i no se eliminará. Este error no es grave, pero desperdicia espacio en el disco con ficheros que no están en ningún directorio. Debería solucionarse ajustando el recuento de enlaces en el nodo-i al valor correcto.

El otro error es potencialmente catastrófico. Si dos entradas de directorio están vinculadas a un archivo, pero el nodo-i dice que sólo hay una, cuando cualquiera de las entradas de directorio se mueve de nuevo, el recuento del nodo-i irá a cero. Cuando el recuento de un nodo-i llega a cero, el sistema de archivos lo marca como no utilizado y libera todos sus bloques. Esta acción provocará que uno de los directorios apunte ahora a un nodo-i no utilizado, cuyos bloques podrían asignarse pronto a otros archivos. De nuevo, la solución es simplemente forzar el recuento de links en el nodo-i al número real de entradas de directorio.

Estas dos operaciones, la comprobación de bloques y la comprobación de directorios, suelen estar integradas por razones de eficiencia (es decir, sólo se requiere una pasada por los nodos-i). También es posible realizar otras comprobaciones. Por ejemplo, los directorios tienen un formato definido, con números de nodo-i y nombres ASCII. Si un número de nodo-i es mayor que el número de nodos-i del disco, el directorio está dañado.

Además, cada nodo-i tiene un modo, algunos de los cuales son legales pero extraños, como 0007, que no permite al propietario y su grupo ningún acceso, pero permite a los externos leer, escribir y ejecutar el archivo.  Podría ser útil al menos informar de los archivos que dan a los extraños más derechos que al propietario. Los directorios con más de, digamos, 1000 entradas también son sospechosos. Los archivos ubicados en directorios de usuario, pero que son propiedad del superusuario y tienen el bit SETUID activado, son potenciales problemas de seguridad porque tales archivos adquieren los poderes del superusuario cuando son ejecutados por cualquier usuario. Con un poco de esfuerzo, se puede confeccionar una lista bastante larga de situaciones técnicamente legales pero peculiares que merecen ser reportadas.

En los párrafos anteriores se ha tratado el problema de la protección del usuario frente a las caídas. Algunos sistemas de ficheros también se preocupan de proteger al usuario contra sí mismo.  Si el usuario intenta escribir

``` shell
    rm *.o
```

para eliminar todos los archivos que terminan en .o (archivos objeto generados por el compilador), pero accidentalmente escribe

``` shell
    rm * .o
```

(observe el espacio después del asterisco), rm eliminará todos los archivos del directorio actual y se quejará de que no puede encontrar .o. En Windows, los archivos que se eliminan se colocan en la papelera de reciclaje (un directorio especial), desde donde se pueden recuperar más tarde si es necesario. Por supuesto, no se recupera almacenamiento hasta que se eliminan realmente de este directorio.

## 4.4.4 File System Performance

El acceso al disco es mucho más lento que el acceso a la memoria. La lectura de una palabra de memoria de 32 bits puede tardar 10 nseg.  La lectura desde un disco duro puede realizarse a 100 MB/s, lo que es cuatro veces más lento por palabra de 32 bits, pero a esto hay que añadir 5-10 mseg para buscar la pista y esperar a que el sector deseado llegue bajo la cabeza de lectura. Si sólo se necesita una palabra, el acceso a la memoria es del orden de un millón de veces más rápido que el acceso al disco. Como resultado de esta diferencia en el tiempo de acceso, muchos sistemas de archivos se han diseñado con diversas optimizaciones para mejorar el rendimiento.  En esta sección trataremos tres de ellas.


### Caching

La técnica más utilizada para reducir los accesos al disco es la caché de bloques o caché de búfer. (Caché se pronuncia ''cash'' y deriva del francéscacher, que significa esconder). En este contexto, una caché es una colección de bloques que lógicamente pertenecen al disco, pero que se mantienen en memoria por razones de rendimiento.

Se pueden utilizar varios algoritmos para gestionar la caché, pero uno común es comprobar todas las peticiones de lectura para ver si el bloque necesario está en la caché. Si lo está, la solicitud de lectura puede satisfacerse sin acceder al disco. Si el bloque no está en la caché, primero se lee en la caché y luego se copia donde sea necesario. Las solicitudes posteriores del mismo bloque se pueden satisfacer desde la caché.

El funcionamiento de la caché se ilustra en la Fig. 4-28.  Dado que hay muchos (a menudo miles) bloques en la caché, se necesita alguna forma de determinar rápidamente si un bloque está presente. La forma habitual es hacer un hash de la dirección del dispositivo y del disco y buscar el resultado en una tabla hash. Todos los bloques con el mismo valor hash se encadenan en una lista enlazada para poder seguir la cadena de colisiones.

![figure4-28](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-28.png?raw=true)

Cuando hay que cargar un bloque en una caché llena, hay que eliminar algún bloque(y reescribirlo en el disco si se ha modificado desde que se introdujo). Esta situación es muy parecida a la paginación, y todos los algoritmos habituales de sustitución de páginas descritos en el capítulo 3, como FIFO, segunda oportunidad y LRU, son aplicables. Una agradable diferencia entre paginación y caché es que las referencias a la caché son relativamente infrecuentes, por lo que es factible mantener todos los bloques en el orden exacto LRU con listas enlazadas.

En la Fig. 4-28, vemos que además de las cadenas de colisión que comienzan en la tabla de hash, también hay una lista bidireccional que recorre todos los bloques en el orden de uso, con el bloque usado menos recientemente al principio de esta lista y el bloque usado más recientemente al final. Cuando se hace referencia a un bloque, se puede quitar de su posición en la lista bidireccional y ponerlo al final. De este modo, se puede mantener el orden LRU exacto.

Por desgracia, hay un inconveniente. Ahora que tenemos una situación en la que la LRU exacta es posible, resulta que LRU no es deseable. El problema tiene que ver con las caídas y la consistencia del sistema de archivos que se comentaron en la sección anterior. Si un bloque crítico, como un bloque de nodo-i, se lee en la caché y se modifica, pero no se reescribe en el disco, una caída dejará el sistema de ficheros en un estado inconsistente. Si el bloque i-node se coloca al final de la cadena LRU, puede pasar bastante tiempo antes de que llegue al principio y se reescriba en el disco.

Además, algunos bloques, como los de nodo i, rara vez se referencian dos veces en un intervalo corto. Estas consideraciones conducen a un esquema LRU modificado, que tiene en cuenta dos factores:

1. ¿Es probable que el bloque vuelva a necesitarse pronto? 
2. ¿Es el bloque esencial para la coherencia del sistema de archivos?
   
En ambos casos, los bloques pueden dividirse en categorías, como bloques de nodo-i, bloques indirectos, bloques de directorio, bloques de datos completos y bloques de datos parcialmente completos. Los bloques que podrían volver a necesitarse pronto, como un bloque parcialmente lleno que se está escribiendo, van al final de la lista, por lo que permanecerán durante mucho tiempo.

La segunda cuestión es independiente de la primera. Si el bloque es esencial para la coherencia del sistema de archivos (básicamente, todo excepto los bloques de datos), y ha sido modificado, debe escribirse en el disco inmediatamente, independientemente del final de la lista LRU en el que se encuentre. Al escribir rápidamente los bloques críticos, reducimos en gran medida la probabilidad de que un fallo destroce el sistema de ficheros. Aunque un usuario puede estar descontento si uno de sus archivos se estropea en un fallo, es probable que esté mucho más descontento si se pierde todo el sistema de archivos.

Incluso con esta medida para mantener intacta la integridad del sistema de archivos, no es deseable mantener bloques de datos en la caché demasiado tiempo antes de escribirlos. Considere la situación de alguien que está utilizando un ordenador personal para escribir un libro. Incluso si nuestro escritor le dice periódicamente al editor que escriba el archivo que está editando en el disco, hay muchas posibilidades de que todo siga en la caché y nada en el disco. Si el sistema se bloquea, la estructura del sistema de archivos no se dañará, pero se perderá el trabajo de todo un día.

Esta situación no tiene por qué darse muy a menudo para que tengamos un usuario bastante descontento. Los sistemas adoptan dos enfoques para resolverlo. La forma UNIX es tener una llamada al sistema, sync, que fuerza todos los bloques modificados al disco inmediatamente. Cuando se inicia el sistema, un programa, normalmente llamado *update*, se pone en marcha en segundo plano para permanecer en un bucle sin fin emitiendo llamadas a sync, durmiendo 30 segundos entre llamadas. Como resultado, no se pierden más de 30 segundos de trabajo por un fallo.

Aunque Windows tiene ahora una llamada al sistema equivalente a sync, llamada *FlushFile-Buffers*, en el pasado no la tenía. En su lugar, tenía una estrategia diferente que era en cierto modo mejor que el enfoque UNIX (y en cierto modo peor).  Lo que hacía era escribir cada bloque modificado en el disco tan pronto como se escribía en la caché. Las cachés en las que todos los bloques modificados se escriben inmediatamente en el disco se denominan **write-through caches**. Requieren más I/O de disco que las cachés sin escritura.

La diferencia entre estos dos enfoques se puede ver cuando un programa escribe un bloque de 1-KB lleno, un carácter cada vez. UNIX recogerá todos los caracteres en la caché y escribirá el bloque una vez cada 30 segundos, o cada vez que el bloque sea eliminado de la caché. Con una caché de escritura, hay un acceso al disco por cada carácter escrito. Por supuesto, la mayoría de los programas hacen buffering interno, por lo que normalmente no escriben un caracter, sino una línea o una unidad mayor en cada llamada al sistema de escritura.

Una consecuencia de esta diferencia en la estrategia de almacenamiento en caché es que la simple eliminación de un disco de un sistema UNIX sin hacer una sincronización casi siempre resultará en la pérdida de datos, y con frecuencia también en un sistema de archivos corrupto. Con write-through caching no surge ningún problema. Estas estrategias diferentes se eligieron porque UNIX se desarrolló en un entorno en el que todos los discos eran duros y no extraíbles, mientras que el primer sistema de archivos de Windows se heredó de MS-DOS, que empezó en el mundo de los disquetes.  A medida que los discos duros se convirtieron en la norma, el enfoque UNIX, con su mejor eficiencia (pero peor fiabilidad), se convirtió en la norma, y ahora también se utiliza en Windows para los discos duros. Sin embargo, NTFS toma otras medidas (por ejemplo, el registro en el journaling) para mejorar la fiabilidad, como se ha comentado anteriormente.

Algunos sistemas operativos integran la caché de memoria intermedia con la caché de páginas. Esto es especialmente atractivo cuando se soportan archivos mapeados en memoria. Si un archivo está mapeado en memoria, entonces algunas de sus páginas pueden estar en memoria porque fueron paginadas bajo demanda. Estas páginas apenas se diferencian de los bloques de archivo en la caché de búfer.  En este caso, pueden tratarse del mismo modo, con una única caché para los bloques de archivo y las páginas.

### Block Read Ahead

Una segunda técnica para mejorar el rendimiento percibido del sistema de archivos es intentar introducir bloques en la caché antes de que sean necesarios para aumentar la tasa de aciertos. En particular, muchos archivos se leen secuencialmente. Cuando se pide al sistema de archivos que produzca el bloque k en un archivo, lo hace, pero cuando termina, hace una comprobación furtiva en la caché para ver si el bloque k + 1 ya está allí. Si no lo está, programa una lectura para el bloque k + 1 con la esperanza de que, cuando se necesite, ya habrá llegado a la caché y, como mínimo, estará en camino.

Por supuesto, esta estrategia de lectura anticipada sólo funciona para archivos que se leen secuencialmente. Si un archivo está siendo accedido aleatoriamente, la lectura anticipada no ayuda. De hecho, perjudica al ocupar ancho de banda de disco leyendo bloques inútiles y eliminando bloques potencialmente útiles de la caché (y posiblemente ocupando más ancho de banda de disco escribiéndolos de nuevo al disco si están sucios). Para ver si merece la pena adelantar la lectura, el sistema de archivos puede hacer un seguimiento de los patrones de acceso a cada archivo abierto. Por ejemplo, un bit asociado a cada fichero puede llevar la cuenta de si el fichero está en "modo de acceso secuencial" o en "modo de acceso aleatorio". Inicialmente, se da al archivo el beneficio de la duda y se pone en modo de acceso secuencial. Sin embargo, cada vez que se realiza una búsqueda, el bit se borra. Si vuelven a producirse lecturas secuenciales, el bit se activa de nuevo.  De esta forma, el sistema de archivos puede hacer una estimación razonable sobre si debe leer por adelantado o no. Si se equivoca de vez en cuando, no es un problema, sólo un poco de ancho de banda de disco desperdiciado.

## Reducing Disk-Arm Motion

El almacenamiento en caché y la lectura anticipada no son las únicas formas de aumentar el rendimiento del sistema de archivos. Otra técnica importante es reducir la cantidad de movimiento del brazo del disco colocando los bloques a los que es probable que se acceda en secuencia cerca unos de otros, preferiblemente en el mismo cilindro. Cuando se escribe un archivo de salida, el sistema de archivos tiene que asignar los bloques de uno en uno, bajo demanda. Si los bloques libres se registran en un mapa de bits, y todo el mapa de bits está en la memoria principal, es bastante fácil elegir un bloque libre lo más cerca posible del bloque anterior. Con una lista de libres, parte de la cual está en el disco, es mucho más difícil asignar bloques cercanos entre sí.

Sin embargo, incluso con una lista libre, se puede hacer algo de agrupación de bloques. El truco consiste en llevar la cuenta del almacenamiento en disco no en bloques, sino en grupos de bloques consecutivos.  Si todos los sectores constan de 512 bytes, el sistema podría utilizar bloques de 1 KB (2 sectores) pero asignar el almacenamiento en disco en unidades de 2 bloques (4 sectores). Esto no es lo mismo que tener bloques de disco de 2 KB, ya que la caché seguiría utilizando bloques de 1 KB y las transferencias de disco seguirían siendo de 1 KB, pero la lectura secuencial de un archivo en un sistema inactivo reduciría el número de búsquedas en un factor de dos, mejorando considerablemente el rendimiento.  Una variación sobre el mismo tema es tener en cuenta el posicionamiento rotacional.  Al asignar bloques, el sistema intenta colocar los bloques consecutivos de un archivo en el mismo cilindro.

Otro cuello de botella en el rendimiento de los sistemas que utilizan nodos-i o algo parecido es que la lectura incluso de un fichero corto requiere dos accesos al disco: uno para el nodo-i y otro para el bloque. La ubicación habitual de los nodos-i se muestra en la Fig. 4-29(a).  Aquí todos los nodos-i están cerca del inicio del disco, por lo que la distancia media entre un nodo-i y sus bloques será la mitad del número de cilindros, requiriendo largas búsquedas.

Una mejora sencilla del rendimiento es colocar los nodos-i en el centro del disco, en lugar de al principio, reduciendo así la búsqueda media entre el nodo-i y el primer bloque en un factor de dos. Otra idea, mostrada en la Fig. 4-29(b), es dividir el disco en grupos de cilindros, cada uno con sus propios i-nodos, bloques y lista libre(McKusick et al., 1984). Cuando se crea un nuevo archivo, se puede elegir cualquier nodo-i, pero se intenta encontrar un bloque en el mismo grupo de cilindros que el nodo-i. Si no hay ninguno disponible, entonces se crea un nuevo bloque. Si no hay ninguno disponible, se utiliza un bloque de un grupo de cilindros cercano.

Por supuesto, el movimiento del brazo del disco y el tiempo de rotación sólo son relevantes si el disco los tiene. Cada vez hay más ordenadores equipados con **solid-state disks (SSD)** que no tienen partes móviles. Para estos discos, construidos con la misma tecnología que las tarjetas flash, los accesos aleatorios son tan rápidos como los secuenciales y muchos de los problemas de los discos tradicionales desaparecen. Por desgracia, surgen nuevos problemas.

![figure4-29](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-29.png?raw=true)

Por ejemplo, los SSD tienen propiedades peculiares en lo que respecta a la lectura, escritura y borrado. En concreto, cada bloque sólo puede escribirse un número limitado de veces, por lo que hay que tener mucho cuidado para que el desgaste del disco sea uniforme.

## 4.4.5 Defragmenting Disks

Cuando el sistema operativo se instala por primera vez, los programas y archivos que necesita se instalan consecutivamente empezando por el principio del disco, cada uno directamente después del anterior. Todo el espacio libre del disco se encuentra en una única unidad contigua a continuación de los archivos instalados.  Sin embargo, a medida que pasa el tiempo, se crean y eliminan archivos y, normalmente, el disco se fragmenta mucho, con archivos y agujeros por todas partes.  Como consecuencia, cuando se crea un nuevo archivo, los bloques utilizados para él pueden estar repartidos por todo el disco, lo que reduce el rendimiento.

El rendimiento puede recuperarse moviendo los archivos para que sean contiguos y para poner todo (o al menos la mayor parte) del espacio libre en una o más regiones contiguas del disco. Los usuarios de Windows deberían ejecutarlo con regularidad, excepto en las unidades SSD.

La desfragmentación funciona mejor en sistemas de archivos que tienen mucho espacio libre en una región contigua al final de la partición. Este espacio permite al programa de desfragmentación seleccionar archivos fragmentados cerca del inicio de la partición y copiar todos sus bloques al espacio libre. Al hacerlo, se libera un bloque contiguo de espacio cerca del inicio de la partición en el que se pueden colocar los archivos originales u otros archivos de forma contigua. El proceso puede repetirse con el siguiente trozo de espacio en disco, etc.

Algunos archivos no se pueden mover, incluidos el archivo de paginación, el archivo de hibernación y el registro del journaling, porque la administración que se requeriría para hacerlo es más problemática de lo que vale la pena.  En algunos sistemas, se trata de áreas contiguas de tamaño fijo, por lo que no es necesario desfragmentarlas. El único momento en el que su falta de movilidad es un problema es cuando se encuentran cerca del final de la partición y el usuario quiere reducir el tamaño de la partición. La única forma de resolver este problema es eliminarlos por completo, redimensionar la partición y volver a crearla después.

Los sistemas de archivos Linux (especialmente ext2 y ext3) suelen sufrir menos la desfragmentación que los sistemas Windows debido a la forma en que se seleccionan los bloques de disco, por lo que rara vez es necesaria la desfragmentación manual. Además, las unidades SSD no sufren en absoluto la fragmentación. De hecho, desfragmentar un SSD es contraproducente. No sólo no aumenta el rendimiento, sino que las unidades SSD se desgastan, por lo que desfragmentarlas sólo acorta su vida útil.

# Preguntas

¿Qué pros y contras tienen una página grande y pequeña?

¿Cómo funcina el disk space utilization y el data rate(Mb/sec) con distintos tamaños de página.

¿Cuál es el tamaño de página recomendado y cómo afectan los tamaños de página?

¿Cómo se mantiene el registro de los discos libres con una lista enlazada?
¿Cómo se mantiene el registro de los discos libres con la otra estrategia?


¿Qué es una disk Quota?

¿Cómo funciona el dumping y qué tipos hay?
¿Cómo funcionan estos tipos de dumping?
¿Qué problemas presenta el logical dump?

¿Porqué es importante la consistencia en sistema de archivos?
¿Cómo funciona el algoritmo fsck?

De qué trata el file system performance, qué problema existe?

¿Qué problema existe al implementar LRU en la caché?

¿Qué es el Block Read Ahead? ¿Qué pros y contras tiene?

¿Cómo se puede aplicar el Reducing Disk-Arm Motion?

¿Qué es la defragmentación de disco, cuando se puede realizar y que contras tiene?