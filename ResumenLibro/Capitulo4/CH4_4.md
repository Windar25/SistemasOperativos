# 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION

Hacer que el sistema de archivos funcione es una cosa; hacer que funcione de forma eficiente y robusta en la vida real es algo muy distinto.  En las secciones siguientes veremos algunos de los problemas que plantea la gestión de discos.

## 4.4.1 Disk-Space Management

Los archivos se almacenan normalmente en disco, por lo que la gestión del espacio en disco es una de las principales preocupaciones de los diseñadores de sistemas de archivos. Existen dos estrategias generales para almacenar un archivo de n bytes: se asignan n bytes consecutivos de espacio en disco, o el archivo se divide en un número de bloques (no necesariamente) contiguos. Como hemos visto, el almacenamiento de un archivo como una secuencia contigua de bytes tiene el problema obvio de que si un archivo crece, es posible que haya que moverlo en el disco. El mismo problema se aplica a los segmentos en memoria, excepto que mover un segmento en memoria es una operación relativamente rápida en comparación con mover un archivo de una posición de disco a otra. Por este motivo, casi todos los sistemas de archivos dividen los archivos en bloques de tamaño fijo que no tienen por qué ser adyacentes.

### Block Size

Una vez que se ha decidido almacenar los archivos en bloques de tamaño fijo, surge la pregunta de qué tamaño debe tener el bloque. Dada la forma en que están organizados los discos, el sector, la pista y el cilindro son candidatos obvios para la unidad de asignación (aunque todos ellos dependen del dispositivo, lo cual es un inconveniente). En un sistema de paginación, el tamaño de la página también es un contendiente importante.

Un tamaño de bloque grande significa que cada archivo, aunque sea de 1 byte, ocupa un cilindro entero. Por otro lado, un tamaño de bloque pequeño significa que la mayoría de los archivos abarcarán varios bloques y, por tanto, necesitarán varias búsquedas y retrasos de rotación para leerlos, lo que reduce el rendimiento. Por tanto, si la unidad de asignación es demasiado grande, desperdiciamos espacio; si es demasiado pequeña, desperdiciamos tiempo.

Un tamaño de bloque grande significa que cada archivo, aunque sea de 1 byte, ocupa un cilindro entero. Por otro lado, un tamaño de bloque pequeño significa que la mayoría de los archivos abarcarán varios bloques y, por tanto, necesitarán varias búsquedas y retrasos de rotación para leerlos, lo que reduce el rendimiento. Por tanto, si la unidad de asignación es demasiado grande, desperdiciamos espacio; si es demasiado pequeña, desperdiciamos tiempo.

Para hacer una buena elección es necesario disponer de información sobre la distribución del tamaño de los archivos.  Tanenbaum et al. (2006) estudiaron la distribución del tamaño de los archivos en el Departamento de Informática de una gran universidad de investigación (la VU) en 1984 y de nuevo en 2005, así como en un servidor web comercial que albergaba un sitio web político (www.electoral-vote.com).  Los resultados se muestran en la Fig. 4-20, donde para cada tamaño de archivo de potencia de dos, se enumera el porcentaje de todos los archivos menores o iguales para cada uno de los tres conjuntos de datos. Por ejemplo, en 2005, el 59.13% de todos los archivos de la VU tenían un tamaño igual o inferior a 4 KB y el 90,84% de todos los archivos tenían un tamaño igual o inferior a 64 KB. El tamaño medio de los archivos era de 2.475 bytes. Puede que a algunas personas les sorprenda este pequeño tamaño.

![figure4-20](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-20.png?raw=true)

¿Qué conclusiones podemos sacar de estos datos? Por un lado, con un tamaño de bloque de 1 KB, sólo entre el 30 y el 50% de todos los archivos caben en un solo bloque, mientras que con un bloque de 4 KB, el porcentaje de archivos que caben en un bloque sube hasta el 60-70%. Otros datos del artículo muestran que con un bloque de 4 KB, el 93% de los bloques de disco son utilizados por el 10% de los archivos más grandes. Esto significa que perder algo de espacio al final de cada archivo pequeño apenas importa porque el disco está lleno por un pequeño número de archivos grandes (vídeos) y la cantidad total de espacio ocupado por los archivos pequeños apenas importa. Incluso duplicando el espacio que ocupa el 90% de los archivos más pequeños apenas se notaría.

Por otro lado, utilizar un bloque pequeño significa que cada archivo estará formado por muchos bloques. Normalmente, la lectura de cada bloque requiere una búsqueda y un retardo de rotación (excepto en un disco de estado sólido), por lo que la lectura de un archivo formado por muchos bloques pequeños será lenta.

Como ejemplo, consideremos un disco con 1 MB por pista, un tiempo de rotación de 8,33 mseg y un tiempo medio de búsqueda de 5 mseg. El tiempo en milisegundos para leer un bloque de k bytes es la suma de los tiempos de búsqueda, rotación y transferencia: 

    5 + 4. 165 + (k/1000000) × 8. 33

La curva discontinua de la Fig. 4-21 muestra la tasa de datos de un disco de este tipo en función del tamaño del bloque. Para calcular la eficiencia de espacio, tenemos que hacer una suposición sobre el tamaño medio de los archivos. Para simplificar, supongamos que todos los archivos son de 4 KB. Aunque este número es ligeramente mayor que los datos medidos en la VU, los estudiantes probablemente tienen más archivos pequeños de los que estarían presentes en un centro de datos corporativo, por lo que podría ser una mejor suposición en general. La curva continua de la Fig. 4-21 muestra la eficiencia de espacio en función del tamaño de bloque

Las dos curvas pueden entenderse de la siguiente manera. El tiempo de acceso a un bloque está totalmente dominado por el tiempo de búsqueda y el retardo de rotación, por lo que, dado que va a costar 9 mseg acceder a un bloque, cuantos más datos se obtengan, mejor.

![figure4-21](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-21.png?raw=true)

De ahí que la velocidad de transmisión de datos aumente casi linealmente con el tamaño del bloque (hasta que las transferencias duran tanto que el tiempo de transferencia empieza a importar).

Consideremos ahora la eficiencia espacial. Con archivos de 4 KB y bloques de 1, 2 o 4 KB, los archivos utilizan 4, 2 y 1 bloque, respectivamente, sin desperdicio. Con un bloque de 8 KB y archivos de 4 KB, la eficiencia espacial desciende al 50%, y con un bloque de 16 KB baja al 25%. En realidad, pocos archivos son un múltiplo exacto del tamaño del bloque de disco, por lo que siempre se desperdicia algo de espacio en el último bloque de un archivo.

Lo que muestra el gráfico, sin embargo, es que el rendimiento y la utilización del espacio están intrínsecamente en conflicto. Los bloques pequeños son malos para el rendimiento, pero buenos para la utilización del espacio en disco. Para estos datos, no existe un compromiso razonable. El tamaño más cercano al cruce de las dos curvas es 64 KB, pero la velocidad de transmisión de datos es de sólo 6,6 MB/s y la eficiencia del espacio es de aproximadamente el 7%, ninguna de las cuales es muy buena. Históricamente, los sistemas de archivos han elegido tamaños comprendidos entre 1 y 4 KB, pero ahora que los discos superan 1 TB, quizá sea mejor aumentar el tamaño de bloque a 64 KB y aceptar el espacio de disco desperdiciado. El espacio en disco ya no escasea.

En un experimento para ver si el uso de archivos de Windows NT era sensiblemente diferente del uso de archivos de UNIX, Vogels realizó mediciones de archivos en la Universidad de Cornell (Vogels, 1999). Observó que el uso de archivos en NT es más complicado que en UNIX. Escribió:

*When we type a few characters in the Notepad text editor, saving this to afile will trigger 26 system calls, including 3 failed open attempts, 1 fileoverwrite and 4 additional open and close sequences.*

Sin embargo, Vogels observó un tamaño medio (ponderado por el uso) de los archivos sólo leídos de 1 KB, de los archivos sólo escritos de 2,3 KB y de los archivos leídos y escritos de 4,2 KB. Dadas las diferentes técnicas de medición de los conjuntos de datos y el año, estos resultados son ciertamente compatibles con los de la VU.

### Keeping Track of Free Blocks

Una vez elegido el tamaño de bloque, la siguiente cuestión es cómo llevar la cuenta de los bloques libres. Dos métodos son ampliamente usados, como se muestra en la Fig. 4-22. El primero consiste en utilizar una lista enlazada de bloques de disco, en la que cada bloque contiene tantos números de bloque de disco libres como quepan. Con un bloque de 1 KB y un número de bloque de disco de 32 bits, cada bloque de la lista libre contiene los números de 255 bloques libres. (Considere un disco de 1 TB, que tiene alrededor de 1.000 millones de bloques de disco. Para almacenar todas estas direcciones a 255 por bloque se necesitan unos 4 millones de bloques. Generalmente, los bloques libres se utilizan para mantener la lista libre, por lo que el almacenamiento es esencialmente gratuito.

![figure4-22](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-22.png?raw=true)

La otra técnica de gestión del espacio libre es el mapa de bits. Un disco con n bloques requiere un mapa de bits con n bits. Los bloques libres se representan con 1s en el mapa, los bloques asignados con 0s (o viceversa).  Para nuestro disco de ejemplo de 1 TB, necesitamos 1.000 millones de bits para el mapa, lo que requiere almacenar unos 130.000 bloques de 1 KB. No es sorprendente que el mapa de bits requiera menos espacio, ya que utiliza 1 bit por bloque, frente a los 32 bits del modelo de lista enlazada. Sólo si el disco está casi lleno (es decir, tiene pocos bloques libres), el esquema de listas enlazadas necesitará menos bloques que el mapa de bits.

Si los bloques libres tienden a venir en largas series de bloques consecutivos, el sistema de lista libre puede modificarse para llevar la cuenta de series de bloques en lugar de bloques individuales. Se puede asociar a cada bloque un contador de 8, 16 o 32 bits que indique el número de bloques libres consecutivos. En el mejor de los casos, un disco básicamente vacío podría representarse con dos números: la dirección del primer bloque libre seguida del contador de bloques libres. En cambio, si el disco está muy fragmentado, el seguimiento de las ejecuciones es menos eficaz que el de los bloques individuales, ya que no sólo hay que almacenar la dirección, sino también el contador.

Esta cuestión ilustra un problema que suelen tener los diseñadores de sistemas operativos. Hay múltiples estructuras de datos y algoritmos que pueden utilizarse para resolver un problema, pero elegir el mejor requiere datos que los diseñadores no tienen y no tendrán hasta que el sistema se despliegue y se utilice intensamente. E incluso entonces, los datos pueden no estar disponibles.  Por ejemplo, nuestras propias mediciones del tamaño de los archivos en la VU en 1984 y 1995, los datos del sitio web y los datos de Cornell son sólo cuatro muestras. Aunque es mucho mejor que nada, no tenemos ni idea de si también son representativos de los ordenadores domésticos, los ordenadores de empresa, los ordenadores gubernamentales y otros. Con un poco de esfuerzo podríamos haber conseguido un par de muestras de otros tipos de ordenadores, pero incluso así sería absurdo extrapolarlos a todos los ordenadores del tipo medido.

Volviendo por un momento al método de la lista libre, sólo es necesario mantener un bloque de punteros en la memoria principal. Cuando se crea un archivo, los bloques necesarios se toman del bloque de punteros. Cuando se agota, se lee un nuevo bloque de punteros del disco. Del mismo modo, cuando se elimina un archivo, se liberan sus bloques y se añaden al bloque de punteros de la memoria principal. Cuando este bloque se llena, se escribe en el disco.

Bajo ciertas circunstancias, este método conduce a E/S de disco innecesarias. Considere la situación de la Fig. 4-23(a), en la que el bloque de punteros en memoria sólo tiene espacio para dos entradas más. Si se libera un fichero de tres bloques, el bloque de punteros se desborda y tiene que escribirse en disco, lo que lleva a la situación de la Fig. 4-23(b).  Si ahora se escribe un fichero de tres bloques, hay que volver a leer el bloque completo de punteros, lo que nos lleva de nuevo a la Fig. 4-23(a).  Si el archivo de tres bloques que se acaba de escribir era un archivo temporal, cuando se libera, se necesita otra escritura en disco para escribir el bloque completo de punteros de nuevo en el disco. En resumen, cuando el bloque de punteros está casi vacío, una serie de archivos temporales de corta duración pueden causar muchos I/O de disco.

Un enfoque alternativo que evita la mayor parte de esta I/O de disco es dividir el bloque completo de punteros. Así, en lugar de pasar de la Fig. 4-23(a) a la Fig. 4-23(b), pasamos de la Fig. 4-23(a) a la Fig. 4-23(c) cuando se liberan tres bloques. Ahora el sistema puede manejar una serie de archivos temporales sin hacer ninguna I/O de disco. Si el bloque en memoria se llena, se escribe en el disco, y se lee el bloque medio lleno del disco. La idea aquí es mantener la mayoría de los bloques de punteros en el disco llenos (para minimizar el uso del disco), pero mantener el que está en memoria medio lleno, para que pueda manejar tanto la creación como la eliminación de archivos sin I/O de disco en la lista libre.


Con un mapa de bits, también es posible mantener sólo un bloque en memoria, yendo al disco a por otro sólo cuando esté completamente lleno o vacío. Una ventaja adicional de este enfoque es que, al realizar toda la asignación a partir de un único bloque del mapa de bits, los bloques de disco estarán muy juntos, minimizando así el movimiento del brazo del disco.

![figure4-23](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-23.png?raw=true)

Dado que el mapa de bits es una estructura de datos de tamaño fijo, si el kernel está (parcialmente) paginado, el mapa de bits puede colocarse en memoria virtual y paginarse según sea necesario.

### Disk Quotas

Para evitar que los usuarios acaparen demasiado espacio en disco, los sistemas operativos multiusuario suelen ofrecer un mecanismo para imponer cuotas de disco. La idea es que el administrador del sistema asigne a cada usuario una asignación máxima de archivos y bloques, y el sistema operativo se asegure de que los usuarios no excedan sus cuotas. A continuación se describe un mecanismo típico.

Cuando un usuario abre un archivo, se localizan los atributos y las direcciones de disco y se introducen en una tabla de archivos abiertos en la memoria principal. Entre los atributos hay una entrada que indica quién es el propietario. Cualquier aumento en el tamaño del archivo se cargará a la cuota del propietario.

Una segunda tabla contiene el registro de cuota de cada usuario con un archivo actualmente abierto, aunque el archivo haya sido abierto por otra persona. Esta tabla se muestra en la Fig. 4-24. Es un extracto de un archivo de cuotas en disco para los usuarios cuyos archivos están actualmente abiertos. Cuando se cierran todos los archivos, el registro se vuelve a escribir en el archivos de cuotas.

Cuando se crea una nueva entrada en la tabla de archivos abiertos, se introduce en ella un puntero al registro de cuotas del propietario, para facilitar la búsqueda de los distintos límites. Cada vez que se añade un bloque a un archivo, se incrementa el número total de bloques cargados al propietario y se comprueban los soft y hard limits. Es posible que se supere el soft limit, pero no el hard limit. Si se intenta añadir datos a un archivo cuando se ha alcanzado el hard limit de bloques, se producirá un error. También existen comprobaciones análogas para el número de archivos para evitar que un usuario acapare todos los i-nodos

Cuando un usuario intenta iniciar sesión, el sistema examina el archivo de cuotas para ver si el usuario ha superado el límite suave para el número de archivos o el número de bloques de disco.

![figure4-24](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-24.png?raw=true)

Si se ha violado alguno de los límites, se muestra una advertencia y el recuento de advertencias restantes se reduce en uno. Si la cuenta llega a cero, el usuario ha ignorado la advertencia demasiadas veces y no se le permite iniciar sesión. 

Para obtener permiso para iniciar sesión de nuevo será necesario hablar con el administrador del sistema. Este método tiene la propiedad de que los usuarios pueden superar sus soft limits durante una sesión de inicio de sesión, siempre que eliminen el exceso antes de cerrar la sesión. Los hard limits no pueden superarse nunca.

## 4.4.2 File-System Backups

La destrucción de un sistema de archivos suele ser un desastre mucho mayor que la destrucción de un ordenador. Si un ordenador queda destruido por un incendio, un rayo o una taza de café derramada sobre el teclado, es molesto y costará dinero, pero por lo general se puede comprar uno nuevo con un mínimo de complicaciones. Los ordenadores personales más baratos pueden sustituirse incluso en una hora con sólo ir a una tienda de informática (excepto en las universidades, donde la emisión de una orden de compra requiere tres comités, cinco firmas y 90 días).

Si el sistema de archivos de un ordenador se pierde irrevocablemente, ya sea por culpa del hardware o del software, restaurar toda la información será difícil, llevará mucho tiempo y, en muchos casos, será imposible. Para las personas cuyos programas, documentos, registros fiscales, archivos de clientes, bases de datos, planes de marketing u otros datos desaparecen para siempre, las consecuencias pueden ser catastróficas. Aunque el sistema de archivos no puede ofrecer ninguna protección contra la destrucción física de los equipos y soportes, sí puede ayudar a proteger la información.  Es bastante sencillo: hacer copias de seguridad.  Pero no es tan sencillo como parece. Echemos un vistazo.

La mayoría de la gente no cree que merezca la pena dedicar tiempo y esfuerzo a hacer copias de seguridad de sus archivos, hasta que un buen día su disco se estropea abruptamente, momento en el que la mayoría sufre por no poder recuperar su información. Sin embargo, las empresas (normalmente) son conscientes del valor de sus datos y suelen hacer copias de seguridad al menos una vez al día, a menudo en cinta.  Las cintas modernas tienen capacidad para cientos de gigabytes y cuestan céntimos por gigabyte. Sin embargo, hacer copias de seguridad no es tan trivial como parece, por lo que a continuación examinaremos algunas de las cuestiones relacionadas.

as copias de seguridad en cinta suelen realizarse para solucionar uno de estos dos posibles problemas:

1. Recuperarse de un desastre.
2. Recuperarse de errores simples.

La primera consiste en volver a poner en marcha el ordenador tras un fallo de disco, un incendio, una inundación u otra catástrofe natural. En la práctica, estas cosas no ocurren muy a menudo, por lo que mucha gente no se molesta en hacer copias de seguridad. Por la misma razón, estas personas tampoco suelen tener un seguro contra incendios en sus casas.

La segunda razón es que los usuarios suelen eliminar accidentalmente archivos que luego vuelven a necesitar. Este problema ocurre tan a menudo que, cuando un archivo se ''elimina'' en Windows, no se borra del todo, sino que se mueve a un directorio especial, la papelera de reciclaje, para que pueda ser pescado y restaurado fácilmente más tarde. Las copias de seguridad llevan este principio más allá y permiten recuperar archivos borrados hace días, o incluso semanas, a partir de viejas cintas de copia de seguridad.
   
Hacer una copia de seguridad lleva mucho tiempo y ocupa mucho espacio, por lo que hacerla de forma eficaz y cómoda es importante. Estas consideraciones plantean las siguientes cuestiones. En primer lugar, ¿se debe hacer una copia de seguridad de todo el sistema de archivos o sólo de una parte?  En muchas instalaciones, los programas ejecutables (binarios) se guardan en una parte limitada del árbol del sistema de archivos. No es necesario hacer una copia de seguridad de estos archivos si todos ellos se pueden reinstalar desde el sitio web del fabricante o desde el DVD de instalación.  Además, la mayoría de los sistemas tienen un directorio para archivos temporales. Tampoco suele ser necesario hacer una copia de seguridad. En UNIX, todos los archivos especiales (dispositivos de E/S) se guardan en un directorio/dev. No sólo no es necesario hacer una copia de seguridad de este directorio, sino que es francamente peligroso porque el programa de copia de seguridad se colgaría para siempre si intentara leer cada uno de ellos hasta el final.  En resumen, normalmente es deseable hacer copias de seguridad sólo de directorios específicos y de todo lo que contienen en lugar de todo el sistema de ficheros.

En segundo lugar, es un desperdicio hacer copias de seguridad de archivos que no han cambiado desde la copia anterior, lo que nos lleva a la idea de los **incremental dumps**. La forma más sencilla de hacer un dump incremental es hacer un dump completo (copia de seguridad) periódicamente, por ejemplo semanal o mensualmente, y hacer un dump diario sólo de los archivos que se han modificado desde el último dump completo. Aún mejor es dumpear sólo los archivos que han cambiado desde el último dump. Aunque este esquema minimiza el tiempo de dump, complica la recuperación, ya que primero hay que restaurar el dump completo más reciente, seguido de todos los dump incrementales en orden inverso. Para facilitar la recuperación, a menudo se utilizan esquemas de dump incremental más sofisticados.

En tercer lugar, como normalmente se vuelcan cantidades inmensas de datos, puede ser conveniente comprimirlos antes de escribirlos en la cinta. Sin embargo, con muchos algoritmos de compresión, un solo punto defectuoso en la cinta de copia de seguridad puede frustrar el algoritmo de descompresión y hacer ilegible un archivo entero o incluso una cinta entera. Por lo tanto, la decisión de comprimir el flujo de copia de seguridad debe considerarse cuidadosamente.

En cuarto lugar, es difícil realizar una copia de seguridad en un sistema de archivos activo. Si se añaden, eliminan y modifican archivos y directorios durante el proceso de dump, el dump resultante puede ser incoherente. Sin embargo, como hacer un dump puede llevar horas, puede ser necesario desconectar el sistema durante gran parte de la noche para hacer la copia de seguridad, algo que no siempre es aceptable. Por este motivo, se han ideado algoritmos para realizar fotografías del estado del sistema de archivos copiando las estructuras de datos críticas y, a continuación, solicitando a los futuros cambios en los archivos y directorios que copien los bloques en lugar de actualizarlos(Hutchinson et al., 1999). De este modo, el sistema de archivos queda congelado en el momento de la foto, por lo que se puede hacer una copia de seguridad a posteriori.

Quinto y último, hacer copias de seguridad introduce muchos problemas no técnicos en una organización. El mejor sistema de seguridad en línea del mundo puede ser inútil si el administrador del sistema guarda todos los discos o cintas de copia de seguridad en su despacho y lo deja abierto y sin vigilancia cada vez que sale al pasillo a por café. Todo lo que tiene que hacer un espía es asomarse un segundo, meterse un pequeño disco o cinta en el bolsillo y marcharse alegremente. Adiós a la seguridad. Además, hacer una copia de seguridad diaria sirve de poco si el incendio que quema los ordenadores también quema todos los discos de copia de seguridad. Por esta razón, los discos de copia de seguridad deberían guardarse fuera de las instalaciones, pero esto introduce más riesgos de seguridad (porque ahora hay que proteger dos instalaciones). Para un análisis exhaustivo de estas y otras cuestiones prácticas de administración, véase Nemeth et al. (2013). A continuación, sólo trataremos las cuestiones técnicas relacionadas con las copias de seguridad de los sistemas de archivos.

Se pueden utilizar dos estrategias para el dump de un disco a un disco de copia de seguridad: un physical dump o un logical dump. Un **physical dump** comienza en el bloque 0 del disco, escribe todos los bloques del disco en el disco de salida en orden, y se detiene cuando ha copiado el último. Un programa de este tipo es tan sencillo que probablemente se pueda hacer 100% libre de errores, algo que probablemente no se puede decir de ningún otro programa útil.Sin embargo, vale la pena hacer varios comentarios sobre el dump físico. En primer lugar, no tiene sentido hacer copias de seguridad de los bloques de disco no utilizados. Si el programa de volcado puede acceder a la estructura de datos de bloques libres, puede evitar volcar los bloques no utilizados. Sin embargo, saltarse bloques no utilizados requiere escribir el número de cada bloque delante del bloque (o el equivalente), puesto que ya no es cierto que el bloque k de la copia de seguridad fuera el bloque k del disco.

Otra preocupación es el dump de bloques defectuosos. Es casi imposible fabricar discos grandes sin defectos.  Siempre hay algunos bloques defectuosos. A veces, cuando se realiza un formateo de bajo nivel, los bloques defectuosos se detectan, se marcan como defectuosos y se sustituyen por bloques de repuesto reservados al final de cada pista para este tipo de emergencias.  En muchos casos, la controladora de disco gestiona la sustitución de los bloques defectuosos de forma transparente, sin que el sistema operativo se entere.

Sin embargo, a veces los bloques se estropean después de formatearlos, en cuyo caso el sistema operativo acabará detectándolos. Normalmente, resuelve el problema creando un "archivo" con todos los bloques defectuosos para asegurarse de que nunca aparezcan en la reserva de bloques libres ni se asignen. Ni que decir tiene que este archivo es completamente ilegible.

Si todos los bloques defectuosos son reasignados por la controladora de disco y se ocultan al sistema operativo como se acaba de describir, el dump físico funciona correctamente.  Por otra parte, si son visibles para el sistema operativo y se mantienen en uno o más archivos de bloques defectuosos o mapas de bits, es absolutamente esencial que el programa de dump físico tenga acceso a esta información y evite volcarlos para evitar interminables errores de lectura del disco al intentar realizar una copia de seguridad del archivo de bloques defectuosos.

Los sistemas Windows tienen archivos de paginación e hibernación que no se necesitan en caso de restauración y de los que no se debe hacer copia de seguridad. Los sistemas específicos también pueden tener otros archivos internos de los que no se debe hacer copia de seguridad, por lo que el programa de dumping debe tenerlos en cuenta.

Las principales ventajas del dump físico son su sencillez y su gran velocidad (básicamente, puede ejecutarse a la velocidad del disco). Las principales desventajas son la imposibilidad de omitir directorios seleccionados, realizar dump incrementales y restaurar archivos individuales a petición. Por estas razones, la mayoría de las instalaciones realizan volcados lógicos.

Un **logical dump** comienza en uno o más directorios especificados y vuelca recursivamente todos los archivos y directorios que se encuentran allí y que han cambiado desde una fecha base determinada (por ejemplo, la última copia de seguridad para un dump incremental o la instalación del sistema para un dump completo). Así, en un dump lógico, el disco de dumping recibe una serie de directorios y archivos cuidadosamente identificados, lo que facilita la restauración de un archivo o directorio específico a petición.

Dado que el dump lógico es la forma más común, examinemos un algoritmo común en detalle utilizando el ejemplo de la Fig. 4-25 para guiarnos. La mayoría de los sistemas UNIX utilizan este algoritmo. En la figura vemos un árbol de archivos con directorios (cuadrados) y archivos (círculos). Los elementos sombreados han sido modificados desde la fecha base y por tanto necesitan ser volcados. Los que no están sombreados no necesitan ser volcados.

Este algoritmo también vuelca todos los directorios (incluso los no modificados) que se encuentran en la ruta de un archivo o directorio modificado por dos razones. La primera razón es hacer posible restaurar los archivos y directorios volcados a un sistema de archivos nuevo en un ordenador diferente. De este modo, los programas de volcado y restauración pueden utilizarse para transportar sistemas de archivos enteros entre ordenadores.

La segunda razón para volcar directorios no modificados por encima de archivos modificados es hacer posible la restauración incremental de un solo archivo (posiblemente para manejar la recuperación de la estupidez). Supongamos que se realiza un volcado completo del sistema de archivos el domingo por la noche y un volcado incremental el lunes por la noche.  El martes se elimina el directorio/usr/jhs/proj/nr3, junto con todos los directorios y archivos que contiene. Supongamos que el miércoles por la mañana el usuario quiere restaurar el archivo/usr/jhs/proj/nr3/plans/summary. Sin embargo, no es posible restaurar simplemente el archivoummary porque no hay ningún lugar donde colocarlo. Primero hay que restaurar los directorios nr3 y plans.  Para obtener sus propietarios, modos, tiempos, y lo que sea, correcto, estos di-rectorios deben estar presentes en el disco de volcado a pesar de que ellos mismos no fueron modificados desde el volcado completo anterior.


![figure4-25](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-25.png?raw=true)

El algoritmo de dumping mantiene un mapa de bits indexado por número de nodo-i con sever-al bits por nodo-i. Los bits se activan y desactivan en este mapa a medida que avanza el algoritmo.  El algoritmo funciona en cuatro fases. La fase 1 comienza en la dirección de inicio (la raíz en este ejemplo) y examina todas las entradas que contiene. Para cada archivo modificado, se marca su nodo i en el mapa de bits. También se marca cada directorio (tanto si ha sido modificado como si no) y, a continuación, se inspecciona recursivamente.

Al final de la fase 1, todos los archivos modificados y todos los directorios han sido marcados en el mapa de bits, como se muestra (por sombreado) en la Fig. 4-26(a).  La fase 2 vuelve a recorrer conceptualmente el árbol de forma recursiva, desmarcando todos los directorios que no tengan archivos o directorios modificados en ellos o debajo de ellos. Esta fase deja el mapa de bits como se muestra en la Fig. 4-26(b). Observe que los directorios 10, 11, 14, 27, 29 y 30 están ahora sin marcar porque no contienen nada bajo ellos que haya sido modificado. No serán volcados.  Por el contrario, los directorios 5 y 6 serán volcados aunque no hayan sido modificados porque serán necesarios para restaurar los cambios de hoy en una máquina nueva. Por razones de eficacia, las fases 1 y 2 pueden combinarse en un recorrido por el árbol.

En este punto se sabe qué directorios y ficheros deben ser volcados. Estos son los que están marcados en la Fig. 4-26(b).  La fase 3 consiste en escanear los nodos-i en orden numérico y volcar todos los directorios marcados para ser volcados.

Se muestran en la Fig. 4-26(c).  A cada directorio se le anteponen sus atributos (propietario, hora, etc.) para que puedan ser restaurados.   Finalmente, en la fase 4, los archivos marcados en la Fig. 4-26(d) también son volcados, de nuevo precedidos por sus atributos.  Esto completa el volcado.

Restaurar un sistema de archivos desde el disco de dumping es sencillo.  Para empezar, se crea un sistema de archivos vacío en el disco. A continuación, se vuelve a almacenar el volcado completo más reciente. Dado que los directorios aparecen en primer lugar en el disco de volcado, se restauran todos primero, lo que proporciona un esqueleto del sistema de archivos. A continuación, se restauran los archivos. Este proceso se repite con el primer volcado incremental realizado después del volcado completo, luego con el siguiente, etc.

Aunque el volcado lógico es sencillo, hay algunas cuestiones delicadas. Por un lado, como la lista de bloques libres no es un archivo, no se vuelca y, por tanto, debe reconstruirse desde cero una vez restaurados todos los volcados. Esto siempre es posible, ya que el conjunto de bloques libres no es más que el complemento del conjunto de bloques contenidos en todos los ficheros combinados.

Otro problema son los enlaces. Si un archivo está vinculado a dos o más directorios, es importante que el archivo se restaure una sola vez y que todos los directorios que se supone que deben apuntar a él lo hagan.

Otro problema es que los archivos UNIX pueden contener agujeros. Es legal abrir un archivo, escribir unos pocos bytes, luego buscar un offset de archivo distante y escribir unos pocos bytes más. Los bloques intermedios no forman parte del archivo y no deben volcarse ni restaurarse. Los core files a menudo tienen un hueco de cientos de megabytes entre el segmento de datos y la pila. Si no se maneja correctamente, cada core file restaurado llenará esta área con ceros y, por lo tanto, tendrá el mismo tamaño que el espacio de direcciones virtual (por ejemplo, 2^32bytes, o peor aún, 2^64bytes).

Por último, los archivos especiales, las tuberías con nombre y similares (cualquier cosa que no sea un archivo real) nunca deben volcarse, independientemente del directorio en el que se encuentren (no es necesario que se limiten a /dev).  Para más información sobre copias de seguridad de sistemas de ficheros, véase Chervenak et al. (1998) y Zwicky (1991).

## 4.4.3 File-System Consistency

Otra área en la que la fiabilidad es un problema es la consistencia del sistema de archivos. Muchos sistemas de archivos leen bloques, los modifican y los escriben más tarde. Si el sistema falla antes de que se hayan escrito todos los bloques modificados, el sistema de archivos puede quedar en un estado inconsistente. Este problema es especialmente crítico si algunos de los bloques que no se han escrito son bloques de nodo-i, bloques de directorio o bloques que contienen la lista libre.

Para hacer frente a los sistemas de archivos inconsistentes, la mayoría de los ordenadores tienen un programa de utilidad que comprueba la coherencia del sistema de archivos. Por ejemplo, UNIX tiene fsck; Windows tiene sfc(y otros). Esta utilidad puede ejecutarse siempre que se arranque el sistema, especialmente después de un fallo. La siguiente descripción explica cómo funciona fsck. Sfc es algo diferente porque funciona en un sistema de archivos diferente, pero el principio general de utilizar la redundancia inherente del sistema de archivos para repararlo sigue siendo válido. Todos los comprobadores de sistemas de archivos verifican cada sistema de archivos (partición de disco) independientemente de los demás.

Se pueden realizar dos tipos de comprobaciones de consistencia: bloques y archivos.  Para comprobar la coherencia de los bloques, el programa crea dos tablas, cada una de las cuales contiene un contador para cada bloque, inicialmente fijado en 0. Los contadores de la primera tabla registran cuántas veces está presente cada bloque en un archivo; los contadores de la segunda tabla registran cuántas veces está presente cada bloque en la lista de bloques libres (o el mapa de bits de bloques libres).

A continuación, el programa lee todos los nodos-i utilizando un dispositivo en bruto, que ignora la estructura del archivo y sólo devuelve todos los bloques de disco empezando por 0. A partir de un nodo-i, es posible construir una lista de todos los números de bloque utilizados en el archivo correspondiente.  A medida que se lee cada número de bloque, se incrementa su contador en la primera tabla. A continuación, el programa examina la lista libre o mapa de bits para encontrar todos los bloques que no están en uso. Cada vez que aparece un bloque en la lista de bloques libres, se incrementa su contador en la segunda tabla.

Si el sistema de archivos es consistente, cada bloque tendrá un 1 en la primera tabla o en la segunda tabla, como se ilustra en la Fig. 4-27(a).  Sin embargo, como resultado de un crash, las tablas podrían tener el aspecto de la Fig. 4-27(b), en la que el bloque 2 no aparece en ninguna de las tablas. Se informará de que hay un **missing block**. Si bien los bloques faltantes no causan ningún daño real, desperdician espacio y, por lo tanto, reducen la capacidad del disco. La solución a los bloques perdidos es sencilla: el verificador del sistema de archivos simplemente los añade a la lista de libres.

Otra situación que puede darse es la de la Fig. 4-27(c).  Aquí vemos un bloque, el número 4, que aparece dos veces en la lista libre. (Los duplicados sólo pueden ocurrir si la lista libre es realmente una lista; con un mapa de bits es imposible). La solución aquí también es simple: reconstruir la lista libre.

Lo peor que puede ocurrir es que el mismo bloque de datos esté presente en dos o más archivos, como se muestra en la Fig. 4-27(d) con el bloque 5. Si alguno de estos archivos se mueve de nuevo, el bloque 5 pasará a la lista de libres. Si cualquiera de estos archivos es movido, el bloque 5 será puesto en la lista de libres, llevando a una situación en la que el mismo bloque está en uso y libre al mismo tiempo. Si se eliminan ambos archivos, el bloque se pondrá en la lista de libres dos veces.

![figure4-27](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter4/figure4-27.png?raw=true)

La acción apropiada para el verificador del sistema de archivos es asignar un bloque libre, copiar el contenido del bloque 5 en él, e insertar la copia en uno de los archivos. De esta manera, el contenido de la información de los archivos no cambia (aunque casi con seguridad uno está distorsionado), pero la estructura del sistema de archivos al menos se hace consistente. El error debe ser reportado, para permitir al usuario inspeccionar el daño.

Además de comprobar que cada bloque está debidamente contabilizado, el comprobador del sistema de archivos también comprueba el sistema de directorios. También utiliza una tabla de contadores, pero éstos son por archivo, en lugar de por bloque. Comienza en el directorio raíz y desciende recursivamente por el árbol, inspeccionando cada directorio del sistema de archivos. Para cada i-nodo en cada directorio, incrementa un contador para el recuento de uso de ese archivo. Recuerde que, debido a los hard links, un archivo puede aparecer en dos o más directorios. Los symbolic links no cuentan y no hacen que se incremente el contador del archivo de destino.

Cuando el verificador ha terminado, tiene una lista, indexada por número de nodo-i, que indica cuántos directorios contienen cada archivo.  A continuación, compara estos números con los recuentos de links almacenados en los propios nodos-i. Estos recuentos comienzan en 1 cuando se crea un archivo y se incrementan cada vez que se realiza un enlace (duro) al archivo.  En un sistema de archivos coherente, ambos recuentos coinciden. Sin embargo, pueden producirse dos tipos de errores: el recuento de enlaces en el nodo-i puede ser mayor o menor.

Si el recuento de enlaces es mayor que el número de entradas de directorio, aunque se eliminen todos los archivos de los directorios, el recuento seguirá siendo distinto de cero y el nodo i no se eliminará. Este error no es grave, pero desperdicia espacio en el disco con ficheros que no están en ningún directorio. Debería solucionarse ajustando el recuento de enlaces en el nodo-i al valor correcto.

El otro error es potencialmente catastrófico. Si dos entradas de directorio están vinculadas a un archivo, pero el nodo-i dice que sólo hay una, cuando cualquiera de las entradas de directorio se mueve de nuevo, el recuento del nodo-i irá a cero. Cuando el recuento de un nodo-i llega a cero, el sistema de archivos lo marca como no utilizado y libera todos sus bloques. Esta acción provocará que uno de los directorios apunte ahora a un nodo-i no utilizado, cuyos bloques podrían asignarse pronto a otros archivos. De nuevo, la solución es simplemente forzar el recuento de links en el nodo-i al número real de entradas de directorio
# Preguntas

¿Qué pros y contras tienen una página grande y pequeña?

¿Cómo funcina el disk space utilization y el data rate(Mb/sec) con distintos tamaños de página.

¿Cuál es el tamaño de página recomendado y cómo afectan los tamaños de página?

¿Cómo se mantiene el registro de los discos libres con una lista enlazada?
¿Cómo se mantiene el registro de los discos libres con la otra estrategia?


¿Qué es una disk Quota?

¿Cómo funciona el dumping y qué tipos hay?
¿Cómo funcionan estos tipos de dumping?
¿Qué problemas presenta el logical dump?

¿Porqué es importante la consistencia en sistema de archivos?
¿Cómo funciona el algoritmo fsck?
