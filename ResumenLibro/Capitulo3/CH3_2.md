# A MEMORY ABSTRACTION: ADDRESS SPACES

En definitiva, exponer la memoria física a los procesos tiene varios inconvenientes importantes. Primero, si un programa de usuario puede acceder a cada byte de la memoria, ellos pueden fácilmente tumbarse al sistema operativo, intencionalmente o por accidente, deteniendo bruscamente el sistema(a menos que haya un hardware especial como el lock and key scheme del IBM 360). El problema existe siempre y cuando el usuario del programa (aplicacion) se está ejecutando. Segundo, con este modelo es difícil tener múltiples programas ejecutándose al mismo tiempo(tomando turnos si es que hay solo una CPU). En computadoras personales, es común tener varios programas abiertos al mismo tiempo(un procesador de texto, un programa de email, un web browser), one the ellos tiene la actual atención, pero los otros se reactivan con solo un click. Como esta situación es difícil de conseguir cuando no hay abstracción de la memoria física, habrá que hacer algo.

## The Notion of Address Space

Tienen que solucionarse 2 problemas para permitir tener múltiples aplicaciones en memoria al mismo tiempo, sin que una interfiera con la otra. Protección y relocalización. Hemos visto una solución primitiva al primer problema utilazada en el IBM 360: etiquetar trozos de memoria con un clave de protección y comparar la clave del proceso en ejecución con la de cada palabra de memoria obtenida. Sin embargo, este enfoque por sí solo no resuelve el segundo problema, aunque puede solucionarse reubicando los programas a medida que se cargan, pero se trata de una solución lenta y complicada.

Una mejor solución es inventar una nueva abstracción en memoria: la dirección de memoria. Al igual que el concepto de proceso crea una especie de CPU abstracta para ejecutar programas, el espacio de direcciones crea una especie de memoria abstracta para que vivan los programas. Un espacio de direcciones es el conjunto de direcciones que un proceso puede utilizar para dirigirse a la memoria. Cada proceso tiene su propio espacio de direcciones, independiente de los que pertenecen a otros procesos (excepto en algunas circunstancias especiales en las que los procesos quieren compartir sus espacios de direcciones).

El concepto de direccion de memoria es bastante general y se usa en varios contextos. Consideremos los números telefónicos. En los Estados Unidos y muchos otros países, un número telefónico local tiene solo 7 dígitos. La dirección de memoria para los números telefónicos va desde 0 000 000 hasta 9 999 999, aunque algunos números como los que empiezan por 000, no se utilizan. Con el crecimiento de los smarthphones, los módems y las máquinas de fax, este espacio se está quedando pequeño, en cuyo caso hay que utilizar más dígitos. El espacio de direcciones de un puerto de I/O en el x86 va de 0 a 16383. Las direcciones IPv4 son números de 32 bits, por lo que su espacio de direcciones va desde 0 a 2^32 -1 (denuevo, con algunos números reservados).

Las direcciones de memoria no tiene que ser númericas. El set de .com como dominio de internet is una dirección de memoria. Este espacio de direcciones está formado por cadenas de longitud 2 a 63 caracteres que pueden formarse usanod letras, números y guiones seguidos de .com. A estas alturas ya debería hacerse una idea. Es bastante sencillo . 

Algo más difícil es cómo dar a cada programa a cada su propio espacio de direcciones, de modo que la dirección 28 en un programa signifique la ubicación física diferente de la dirección 28 en otro programa. A continuación discutiremos una forma simple que solía ser común pero ha caído en desuso debido a la capacidad de poner esquemas mucho más complicados(y mejores) en los chips de CPU modernos.
### Base and Limit Registers

Esta solución simple usa una versión particular de la **dynamic relocation**. Lo que hace es mapear cada espacio de dirección en una memoria física de una forma simple. La solución clásica, que se utilizó en máquinas que iban desde el CDC 6600(el primer superordenador del mundo) hasta el Intel 8088 (el corazón del IBM PC original), consiste en equipar cada CPU con 2 registros de hardware especiales, normalmente denominados registros **base** y **límite**. Cuando estos registros son usados los programas se cargan en posiciones de memoria consecutivas siempre que haya espacio y sin reubicación durante la carga, como se muestra en la Fig. 3-2(c). Cuando se ejecuta un proceso, el registro base se carga con la dirección física donde comienza su programa en memoria y el registro límite se carga con la longitud del programa. En la Fig. 3-2(c), los valores base y límite que se cargarían en estos registros de hardware cuando se ejecuta el primer programa son 0 y 16,384, respectivamente. Los valores utilizados cuando se ejecuta el segundo programa son 16.384 y 32.768, respectivamente. Si se cargara un tercer programa de 16 KB directamente encima del segundo y se ejecutara, los registros base y límite serían 32.768 y 16.384.

Cada vez que un proceso hace referencia a la memoria, ya sea para obtener una instrucción o para leer o escribir una palabra de datos, el hardware de la CPU añade automáticamente el valor base a la dirección generada por el proceso antes de enviar la dirección al bus de memoria. Simultáneamente, comprueba si la dirección ofrecida es igual o mayor que el valor del registro límite, en cuyo caso se genera un fallo y se aborta el acceso. Así, en el caso de la primera instrucción del segundo programa de la Fig. 3-2(c), el proceso ejecuta

    JMP 28

pero el hardware lo cambia como si fuera 

    JMP 16412

entonces llega a la instrucción CMP como esperábamos. La configuración de los registros base y limit durente la ejecución del segundo programa se pueden ver en la figura 3-3.

Usando la base y los registros límite tenemos una forma fácil para darle a cada proceso su espacio de dirección privado dado que cada espacio de memoria generado se le añade automáticamente el contenido del registro base antes de ser enviada a memoria. En muchas implementaciones, los registros base y límite están protegidos de tal forma que sólo el sistema operativo puede modificarlos. Este era el caso en el CDC 6600, pero no en el Intel 8088, que ni siquiera tenía registro límite. Tenía múltiples registros base, permitiendo que el texto del programa y los datos, por ejemplo, fueran reubicados independientemente, pero no ofrecía protección contra referencias de memoria fuera de rango. 

Una desventaja de la reubicación mediante registros base y límite es la necesidad de realizar una suma y una comparación en cada referencia de memoria. Las comparaciones se pueden hacer rápidamente, pero las sumas son lentas debido al tiempo de propagación de acarreo a menos que usen circuitos de suma especiales.

## 3.2.2 Swapping

Si la memoria física del computador es lo suficientemente larga para mantener a todos los procesos, los esquemas descritos hasta ahora serán más o menos suficientes. Pero en práctica, el tamaño total de RAM necesario para todos los procesos es a menudo mucho más de lo que puede caber en memoria. En un típico Windows, OS X, o sistema Linux, algo de 50 o 100 procesos, o quizás más pueden iniciarse apenas la computadora se encienda. Por ejemplo, cuando se instala una aplicación en Windows, a menudo emite comandos para que en los siguientes arranques del sistema, se inicie un proceso que no hace nada excepto comprobar si hay actualizaciones de la aplicación. Un proceso de este tipo puede ocupar fácilmente entre 5 y 10 MB de memoria. Otros procesos en segundo plano comprueban el correo entrante, las conexiones de red entrantes y muchas otras cosas. Y todo esto antes de que se inicie el primer programa de usuario. Hoy en día, los programas de aplicación de usuario, serios, como Photoshop, pueden requerir fácilmente 500 MB sólo para arrancar y muchos gigabytes una vez que se empiezan a procesar datos.

En consecuencia, mantener todos los procesos en memoria todo el tiempo requiere una enorme cantidad de memoria y no se puede hacer si no hay memoria suficiente. 

![figura3.3](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter3/figure3-3.png?raw=true)

A lo largo de los años se han desarrollado 2 enfoques generales para hacer frente a la sobrecarga de memoria. La estrategia más sencilla, llamada **swapping** consiste en traer cada proceso en su totalidad, ejecutarlo durante un tiempo y luego devolverlo al disco.

Los procesos inactivos se almacenan en su mayor parte en el disco, por lo que no ocupan memoria cuando no se están ejecutando (aunque algunos de ellos se despierten periódicamente para hacer su trabajo y luego vuelven a dormirse). La otra estrategia es, llamada **virtual memory**, permite que los programas se ejecuten incluso cuando sólo están parcialmente en la memoria principal. A continuación estudiaremos swapping, en la Sec 3.3 estudiaremos la memoria virtual.

La operación del sistema de swapping se ve en la figura 3-4. Inicialmente solo un proceso A está en memoria. Luego un proceso B y C son creados o intercambiados desde el disco. En la figura 3-4(d) A es intercambiado al disco. Luego llega D y B sale, para que finalmente vuelva A. Dado que A se encuentre en un distinto espacio, las direcciones contenidas deben ser reasignadas, ya sea por el software cuando se intercambian o (más probablemente) por hardware durante la ejecución del programa. Por ejemplo, los registros base y límite funcionarían bien aquí.

![figura3.4](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter3/figure3-4.png?raw=true)

Cuando intercambiamos creamos múltiples huecos en la memoria, es posible combinarlas en una sola moviendo a todos los procesos abajo lo más que se pueda. Esta técnica es llamada **memory compaction**. Usualmente no se realiza dado que requiere mucho tiempo del CPU. Por ejemplo, en máquina de 16GB que puede copiar 8 bytes in 8 nsegundos, esto tomaría 16 segundos para compactar toda la memoria.

Un punto que merece la pena destacar se refiere a la cantidad de memoria que debe asignarse a un proceso cuando se crea o se intercambia. Si los procesos se crean con un tamaño fijo que nunca cambia, la asignación es sencilla: el sistema operativo asigna exactamente lo que se necesita, ni más ni menos.

Sin embargo, si los segmentos de datos de los procesos pueden crecer, por ejemplo, mediante la asignación dinámica de memoria desde el heap, como en muchos lenguajes de programación, se produce un problema cada vez que un proceso intenta crecer. Si hay un hueco adyacente al proceso, se puede asignar y permitir que el proceso crezca dentro del hueco. Por otro lado si el proceso es adyacente a otro proceso, el proceso en crecimiento deberá ser movido a un hueco en memoria lo suficientemente grande para él, uno o más procesos tendrán que ser intercambiados para crear un hueco lo suficientemente grande. Si un proceso no puede crecer en memoria y el area de intercambio está llena, el proceso tendrá que suspenderse hasta que se libere algo de espacio(o puede ser eliminado).

Si se espera que la mayoría de los procesos crezcan a medida que se ejecutan, probablemente sea una buena idea asignar un poco de memoria cada vez que un proceso es intercambiado o movido, para reducir la sobrecarga asociada con mover o intercambiar procesos que ya no caben en su memoria asignada. Sin embargo, cuando se intercambian procesos a disco, sólo se debe intercambiarse la memoria realmente en uso; es un desperdicio intercambiar también la memoria extra. En la figura 3-5 (a) vemos una configuración de memoria en la que se ha asignado espacio de crecimiento a 2 procesos.

![imagen3.5](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter3/figure3-5.png?raw=true)

Si los procesos pueden tener 2 segmentos crecientes (por ejemplo, el segmento de datos que se utiliza como un heap para las variables que se asignan y liberan dinámica y un segmento de pila para las variables locales normales y las direcciones de entorno) se sugiere una disposición alternativa, a saber, la de la figura 3-5(b). En esta figura vemos que cada proceso ilustrado tiene una pila en la parte superior de su memoria asignada que crece hacia abajo, y un segmento de datos justo después del texto del programa que crece hacia arriba. La memoria entre ellos puede usarse para cualquiera de los segmentos. Si se agota, el proceso tendrá que ser trasladado a un hueco con espacio suficiente, o bien salir de memoria hasta que se pueda crear un hueco lo suficientemente grande, o bien ser eliminado. 

## 3.2.3 Managing Free Memory

Cuando la memoria es asignada dinámicamente, el sistema operativo debe manejarlo. En términos generales, hay 2 maneras para llevar un registro de la memoria usada: bitmapts y free lists. En esta sección y la siguiente vamos a echarle un vistazo a estos 2 métodos. En el capítulo 10, vamos a ver algunos asignadores de memoria específicos en Linux(como buddy y los asignadores slab) en más detalle.

### Manejo de memoria con bitmaps

Con bitmap, la memoria es divida en unidades de asignación tan pequeñas como pocas palabras y tan grandes como varios kilobytes .Correspondiente a cada unidad de asignación tenemos un bit en el bitmap, el cual es 0 si la unidad está libre y 1 si está ocupada(o viceversa). La figura 3-6 muestra parte de la memoria y el bitmap correspondiente.

![figure3.6](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter3/figure3-6.png?raw=true)

El tamaño de la unidad de asignación es un importante tema de diseño. Cuanto menor sea el allocate, mayor será el bitmap. De todos modos incluso con una unidad de asignación de 4 bytes, 32 bits de memoria requieren solo 1 bit de mapa. Una memoria de 32n bits usaría n mapbits, entonces los bitmaps solo ocuparían 1/32 de memoria. Si la unidad de asignación escogida es grande, el bitmap será más pequeño, pero se desperdiciará memoria en la última unidad del proceso si el tamaño del proceso no es un múltiplo exacto de la unidad de asignación.

Un bitmap provee una simple forma de mantener registro de las palabras de memorias en una cantidad fija de memoria, dado que el tamaño del bitmap, depende solo del tamaño y del tamaño de la unidad de memoria. El problema principal es que cuando se ha decidido traer a memoria un proceso de k unidades, el gestor de memoria debe buscar en el mapa de bits una tirada de k bits consecutivos con 0 bits en el mapa. Buscar en el bitmap una tira de una longitud determinada es una operación lenta(porque la tira puede cruzar los límites de las palabras del mapa); este es un argumento en contra de los bitmaps.

### Memory Management with Linked Lists

Otra forma de mantener registro de la memoria es mantener con una lista ligada de asignación y liberación de segmentos de memoria, donde un segmento contiene un proceso o es un hueco vacío entre 2 procesos. La memoria de la figura 3-6(a) se presenta en la figura  en la figura 3-6(c) como una lista enlazada de segmentos. Cada entrada de la lista especifica un hueco (H) o proceso (P), la dirección en la que comienza, la longitud y un puntero al siguiente item. En este ejemplo, la lista de segmentos es ordenada por la dirección de memoria. Esta ordenación tiene la ventaja de que cuando un proceso termina o es intercambiado, la actualización de la lista es sencilla. Un procesos que termina tiene usualmente 2 vecinos (excepto cuando está en la parte superior o inferior de la memoria). Estos pueden ser procesos o huecos, dando lugar a las 4 combinaciones que se muestran en la figura 3-7. En la figura 3-7(a) actualizar la lista requiere reemplazar una P por H. En la figura 3-7(b) y la figura 3-7(c), 2 entradas se unen en una y la lista se hace una entrada más corta. En la figura 3-7 3 entradas se unen y 2 items son removidos de la lista.

Dado que la ranura de la tabla de procesos para el proceso que termina normalmente apuntará a la entrada de la lista para el propio proceso, puede ser más conveniente tener una lista doblemente-enlazada en vez de una simplemente enlazada como la de la figura 3-6(c). Esta estructura permite encontrar más fácilmente la entrada anterior y ver si es posible realizar un merge.

![figure3-7](https://github.com/gabo52/SistemasOperativos/blob/main/figures/Chapter3/figure3-7.png?raw=true)


Cuando los procesos y huecos se mantiene en una lista ordenada or direcciones, se pueden usar varios algoritmos para asignar memoria para un proceso creado(o un proceso existente que fue intercambiado del disco). Asumimos que el gestor de memoria conoce cuanta memoria se va a asignar. El algoritmo más simple es **first fit**. El gestor de memoria escanea a lo largo de la lista de segmentos hasta que encuentre un hueco el cual es lo suficientemente grande. El hueco es partido  en 2, uno para el proceso y otro para la memoria no usada, excepto en el caso estadísticamente improbable de un ajuste exacto. First fit es algoritmo rápido porque busca lo menos posible.

Una variación menor de first fit es **next fit**. Este funciona igual que first fit, excepto que mantiene un registro de dónde se encuentra cada vez que encuentra un hueco adecuado. El siguiemete momento donde se quiera encontrar un agujero, este comienza a buscar en la lista desde el lugar donde lo dejó la última vez, en lugar de hacerlo desde el principio, como hace first fit. Las simulaciones de Bays (1977) muestran que next fit ofrece un rendimiento ligeramente peor que first fit.

Otro algoritmo bastante conocido y usado es el algoritmo **best fit**. Best fit busca en toda la lista, desde el inicio hasta el final, y toma el hueco más pequeño que sea adecuado. En lugar de romper un agujero grande que podría necesitarse más tarde, best fit trata de buscar al agujero que está más cerca al tamaño actual necesitado, para que coincida mejor con la solicitud y los agujeros disponibles.

Como un ejemplo de first fit y best fit, consideremos la figura 3-6 denuevo. Si se necesita un bloque de 2, first fit va a asignar la memoria en 5, pero best fit lo haría en 18.

Best fit es más lento que firs fit dado que tiene que buscar en toda la lista todas las veces que sea llamado. Sorprendentemente, también supone un mayor desperdicio de memoria que first fit o next fit, ya que tiende a llenar la memoria con pequeños agujeros inútil. First fit genera agujeros más grandes por término medio.

Para evitar el problema de dividir las coincidencias casi exactas en un proceso y agujero minúsculo, se podría pensar en **worst fit**, el cual toma siempre al agujero más grande disponible, entonces el nuevo agujero va a ser lo suficientemente grande para ser útil. La simulación ha demostrado que worst fit tampoco es buena idea.

Los cuatro algoritmos pueden acelerarse manteniendo listas separadas para procesos y agujeros. De esta forma, ellos dedican todo su tiempo a inspeccionar los agujeros, no los procesos. El precio inevitable que hay que pagar por esta aceleración en la asignación, es la complejidad adicional y la ralentización cuando se desasigna memoria, dado que un elemento liberado tiene que ser eliminado de la lista de procesos y e insertado en la lista de agujeros.

Si se mantiene listas distintas para procesos y agujeros, la lista de agujeros puede ser ordenada por el tamaño, para hacer el best fit más rápido. Cuando el best fit busque en la lista de agujeros del más pequeño al más largo, apenas encuentre un agujero que encaje, sabe que ese agujero es el más pequeño que hará el trabajo y por tanto, el best fit. No es necesario buscar más, como ocurre con el esquema de lista simplemente enlazada. Con una lista de agujeros ordenadas por tamaño, first y best fit son igualmente de rápidos y next fit no tiene sentido.

Cuando los huecos se mantiene en listas separadas de los procesos, se pueden realizar pequeñas optimizaciones. En lugar de tener un conjunto separado de estructuras de datos para mantener la lista de agujeros, como se hace en la figura 3-6(c), la información pued guardarse en agujeros. 

La primera palabra de cada agujero podría ser el tamaño del agujero y la segunda palabra un puntero a la siguiente entrada. Los nodos de la lista de la figura 3-6(c), que requieren 3 palabras y un bit (P/H) ya no son necesarios.

Otro algoritmo de asignación es **quick fit**, que mantiene listas separadas para algunos de los tamaños más comunes solicitados. Por ejemplo, podría tener una tabla con n entradas, en la que la primera entrada es un puntero a la cabecera de la lista de huecos de 4KB, la segunda entrada es un puntero a la lista de huecos de 8KB, la tercera entrada es un puntero a huecos de 12KB, y así sucesivamente.

Los agujeros de, por ejemplo, 21 KB, podrían colocarse en la lista de 20 KB o en una lista especial de agujeros de tamaño impar.

Con **quick fit**, encontrar el tamaño de un agujero requerido es extremadamente rápido, pero tiene la misma desventaja que todos los esquemas que ordenan por tamaño de agujero, es decir, cuando un proceso termina o es intercambiado, encontrar a sus vecinos para saber si es posible una fusión con ellos es bastante caro. Si no se realiza la fusión, la memoria se fragmentará rápidamente en un gran número de pequeños agujeros en los que no cabe ningún proceso.






